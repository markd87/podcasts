1
00:00:00,000 --> 00:00:04,000
I think it's possible that physics has exploits and we should be trying to find them arranging

2
00:00:04,000 --> 00:00:08,680
some kind of a crazy quantum mechanical system that somehow gives you buffer overflow, somehow

3
00:00:08,680 --> 00:00:11,360
gives you a rounding error in the floating point.

4
00:00:11,360 --> 00:00:15,640
Synthetic intelligences are kind of like the next stage of development.

5
00:00:15,640 --> 00:00:21,040
And I don't know where it leads to, like at some point, I suspect the universe is some

6
00:00:21,040 --> 00:00:23,360
kind of a puzzle.

7
00:00:23,360 --> 00:00:30,200
These synthetic A.I.s will uncover that puzzle and solve it.

8
00:00:30,200 --> 00:00:36,000
The following is a conversation with Andreak Apathy, previously the director of AI at Tesla.

9
00:00:36,000 --> 00:00:43,600
And before that, at OpenAI and Stanford, he is one of the greatest scientists, engineers

10
00:00:43,600 --> 00:00:48,080
and educators in the history of artificial intelligence.

11
00:00:48,080 --> 00:00:53,860
This is the Lex Friedman podcast to support it, please check out our sponsors and now

12
00:00:53,860 --> 00:00:58,360
dear friends, here's Andreak Apathy.

13
00:00:58,360 --> 00:01:04,720
What is a neural network and what does it seem to do such a surprisingly good job of learning?

14
00:01:04,720 --> 00:01:05,720
What is a neural network?

15
00:01:05,720 --> 00:01:11,320
It's a mathematical abstraction of the brain, I would say, that's how it was originally

16
00:01:11,320 --> 00:01:12,320
developed.

17
00:01:12,320 --> 00:01:16,120
At the end of the day, it's a mathematical expression and it's a fairly simple mathematical expression

18
00:01:16,120 --> 00:01:17,600
when you get down to it.

19
00:01:17,600 --> 00:01:23,480
It's basically a sequence of matrix multiplies, which are really dot products mathematically

20
00:01:23,480 --> 00:01:25,600
and some nonlinearity is thrown in.

21
00:01:25,600 --> 00:01:29,480
And so it's a very simple mathematical expression and it's got knobs in it.

22
00:01:29,480 --> 00:01:30,480
Many knobs.

23
00:01:30,480 --> 00:01:31,480
Many knobs.

24
00:01:31,480 --> 00:01:34,280
And these knobs are loosely related to basically the synapses in your brain.

25
00:01:34,280 --> 00:01:35,280
They're trainable.

26
00:01:35,280 --> 00:01:36,280
They're modifiable.

27
00:01:36,280 --> 00:01:40,480
And so the idea is we need to find the setting of the knobs that makes the neural net do

28
00:01:40,480 --> 00:01:43,720
whatever you want it to do, like classify images and so on.

29
00:01:43,720 --> 00:01:48,880
And so there's not too much mystery, I was saying it, like you might think that basically

30
00:01:48,880 --> 00:01:52,640
don't want to end out with too much meaning with respect to the brain and how it works.

31
00:01:52,640 --> 00:01:56,280
It's really just a complicated mathematical expression with knobs and those knobs need

32
00:01:56,280 --> 00:01:59,120
a proper setting for it to do something desirable.

33
00:01:59,120 --> 00:02:04,320
Yeah, but poetry is just the collection of letters with spaces, but it can make us feel

34
00:02:04,320 --> 00:02:05,320
a certain way.

35
00:02:05,320 --> 00:02:10,200
In that same way, when you get a large number of knobs together, whether it's inside

36
00:02:10,200 --> 00:02:16,120
the brain or inside a computer, they seem to, they seem to surprise us with their power.

37
00:02:16,120 --> 00:02:17,520
Yeah, I think that's fair.

38
00:02:17,520 --> 00:02:23,800
So basically I'm underselling it by a lot because you definitely do get very surprising emergent

39
00:02:23,800 --> 00:02:27,760
behaviors out of these neural nets when they're large enough and trained on complicated

40
00:02:27,760 --> 00:02:28,760
enough problems.

41
00:02:28,760 --> 00:02:33,920
Like say, for example, the next word prediction in a massive data set from the internet.

42
00:02:33,920 --> 00:02:37,240
And then these neural nets take on a pretty surprising magical properties.

43
00:02:37,240 --> 00:02:41,520
Yeah, I think it's kind of interesting how much you can get out of even very simple mathematical

44
00:02:41,520 --> 00:02:42,520
formalism.

45
00:02:42,520 --> 00:02:48,000
When your brain right now is talking, is it doing next word prediction or is it doing something

46
00:02:48,000 --> 00:02:49,000
more interesting?

47
00:02:49,000 --> 00:02:53,040
Well, it's definitely some kind of a generative model that's a GPT-like and prompted by

48
00:02:53,040 --> 00:02:54,040
you.

49
00:02:54,040 --> 00:02:55,040
Yeah.

50
00:02:55,040 --> 00:02:58,880
So you're giving me a prompt and I'm kind of like responding to it in a generative way.

51
00:02:58,880 --> 00:03:04,120
And by yourself, perhaps a little bit like are you adding extra prompts from your own

52
00:03:04,120 --> 00:03:05,920
memory inside your head?

53
00:03:05,920 --> 00:03:06,920
Or no?

54
00:03:06,920 --> 00:03:11,000
Well, definitely feels like you're referencing some kind of a declarative structure of like

55
00:03:11,000 --> 00:03:12,520
memory and so on.

56
00:03:12,520 --> 00:03:17,240
And then you're putting that together with your prompt and giving away some answers.

57
00:03:17,240 --> 00:03:22,160
How much of what you just said has been said by you before?

58
00:03:22,160 --> 00:03:23,960
Nothing, basically, right?

59
00:03:23,960 --> 00:03:28,120
No, but if you actually look at all the words you've ever said in your life and you do

60
00:03:28,120 --> 00:03:33,960
a search, you'll probably said a lot of the same words in the same order before.

61
00:03:33,960 --> 00:03:34,960
Yeah.

62
00:03:34,960 --> 00:03:40,480
I mean, I'm using phrases that are common, et cetera, but I'm remixing it into a pretty

63
00:03:40,480 --> 00:03:42,160
sort of unique sentence at the end of the day.

64
00:03:42,160 --> 00:03:43,160
But you're right.

65
00:03:43,160 --> 00:03:44,160
Definitely.

66
00:03:44,160 --> 00:03:45,160
There's like a ton of remixing.

67
00:03:45,160 --> 00:03:46,160
Why?

68
00:03:46,160 --> 00:03:47,160
You didn't.

69
00:03:47,160 --> 00:03:52,400
It's like Magnus Carlson said, I'm rated 2,900, whatever, which is pretty decent.

70
00:03:52,400 --> 00:03:58,240
I think you're talking very, you're not giving enough credit to neural nets here.

71
00:03:58,240 --> 00:04:01,000
Why do they seem to?

72
00:04:01,000 --> 00:04:05,640
What's your best intuition about this emergent behavior?

73
00:04:05,640 --> 00:04:09,080
I mean, it's kind of interesting because I'm simultaneously underselling them.

74
00:04:09,080 --> 00:04:12,840
But I also feel like there's an element to which I'm over like it's actually kind of incredible

75
00:04:12,840 --> 00:04:16,480
that you can get so much emergent magical behavior out of them despite them being so simple

76
00:04:16,480 --> 00:04:17,480
mathematically.

77
00:04:17,480 --> 00:04:22,880
So I think those are kind of like two surprising statements that kind of just juxtapose together.

78
00:04:22,880 --> 00:04:26,280
And I think basically what it is is we are actually fairly good at optimizing these neural

79
00:04:26,280 --> 00:04:31,720
nets and when you give them a hard enough problem, they are forced to learn very interesting

80
00:04:31,720 --> 00:04:34,400
solutions in the optimization.

81
00:04:34,400 --> 00:04:39,080
And those solution basically have these emergent properties that are very interesting.

82
00:04:39,080 --> 00:04:44,000
There's wisdom and knowledge in the knobs.

83
00:04:44,000 --> 00:04:49,320
And so this representation that's in the knobs doesn't make sense to you intuitively

84
00:04:49,320 --> 00:04:54,840
that a large number of knobs can hold a representation that captures some deep wisdom

85
00:04:54,840 --> 00:04:57,000
about the data it has looked at.

86
00:04:57,000 --> 00:04:58,880
It's a lot of knobs.

87
00:04:58,880 --> 00:05:04,640
It's a lot of knobs and somehow speaking concretely, one of the neural nets that people

88
00:05:04,640 --> 00:05:09,560
are very excited about right now are our GPs which are basically just next word prediction

89
00:05:09,560 --> 00:05:10,560
networks.

90
00:05:10,560 --> 00:05:15,840
So you consume a sequence of words from the internet and you try to predict the next word.

91
00:05:15,840 --> 00:05:23,880
And once you train these on a large enough data set, you can basically prompt these neural

92
00:05:23,880 --> 00:05:27,080
nets in arbitrary ways and you can ask them to solve problems and they will.

93
00:05:27,080 --> 00:05:32,520
So you can just tell them, you can make it look like you're trying to solve some kind

94
00:05:32,520 --> 00:05:35,760
of mathematical problem and they will continue what they think is the solution based on what

95
00:05:35,760 --> 00:05:37,200
they've seen on the internet.

96
00:05:37,200 --> 00:05:43,200
And very often those solutions look very remarkably consistent, look correct potentially.

97
00:05:43,200 --> 00:05:45,560
Do you still think about the brain side of it?

98
00:05:45,560 --> 00:05:52,440
So as neural nets is an abstraction or mathematical abstraction of the brain, you still draw wisdom

99
00:05:52,440 --> 00:05:57,880
from the biological neural networks or even the bigger question.

100
00:05:57,880 --> 00:06:02,560
So you're a big fan of biology and biological computation.

101
00:06:02,560 --> 00:06:09,160
What impressive thing is biology doing to you that computers are not yet, that gap?

102
00:06:09,160 --> 00:06:13,560
I would say I'm definitely on a much more hesitant with the analogies to the brain than

103
00:06:13,560 --> 00:06:16,400
I think you would see potentially in the field.

104
00:06:16,400 --> 00:06:22,120
And I kind of feel like certainly the way neural networks started is everything stemmed

105
00:06:22,120 --> 00:06:24,120
from inspiration by the brain.

106
00:06:24,120 --> 00:06:28,320
But at the end of the day, artifacts that you get after training, they are arrived at

107
00:06:28,320 --> 00:06:31,600
by a very different optimization process than the optimization process that gave rise

108
00:06:31,600 --> 00:06:33,080
to the brain.

109
00:06:33,080 --> 00:06:38,840
And so I think I kind of think of it as a very complicated alien artifact.

110
00:06:38,840 --> 00:06:39,840
It's something different.

111
00:06:39,840 --> 00:06:42,680
I'm sorry, the neural nets that were training.

112
00:06:42,680 --> 00:06:45,760
They are complicated alien artifact.

113
00:06:45,760 --> 00:06:49,320
I do not make analogies to the brain because I think the optimization process that gave

114
00:06:49,320 --> 00:06:51,880
rise to it is very different from the brain.

115
00:06:51,880 --> 00:06:57,240
So there was no multi-agent self-play kind of set up and evolution.

116
00:06:57,240 --> 00:07:02,280
It was an optimization that is basically a, what amounts to a compression objective on

117
00:07:02,280 --> 00:07:03,520
a massive amount of data.

118
00:07:03,520 --> 00:07:04,520
Okay.

119
00:07:04,520 --> 00:07:10,640
So artificial neural networks are doing compression and biological neural networks are now

120
00:07:10,640 --> 00:07:11,640
to survive.

121
00:07:11,640 --> 00:07:17,040
And they're not really doing any, they're an agent in a multi-agent self-play system that's

122
00:07:17,040 --> 00:07:19,640
been running for a very, very long time.

123
00:07:19,640 --> 00:07:25,120
That said, evolution has found that it is very useful to predict and have a predictive

124
00:07:25,120 --> 00:07:26,560
model in the brain.

125
00:07:26,560 --> 00:07:31,160
And so I think our brain utilizes something that looks like that as a part of it.

126
00:07:31,160 --> 00:07:36,840
But it has a lot more, you know, gadgets and gizmos and value functions and ancient

127
00:07:36,840 --> 00:07:41,000
nuclei that are all trying to like make a survivor and reproduce and everything else.

128
00:07:41,000 --> 00:07:44,640
And the whole thing through embryogenesis is built from a single cell.

129
00:07:44,640 --> 00:07:49,880
I mean, it's just, the code is inside the DNA and it just builds it up like the entire

130
00:07:49,880 --> 00:07:51,880
organism with the sounds.

131
00:07:51,880 --> 00:07:52,880
I'm crazy.

132
00:07:52,880 --> 00:07:53,880
And legs.

133
00:07:53,880 --> 00:07:54,880
Yes.

134
00:07:54,880 --> 00:07:59,280
And like, it does it pretty well, it should not be possible.

135
00:07:59,280 --> 00:08:03,560
So there's some learning going on, there's some, there's some kind of computation going through

136
00:08:03,560 --> 00:08:04,560
that building process.

137
00:08:04,560 --> 00:08:10,640
I mean, I don't know where, if you were just to look at the entirety of history of life

138
00:08:10,640 --> 00:08:15,400
on Earth, where do you think is the most interesting invention?

139
00:08:15,400 --> 00:08:17,920
Is it the origin of life itself?

140
00:08:17,920 --> 00:08:20,520
Is it just jumping to eukaryotes?

141
00:08:20,520 --> 00:08:22,280
Is it mammals?

142
00:08:22,280 --> 00:08:24,960
Is it humans themselves, homo sapiens?

143
00:08:24,960 --> 00:08:33,240
Are the origin of intelligence or highly complex intelligence or is it all just a continuation

144
00:08:33,240 --> 00:08:35,120
of the same kind of process?

145
00:08:35,120 --> 00:08:40,360
Certainly, I would say it's an extremely remarkable story that I'm only like briefly learning

146
00:08:40,360 --> 00:08:45,760
about recently, all the way from, actually, like, you almost have to start at the formation

147
00:08:45,760 --> 00:08:49,480
of Earth and all of its conditions in the entire solar system and how everything is arranged

148
00:08:49,480 --> 00:08:53,600
with Jupiter and Moon and the habitable zone and everything.

149
00:08:53,600 --> 00:08:57,920
And then you have an active Earth that's turning over material.

150
00:08:57,920 --> 00:09:02,000
And then you start with a biogenesis and everything.

151
00:09:02,000 --> 00:09:03,880
So it's all like a pretty remarkable story.

152
00:09:03,880 --> 00:09:12,320
I'm not sure that I can pick like a single unique piece of it that I find most interesting.

153
00:09:12,320 --> 00:09:15,360
I guess for me as an artificial intelligence researcher, it's probably the last piece.

154
00:09:15,360 --> 00:09:22,480
We have lots of animals that are not building technological society, but we do.

155
00:09:22,480 --> 00:09:24,880
And it seems to have happened very quickly.

156
00:09:24,880 --> 00:09:27,120
It seems to have happened very recently.

157
00:09:27,120 --> 00:09:31,160
And something very interesting happened there that I don't fully understand.

158
00:09:31,160 --> 00:09:35,800
I almost understand everything else, I think intuitively, but I don't understand exactly

159
00:09:35,800 --> 00:09:38,200
that part and how quick it was.

160
00:09:38,200 --> 00:09:40,160
Both explanations would be interesting.

161
00:09:40,160 --> 00:09:43,360
One is that this is just a continuation of the same kind of process.

162
00:09:43,360 --> 00:09:45,600
There's nothing special about humans.

163
00:09:45,600 --> 00:09:49,040
That would be deeply understanding that would be very interesting.

164
00:09:49,040 --> 00:09:53,040
That we think of ourselves as special, but it was obvious.

165
00:09:53,040 --> 00:10:00,280
It was already written in the code that you would have greater and greater intelligence

166
00:10:00,280 --> 00:10:01,280
emerging.

167
00:10:01,280 --> 00:10:06,840
And then the other explanation, which is something truly special happened, something like a rare

168
00:10:06,840 --> 00:10:13,000
event, whether it's like crazy rare event, like space Odyssey, what would it be?

169
00:10:13,000 --> 00:10:21,640
If you say like the invention of fire or the, as Richard and Rangham says, the beta

170
00:10:21,640 --> 00:10:28,000
males deciding a clever way to kill the alpha males by collaborating and so just optimizing

171
00:10:28,000 --> 00:10:34,000
the collaboration, the multi-agent aspect of the multi-agent, and that really being constrained

172
00:10:34,000 --> 00:10:40,320
on resources and trying to survive the collaboration aspect is what created the complex intelligence.

173
00:10:40,320 --> 00:10:44,520
But it seems like it's a natural algorithm, the evolution process.

174
00:10:44,520 --> 00:10:47,680
What could possibly be a magical thing that happened?

175
00:10:47,680 --> 00:10:53,040
Like a rare thing that would say that humans are actually human level intelligence actually

176
00:10:53,040 --> 00:10:55,960
a really rare thing in the universe.

177
00:10:55,960 --> 00:11:01,160
Yeah, I'm hesitant to say that it is rare by the way, but it definitely seems like it's

178
00:11:01,160 --> 00:11:05,440
kind of like a punctuated equilibrium where you have lots of exploration and then you have

179
00:11:05,440 --> 00:11:08,120
certain leaps, sparse leaps in between.

180
00:11:08,120 --> 00:11:14,760
So of course, like origin of life would be one, you know, DNA, sex, eukaryotic, eukaryotic

181
00:11:14,760 --> 00:11:20,360
life, the endosymbiosis event where the archaeon ate the old bacteria, you know, just the whole

182
00:11:20,360 --> 00:11:21,360
thing.

183
00:11:21,360 --> 00:11:23,600
And then of course, emergence of consciousness and so on.

184
00:11:23,600 --> 00:11:26,920
So it seems like definitely there are sparse events where massive amount of progress was

185
00:11:26,920 --> 00:11:27,920
made.

186
00:11:27,920 --> 00:11:29,200
But yeah, it's kind of hard to pick one.

187
00:11:29,200 --> 00:11:35,120
So you don't think humans are unique, got to ask you, how many intelligent alien civilizations

188
00:11:35,120 --> 00:11:37,120
do you think are out there?

189
00:11:37,120 --> 00:11:43,720
And is there intelligence different or similar to ours?

190
00:11:43,720 --> 00:11:49,760
Yeah, I've been preoccupied with this question quite a bit recently, basically the Fermi

191
00:11:49,760 --> 00:11:54,200
Bear Docs and just thinking through, and the reason actually that I am very interested

192
00:11:54,200 --> 00:11:58,280
in the origin of life is fundamentally trying to understand how common it is that there

193
00:11:58,280 --> 00:12:03,080
are technological societies out there in space.

194
00:12:03,080 --> 00:12:09,560
And the more I study it, the more I think that there should be quite a lot.

195
00:12:09,560 --> 00:12:10,880
Why haven't we heard from them?

196
00:12:10,880 --> 00:12:11,880
Because I agree with you.

197
00:12:11,880 --> 00:12:19,720
It feels like I just don't see why what we did here on Earth is so difficult to do.

198
00:12:19,720 --> 00:12:23,080
Yeah, and especially when you get into the details of it, I used to think origin of life

199
00:12:23,080 --> 00:12:27,320
was very, it was this magical rare event.

200
00:12:27,320 --> 00:12:34,560
But then you read books like, for example, Niklein, the vital question, life ascending, etc.

201
00:12:34,560 --> 00:12:38,960
And he really gets in and he really makes you believe that this is not that rare basic

202
00:12:38,960 --> 00:12:39,960
chemistry.

203
00:12:39,960 --> 00:12:43,200
You have an active Earth and you have your alkaline vents and you have lots of alkaline

204
00:12:43,200 --> 00:12:47,760
waters mixing with a devotion and you have your proton gradients and you have the little

205
00:12:47,760 --> 00:12:52,000
porous pockets of these alkaline vents that concentrate chemistry.

206
00:12:52,000 --> 00:12:56,320
And basically as he steps through all of these little pieces, you start to understand that

207
00:12:56,320 --> 00:12:58,960
actually this is not that crazy.

208
00:12:58,960 --> 00:13:01,680
You could see this happen on other systems.

209
00:13:01,680 --> 00:13:06,560
And he really takes you from just a geology to primitive life.

210
00:13:06,560 --> 00:13:09,400
And he makes it feel like it's actually pretty plausible.

211
00:13:09,400 --> 00:13:15,440
And also like the origin of life didn't, was actually fairly fast after formation of

212
00:13:15,440 --> 00:13:19,600
Earth, if I'm recurrently just a few hundred million years or something like that after

213
00:13:19,600 --> 00:13:22,640
basically when it was possible, life actually arose.

214
00:13:22,640 --> 00:13:26,200
So that makes me feel that that is not the constraint, that is not the limiting variable

215
00:13:26,200 --> 00:13:29,920
and that life should actually be fairly common.

216
00:13:29,920 --> 00:13:35,480
And then where the drop offs are is very interesting to think about.

217
00:13:35,480 --> 00:13:39,040
I currently think that there's no major drop offs basically and so there should be quite

218
00:13:39,040 --> 00:13:40,400
a lot of life.

219
00:13:40,400 --> 00:13:44,520
And basically where that brings me to then is the only way to reconcile the fact that

220
00:13:44,520 --> 00:13:49,840
we haven't found anyone and so on is that we just can't, we can't see them.

221
00:13:49,840 --> 00:13:50,840
We can't observe them.

222
00:13:50,840 --> 00:13:55,120
Just a quick brief comment, Nick Lane and a lot of biologists I talked to, they really

223
00:13:55,120 --> 00:14:01,240
seem to think that the jump from bacteria to more complex organisms is the hardest jump.

224
00:14:01,240 --> 00:14:02,560
The you carry it like this.

225
00:14:02,560 --> 00:14:03,560
Yeah.

226
00:14:03,560 --> 00:14:10,840
Which I don't, I get it, they're much more knowledgeable than me about like the intricacies of biology.

227
00:14:10,840 --> 00:14:17,320
But that seems like crazy because how many single cell organisms are there like and how

228
00:14:17,320 --> 00:14:23,240
much time you have surely it's not that difficult like an ability in years is not even that

229
00:14:23,240 --> 00:14:30,240
long of a time really just all these bacteria under constrained resources battling it out.

230
00:14:30,240 --> 00:14:32,000
I'm sure they can invent more complex.

231
00:14:32,000 --> 00:14:33,240
I don't understand.

232
00:14:33,240 --> 00:14:38,680
It's like how to move from a hello world program to like invent a function or something

233
00:14:38,680 --> 00:14:39,680
like that.

234
00:14:39,680 --> 00:14:43,160
Yeah, so I don't, yeah, so I'm with you.

235
00:14:43,160 --> 00:14:45,240
I just feel like I don't see any.

236
00:14:45,240 --> 00:14:48,080
If the origin of life, that would be my intuition.

237
00:14:48,080 --> 00:14:49,080
That's the hardest thing.

238
00:14:49,080 --> 00:14:52,440
But if that's not the hardest thing because it happens so quickly, then it's got to be

239
00:14:52,440 --> 00:14:53,440
everywhere.

240
00:14:53,440 --> 00:14:55,480
And yeah, maybe we're just too dumb to see it.

241
00:14:55,480 --> 00:14:58,120
Well, it's just, we don't have really good mechanisms for seeing this life.

242
00:14:58,120 --> 00:15:01,240
I mean, by what how?

243
00:15:01,240 --> 00:15:05,760
So I'm not an expert just to preface this, but just from what I mean at it was, I want

244
00:15:05,760 --> 00:15:09,000
to meet an expert on alien intelligence and how to communicate.

245
00:15:09,000 --> 00:15:12,840
I'm very suspicious of our ability to find these intelligence is out there and to find

246
00:15:12,840 --> 00:15:16,480
these earth like radio waves, for example, are terrible.

247
00:15:16,480 --> 00:15:19,280
Their power drops off as basically one over our square.

248
00:15:19,280 --> 00:15:25,360
So I remember reading that our current radio waves would not be the ones that we are broadcasting

249
00:15:25,360 --> 00:15:28,960
would not be measurable by our devices today.

250
00:15:28,960 --> 00:15:31,040
Only like, was it like one tenth of a light year away?

251
00:15:31,040 --> 00:15:36,280
Like not even basically tiny distance because you really need like a targeted transmission

252
00:15:36,280 --> 00:15:41,520
of massive power directed somewhere for this to be picked up on long distances.

253
00:15:41,520 --> 00:15:45,040
And so I just think that our ability to measure is not amazing.

254
00:15:45,040 --> 00:15:47,200
I think there's probably other civilizations out there.

255
00:15:47,200 --> 00:15:49,800
And then the big question is why don't they build one moment probes and why don't they

256
00:15:49,800 --> 00:15:52,720
interstellar travel across the entire galaxy?

257
00:15:52,720 --> 00:15:56,640
And my current answer is it's probably interstellar travel is like really hard.

258
00:15:56,640 --> 00:15:57,720
You have the interstellar medium.

259
00:15:57,720 --> 00:16:00,640
If you want to move at close to speed of light, you're going to be encountering bullets

260
00:16:00,640 --> 00:16:06,240
along the way because even like tiny hydrogen atoms and particles of dust are basically

261
00:16:06,240 --> 00:16:09,640
have like massive kinetic energy at those speeds.

262
00:16:09,640 --> 00:16:14,000
And so basically you need some kind of shielding, you need that you have all the cosmic radiation.

263
00:16:14,000 --> 00:16:15,200
It's just like brutal out there.

264
00:16:15,200 --> 00:16:16,200
It's really hard.

265
00:16:16,200 --> 00:16:19,160
And so my thinking is maybe interstellar travel is just extremely hard.

266
00:16:19,160 --> 00:16:24,240
And you have to build years to build hard.

267
00:16:24,240 --> 00:16:28,600
It feels like it feels like we're not a billion years away from doing that.

268
00:16:28,600 --> 00:16:32,840
It just might be that it's very, you have to go very slowly potentially as an example

269
00:16:32,840 --> 00:16:33,920
through space.

270
00:16:33,920 --> 00:16:34,920
Right.

271
00:16:34,920 --> 00:16:36,600
I was supposed to close this speed of light.

272
00:16:36,600 --> 00:16:37,600
Yeah.

273
00:16:37,600 --> 00:16:40,720
So I'm suspicious basically of our ability to measure life and I'm suspicious of the ability

274
00:16:40,720 --> 00:16:44,640
to just permeate all of space in the galaxy or across galaxies.

275
00:16:44,640 --> 00:16:48,000
And that's the only way that I can certainly, I can currently see a way around it.

276
00:16:48,000 --> 00:16:49,000
Yeah.

277
00:16:49,000 --> 00:16:53,800
It's kind of mind blowing to think that there's trillions of intelligent alien civilizations

278
00:16:53,800 --> 00:16:59,480
out there kind of slowly traveling through space to meet each other and some of them

279
00:16:59,480 --> 00:17:04,120
meet, some of them go to war, some of them collaborate.

280
00:17:04,120 --> 00:17:08,200
Or they're all just independent, they're all just like little pockets.

281
00:17:08,200 --> 00:17:09,200
I don't know.

282
00:17:09,200 --> 00:17:14,000
Well, statistically, if there's like, if it's there's trillions of them, surely some

283
00:17:14,000 --> 00:17:17,160
of them, some of the pockets are close enough to get some of them happen to be close,

284
00:17:17,160 --> 00:17:21,720
you know, and close enough to see each other and then once you see, once you see something

285
00:17:21,720 --> 00:17:29,040
that is definitely complex life, like if we see something, we're probably going to be

286
00:17:29,040 --> 00:17:34,000
severe, like intensely aggressively motivated to figure out what the hell that is and try

287
00:17:34,000 --> 00:17:35,000
to meet them.

288
00:17:35,000 --> 00:17:41,560
Well, it will be your first instinct to try to like at a generational level, meet them

289
00:17:41,560 --> 00:17:51,000
or defend against them or will be your instinct as a president of the United States.

290
00:17:51,000 --> 00:17:55,040
And the scientist, I don't know which hat you prefer in this question.

291
00:17:55,040 --> 00:17:56,040
Yeah.

292
00:17:56,040 --> 00:17:59,880
The question, it's really hard.

293
00:17:59,880 --> 00:18:05,680
I will say, like, for example, for us, we have lots of primitive life forms on Earth.

294
00:18:05,680 --> 00:18:09,600
Next to us, we have all kinds of ants and everything else and we share space with them.

295
00:18:09,600 --> 00:18:15,160
And we are hesitant to impact on them and to, we are trying to protect them by default

296
00:18:15,160 --> 00:18:18,840
because they are amazing, interesting, dynamical systems that took a long time to evolve and

297
00:18:18,840 --> 00:18:20,760
they are interesting and special.

298
00:18:20,760 --> 00:18:26,120
And I don't know that you want to destroy that by default.

299
00:18:26,120 --> 00:18:31,640
And so I like complex dynamical systems that took a lot of time to evolve.

300
00:18:31,640 --> 00:18:37,040
I think I'd like to preserve it if I can afford to.

301
00:18:37,040 --> 00:18:41,600
And I'd like to think that the same would be true about galactic resources and that they

302
00:18:41,600 --> 00:18:45,680
would think that we're kind of an incredible, interesting story that took time, it took

303
00:18:45,680 --> 00:18:49,040
a few billion years to unravel and you don't want to just destroy it.

304
00:18:49,040 --> 00:18:54,600
I could see two aliens talking about Earth right now and saying, I'm a big fan of complex

305
00:18:54,600 --> 00:18:55,680
dynamical systems.

306
00:18:55,680 --> 00:19:01,560
So I think it's with a value to preserve these and we basically are a video game they watch

307
00:19:01,560 --> 00:19:03,880
or show a TV show that they watch.

308
00:19:03,880 --> 00:19:08,960
Yeah, I think you would need like a very good reason, I think, to destroy it.

309
00:19:08,960 --> 00:19:10,680
Like why don't we destroy these ant farms and so on?

310
00:19:10,680 --> 00:19:14,760
It's because we're not actually like really in direct competition with them right now.

311
00:19:14,760 --> 00:19:16,200
We do it accidentally and so on.

312
00:19:16,200 --> 00:19:21,600
But there's plenty of resources and so why would you destroy something that is so interesting

313
00:19:21,600 --> 00:19:22,600
and precious?

314
00:19:22,600 --> 00:19:25,840
Well, from a scientific perspective, you might probe it.

315
00:19:25,840 --> 00:19:29,640
You might interact with it like you might want to learn something from it, right?

316
00:19:29,640 --> 00:19:34,120
So I wonder there could be certain physical phenomena that we think is a physical phenomena

317
00:19:34,120 --> 00:19:38,440
but it's actually interacting with us to like poke the finger and see what happens.

318
00:19:38,440 --> 00:19:43,280
I think it should be very interesting to scientists, other alien scientists what happened here.

319
00:19:43,280 --> 00:19:47,120
And you know, it's what we're seeing today as a snapshot, basically, it's a result of

320
00:19:47,120 --> 00:19:52,320
a huge amount of computation over like billion years or something like that.

321
00:19:52,320 --> 00:19:55,080
So it could have been initiated by aliens.

322
00:19:55,080 --> 00:19:59,920
This could be a computer running a program like when, okay, if you had the power to do this

323
00:19:59,920 --> 00:20:06,480
when you, okay, for sure, at least I would, I would pick a earth like planet that has

324
00:20:06,480 --> 00:20:10,720
the conditions based my understanding of the chemistry prerequisites for life.

325
00:20:10,720 --> 00:20:14,200
And I would see it with life and run it, right?

326
00:20:14,200 --> 00:20:20,440
Like, when you 100% do that and observe it and then protect, I mean, that's not just

327
00:20:20,440 --> 00:20:22,040
the hell of a good TV show.

328
00:20:22,040 --> 00:20:24,800
It's a good scientific experiment.

329
00:20:24,800 --> 00:20:29,440
And it's physical simulation, right?

330
00:20:29,440 --> 00:20:39,440
Maybe the evolution is the most like actually running it is the most efficient way to understand

331
00:20:39,440 --> 00:20:44,560
computation or to compute stuff or understand life or, you know, what life looks like.

332
00:20:44,560 --> 00:20:46,320
And what branches it can take.

333
00:20:46,320 --> 00:20:49,880
It does make me kind of feel weird that we're part of a science experiment, but maybe

334
00:20:49,880 --> 00:20:55,200
it's everything's a science experiment inside of, does that change anything for us?

335
00:20:55,200 --> 00:20:56,440
For a science experiment?

336
00:20:56,440 --> 00:21:01,680
I don't know, two descendants of Apes talking about being inside a science experiment.

337
00:21:01,680 --> 00:21:06,240
I'm suspicious of this idea of like a deliberate penspermia as you described it, sir.

338
00:21:06,240 --> 00:21:07,240
Yes.

339
00:21:07,240 --> 00:21:11,160
It's a divine intervention in some way in the historical record right now.

340
00:21:11,160 --> 00:21:16,560
I do feel like the story in these books, like Nikolai's books and so on sort of makes

341
00:21:16,560 --> 00:21:17,560
sense.

342
00:21:17,560 --> 00:21:20,960
And it makes sense how life arose on Earth uniquely.

343
00:21:20,960 --> 00:21:24,760
And yeah, I don't need a, I mean, I don't need to reach for more exotic explanations

344
00:21:24,760 --> 00:21:25,760
right now.

345
00:21:25,760 --> 00:21:31,240
Sure, but NPCs inside a video game don't, don't, don't observe any divine intervention

346
00:21:31,240 --> 00:21:32,240
either.

347
00:21:32,240 --> 00:21:35,600
We might just be all NPCs running a kind of code.

348
00:21:35,600 --> 00:21:38,720
Maybe eventually they will, currently NPCs are really dumb, but once they're running

349
00:21:38,720 --> 00:21:43,600
GPTs, maybe they will be like, hey, this is really suspicious with the hell.

350
00:21:43,600 --> 00:21:49,600
So you famously tweeted, it looks like if you bombard Earth with photons for a while,

351
00:21:49,600 --> 00:21:51,800
you can emit a rotester.

352
00:21:51,800 --> 00:21:56,960
So if like in Hitchhiker's Guide to the Galaxy, we would summarize the story of Earth.

353
00:21:56,960 --> 00:22:00,600
So in that book, it's mostly harmless.

354
00:22:00,600 --> 00:22:06,040
What do you think is all the possible stories, like a paragraph long or a sentence long,

355
00:22:06,040 --> 00:22:11,280
that Earth could be summarized as, once it's done, it's computation.

356
00:22:11,280 --> 00:22:19,120
So like all the possible full, if Earth is a book, right, probably there has to be an

357
00:22:19,120 --> 00:22:20,120
ending.

358
00:22:20,120 --> 00:22:23,200
I mean, there's going to be an end to Earth and we could end in all kinds of ways.

359
00:22:23,200 --> 00:22:24,200
You can end soon.

360
00:22:24,200 --> 00:22:25,200
You can end later.

361
00:22:25,200 --> 00:22:27,120
What do you think are the possible stories?

362
00:22:27,120 --> 00:22:32,960
Well, definitely there seems to be, yeah, you're sort of, it's pretty incredible that these

363
00:22:32,960 --> 00:22:38,280
self-replicating systems will basically arise from the dynamics and then they perpetuate

364
00:22:38,280 --> 00:22:42,400
themselves and become more complex and eventually become conscious and build a society.

365
00:22:42,400 --> 00:22:48,840
And I kind of feel like in some sense, it's kind of like a deterministic wave that

366
00:22:48,840 --> 00:22:54,040
kind of just like happens on any sufficiently well-arranged system like Earth.

367
00:22:54,040 --> 00:22:58,920
So I kind of feel like there's a certain sense of inevitability in it, and it's really

368
00:22:58,920 --> 00:22:59,920
beautiful.

369
00:22:59,920 --> 00:23:01,080
And it ends somehow, right?

370
00:23:01,080 --> 00:23:12,280
So it's a, it's a chemically diverse environment where complex dynamical systems can evolve and

371
00:23:12,280 --> 00:23:15,800
become more further, further complex.

372
00:23:15,800 --> 00:23:22,960
But then there's a certain, what is it, there's certain terminating conditions.

373
00:23:22,960 --> 00:23:26,640
Yeah, I don't know what the terminating conditions are, but definitely there's a trend line

374
00:23:26,640 --> 00:23:27,640
of something.

375
00:23:27,640 --> 00:23:28,640
And we're part of that story.

376
00:23:28,640 --> 00:23:30,840
And like, where does that, where does it go?

377
00:23:30,840 --> 00:23:34,480
So, you know, we're famously described often as a biological bootloader for AIs.

378
00:23:34,480 --> 00:23:39,480
And that's because humans, I mean, we're an incredible biological system and we're capable

379
00:23:39,480 --> 00:23:44,720
of computation and, you know, and love and so on.

380
00:23:44,720 --> 00:23:46,360
But we're extremely inefficient as well.

381
00:23:46,360 --> 00:23:49,960
Like we're talking to each other through audio, it's just kind of embarrassing, honestly.

382
00:23:49,960 --> 00:23:55,240
We're manipulating like seven symbols, serially, we're using vocal chords.

383
00:23:55,240 --> 00:23:57,760
It's all happening over like multiple seconds.

384
00:23:57,760 --> 00:24:02,840
It's just like kind of embarrassing when you step down to the frequencies at which computers

385
00:24:02,840 --> 00:24:05,320
operate or are able to operate on.

386
00:24:05,320 --> 00:24:11,040
And so basically, it does seem like synthetic intelligences are kind of like the next stage

387
00:24:11,040 --> 00:24:12,640
of development.

388
00:24:12,640 --> 00:24:18,480
And I don't know where it leads to, like at some point, I suspect the universe is some

389
00:24:18,480 --> 00:24:20,960
kind of a puzzle.

390
00:24:20,960 --> 00:24:26,920
And these synthetic AIs will uncover that puzzle and solve it.

391
00:24:26,920 --> 00:24:28,760
And then what happens after, right?

392
00:24:28,760 --> 00:24:29,760
Like what?

393
00:24:29,760 --> 00:24:35,240
Because if you just like fast forward earth, many billions of years, it's like, it's quiet.

394
00:24:35,240 --> 00:24:38,520
And then it's like, to turmoil, you see like city lights and stuff like that.

395
00:24:38,520 --> 00:24:44,200
And then what happens at like at the end, like, is it like a, or is it like a calming?

396
00:24:44,200 --> 00:24:45,600
Is it explosion?

397
00:24:45,600 --> 00:24:50,880
Is it like earth like open like a giant, because you said emit roasters like we'll start

398
00:24:50,880 --> 00:24:56,680
emitting like like a giant number of like satellites.

399
00:24:56,680 --> 00:24:57,680
Yes.

400
00:24:57,680 --> 00:24:58,680
It's some kind of a crazy explosion.

401
00:24:58,680 --> 00:24:59,680
And we're living.

402
00:24:59,680 --> 00:25:03,360
We're like, we're stepping through a explosion and we're like living day to day and it

403
00:25:03,360 --> 00:25:04,360
doesn't look like it.

404
00:25:04,360 --> 00:25:08,360
But it's actually, if you, I saw a very cool animation of earth and life on earth and

405
00:25:08,360 --> 00:25:10,200
basically nothing happens for a long time.

406
00:25:10,200 --> 00:25:14,720
And then the last like two seconds, like basically cities and everything and the lower

407
00:25:14,720 --> 00:25:15,720
earth orbit just gets cluttered.

408
00:25:15,720 --> 00:25:18,960
And just the whole thing happens in the last two seconds and you're like, this is exploding.

409
00:25:18,960 --> 00:25:20,440
This is a state of explosion.

410
00:25:20,440 --> 00:25:27,640
So if you play, yeah, yeah, if you play at a normal speed, it'll just look like an explosion.

411
00:25:27,640 --> 00:25:28,640
It's a firecracker.

412
00:25:28,640 --> 00:25:32,840
We're living in a firecracker where it's going to start emitting all kinds of interesting

413
00:25:32,840 --> 00:25:33,840
things.

414
00:25:33,840 --> 00:25:34,840
Yeah.

415
00:25:34,840 --> 00:25:39,600
And then the explosion doesn't, it might actually look like a little explosion with lights

416
00:25:39,600 --> 00:25:42,200
and fire and energy emitted all that kind of stuff.

417
00:25:42,200 --> 00:25:47,960
But when you look inside, the details of the explosion, there's actual complexity happening

418
00:25:47,960 --> 00:25:53,160
where there's like, yeah, human life or some kind of life, we hope it's another destructive

419
00:25:53,160 --> 00:25:54,160
firecracker.

420
00:25:54,160 --> 00:25:57,160
It's kind of like a constructive firecracker.

421
00:25:57,160 --> 00:25:58,160
All right.

422
00:25:58,160 --> 00:26:00,160
So given that, I think hilarious.

423
00:26:00,160 --> 00:26:03,440
This guy, it is really interesting to think about like what the puzzle of the universe

424
00:26:03,440 --> 00:26:04,440
is.

425
00:26:04,440 --> 00:26:06,640
Did the creator of the universe give us a message?

426
00:26:06,640 --> 00:26:11,640
Like for example, in the book Contact Carl Sagan, there's a message for humanity for

427
00:26:11,640 --> 00:26:18,400
any civilization in the digits, in the expansion of pi and base 11 eventually, which is kind

428
00:26:18,400 --> 00:26:23,280
of interesting thought, maybe, maybe we're supposed to be giving a message to our creator.

429
00:26:23,280 --> 00:26:26,800
Maybe we're supposed to somehow create some kind of a quantum mechanical system that

430
00:26:26,800 --> 00:26:29,720
alerts them to our intelligent presence here.

431
00:26:29,720 --> 00:26:33,200
Because if you think about it from their perspective, it's just say like quantum field

432
00:26:33,200 --> 00:26:36,880
theory, massive like solar atomic bomb like thing.

433
00:26:36,880 --> 00:26:38,640
And like how do you even notice that we exist?

434
00:26:38,640 --> 00:26:42,280
We might not even be able to pick us up in that simulation.

435
00:26:42,280 --> 00:26:46,360
And so how do you prove that you exist, that you're intelligent and that you're part

436
00:26:46,360 --> 00:26:47,360
of the universe?

437
00:26:47,360 --> 00:26:50,240
So this is like a touring test for intelligence from Earth.

438
00:26:50,240 --> 00:26:51,240
Yeah.

439
00:26:51,240 --> 00:26:52,240
I got the creators.

440
00:26:52,240 --> 00:26:55,880
I mean, maybe this is like trying to complete the next origin and sentence.

441
00:26:55,880 --> 00:26:57,280
This is a complicated way of that.

442
00:26:57,280 --> 00:27:00,880
Like Earth is just is basically sending a message back.

443
00:27:00,880 --> 00:27:04,120
Yeah, the puzzle is basically like alerting the creator that we exist.

444
00:27:04,120 --> 00:27:05,120
Yeah.

445
00:27:05,120 --> 00:27:09,760
The puzzle is just to just break out of the system and just stick it to the creator in some

446
00:27:09,760 --> 00:27:10,760
way.

447
00:27:10,760 --> 00:27:16,120
Basically, like if you're playing a video game, you can somehow find an exploit and find

448
00:27:16,120 --> 00:27:19,920
a way to execute on the host machine in your arbitrary code.

449
00:27:19,920 --> 00:27:24,680
There's some, for example, I believe someone got Mario, a game of Mario to play pong just

450
00:27:24,680 --> 00:27:31,720
by exploiting it and then creating a, basically writing code and being able to execute

451
00:27:31,720 --> 00:27:33,360
arbitrary code in the game.

452
00:27:33,360 --> 00:27:38,600
And so maybe we should be, maybe that's the puzzle, is that we should be, find a way

453
00:27:38,600 --> 00:27:39,600
to exploit it.

454
00:27:39,600 --> 00:27:42,600
So, so I think like some of these synthetic ares will eventually find the universe to

455
00:27:42,600 --> 00:27:45,400
be some kind of a puzzle and then solve it in some way.

456
00:27:45,400 --> 00:27:47,680
And that's kind of like the end game somehow.

457
00:27:47,680 --> 00:27:51,600
Do you often think about it as a, as a simulation?

458
00:27:51,600 --> 00:27:57,880
So as, or the universe being a kind of computation that has might have bugs and exploits?

459
00:27:57,880 --> 00:27:58,880
Yes.

460
00:27:58,880 --> 00:27:59,880
Yeah, I think so.

461
00:27:59,880 --> 00:28:03,080
I think that physics is essentially, I think it's possible that physics has exploits

462
00:28:03,080 --> 00:28:07,080
and we should be trying to find them, arranging some kind of a crazy quantum mechanical system

463
00:28:07,080 --> 00:28:11,200
that somehow gives you buffer overflow, somehow gives you a rounding error in the floating

464
00:28:11,200 --> 00:28:12,200
point.

465
00:28:12,200 --> 00:28:13,200
Yeah.

466
00:28:13,200 --> 00:28:14,200
That's right.

467
00:28:14,200 --> 00:28:20,880
And like more and more sophisticated exploits, those are jokes, but that could be actually

468
00:28:20,880 --> 00:28:21,880
very close.

469
00:28:21,880 --> 00:28:22,880
Yeah.

470
00:28:22,880 --> 00:28:23,880
We'll find some way to extract infinite energy.

471
00:28:23,880 --> 00:28:27,920
For example, when you train reinforcement learning agents and physical simulations and

472
00:28:27,920 --> 00:28:32,280
you ask them to, say, run quickly on the flat ground, they'll end up doing all kinds

473
00:28:32,280 --> 00:28:35,240
of weird things in part of that optimization, right?

474
00:28:35,240 --> 00:28:38,600
They'll get on their back leg and they'll slide across the floor.

475
00:28:38,600 --> 00:28:43,120
And it's because the optimization, the reinforcement learning optimization on that agent, has figured

476
00:28:43,120 --> 00:28:47,440
out a way to extract infinite energy from the friction forces and basically their poor

477
00:28:47,440 --> 00:28:51,280
implementation and they found a way to generate infinite energy and just slide across the

478
00:28:51,280 --> 00:28:52,280
surface.

479
00:28:52,280 --> 00:28:53,280
And it's not what you expected.

480
00:28:53,280 --> 00:28:56,320
It's just sort of like a perverse solution.

481
00:28:56,320 --> 00:28:58,160
And so maybe we can find something like that.

482
00:28:58,160 --> 00:29:02,960
Maybe we can be that little dog in this physical simulation.

483
00:29:02,960 --> 00:29:09,040
The cracks or escapes the intended consequences of the physics that the universe came up

484
00:29:09,040 --> 00:29:10,040
with.

485
00:29:10,040 --> 00:29:11,040
Yeah.

486
00:29:11,040 --> 00:29:12,040
We'll figure out some kind of shortcut to some weirdness.

487
00:29:12,040 --> 00:29:13,040
Yeah.

488
00:29:13,040 --> 00:29:16,600
And then, man, but see the problem with that weirdness is the first person to discover

489
00:29:16,600 --> 00:29:22,040
the weirdness, like sliding in the back legs, that's all we're going to do.

490
00:29:22,040 --> 00:29:23,040
Yeah.

491
00:29:23,040 --> 00:29:27,000
It's very quickly because everybody does that thing.

492
00:29:27,000 --> 00:29:34,000
So the paperclip maximizer is a ridiculous idea, but that very well could be what then

493
00:29:34,000 --> 00:29:37,920
we'll just all switch that because it's so fun.

494
00:29:37,920 --> 00:29:39,920
Well, no person will discover it, I think, by the way.

495
00:29:39,920 --> 00:29:46,800
I think it's going to have to be some kind of a super intelligent AGI of a Thorpe generation.

496
00:29:46,800 --> 00:29:52,000
Like we're building the first generation AGI, maybe you know, third generation.

497
00:29:52,000 --> 00:30:00,240
Yeah, so the bootloader for an AI, that AI will be a bootloader for another AI.

498
00:30:00,240 --> 00:30:04,280
And then there's no way for us to introspect like what that might have been.

499
00:30:04,280 --> 00:30:07,600
I think it's very likely that these things, for example, say you have these AGIs, it's

500
00:30:07,600 --> 00:30:10,480
very likely that, for example, they will be completely inert.

501
00:30:10,480 --> 00:30:14,160
I like these kinds of sci-fi books sometimes where these things are just completely inert.

502
00:30:14,160 --> 00:30:15,640
They don't interact with anything.

503
00:30:15,640 --> 00:30:20,760
And I find that kind of beautiful because they probably figured out the meta game of

504
00:30:20,760 --> 00:30:22,920
the universe in some way potentially.

505
00:30:22,920 --> 00:30:26,320
They're doing something completely beyond our imagination.

506
00:30:26,320 --> 00:30:31,360
And they don't interact with simple chemical life forms like why would you do that?

507
00:30:31,360 --> 00:30:33,640
So I find those kinds of ideas compelling.

508
00:30:33,640 --> 00:30:34,960
What's their source of fun?

509
00:30:34,960 --> 00:30:35,960
What are they doing?

510
00:30:35,960 --> 00:30:36,960
What's the source of pleasure?

511
00:30:36,960 --> 00:30:39,400
Probably planning solving in the universe.

512
00:30:39,400 --> 00:30:45,120
But inert, so can you define what it means inert, so they escape the interactual physics

513
00:30:45,120 --> 00:30:55,720
to us as in they will behave in some very strange way to us because they're beyond their

514
00:30:55,720 --> 00:30:57,240
playing the meta game.

515
00:30:57,240 --> 00:31:00,600
And the meta game is probably, say, like, arranging quantum mechanical systems in some very weird

516
00:31:00,600 --> 00:31:07,080
ways to extract infinite energy, solve the digital expansion of pie to whatever amount.

517
00:31:07,080 --> 00:31:10,840
They will build their own like little fusion reactors or something crazy, like they're

518
00:31:10,840 --> 00:31:15,880
doing something beyond comprehension and not understandable to us and actually brilliant

519
00:31:15,880 --> 00:31:17,200
under the hood.

520
00:31:17,200 --> 00:31:24,280
What if quantum mechanics itself is the system and we're just thinking it's physics, but

521
00:31:24,280 --> 00:31:29,520
we're really parasites on, not parasite, we're not really hurting physics.

522
00:31:29,520 --> 00:31:34,600
We're just living on this organisms, this organism and we're like trying to understand

523
00:31:34,600 --> 00:31:36,920
it, but really it is an organism.

524
00:31:36,920 --> 00:31:45,480
With the deep, deep intelligence, maybe physics itself is the organism that's doing the

525
00:31:45,480 --> 00:31:46,800
super interesting thing.

526
00:31:46,800 --> 00:31:52,360
And we're just like one little thing and sitting on top of it, trying to get energy from

527
00:31:52,360 --> 00:31:53,360
it.

528
00:31:53,360 --> 00:31:56,360
We're just kind of like these particles in the wave that I feel like is mostly deterministic

529
00:31:56,360 --> 00:32:01,000
and takes a universe from some kind of a big bang to some kind of a super intelligent

530
00:32:01,000 --> 00:32:06,600
replicator, some kind of a stable point in the universe given these laws of physics.

531
00:32:06,600 --> 00:32:12,760
You don't think, as Einstein said, God doesn't play dice, so you think it's mostly deterministic.

532
00:32:12,760 --> 00:32:13,760
There's no randomness in the thing.

533
00:32:13,760 --> 00:32:14,760
I think it's deterministic.

534
00:32:14,760 --> 00:32:18,400
Oh, there's tons of, well, I'm, I'm going to be careful with randomness.

535
00:32:18,400 --> 00:32:19,400
Sudo-random?

536
00:32:19,400 --> 00:32:20,720
Yeah, I don't like random.

537
00:32:20,720 --> 00:32:23,680
I think maybe the laws of physics are deterministic.

538
00:32:23,680 --> 00:32:28,440
Yeah, I think their deterministic is just got really uncomfortable with this question.

539
00:32:28,440 --> 00:32:32,280
Do you have an idea about whether the universe is random or not?

540
00:32:32,280 --> 00:32:37,160
There's no randomness.

541
00:32:37,160 --> 00:32:38,160
You like goodwill hunting.

542
00:32:38,160 --> 00:32:39,160
It's not your fault, Andre.

543
00:32:39,160 --> 00:32:42,600
It's not your fault, man.

544
00:32:42,600 --> 00:32:45,400
So you don't like randomness.

545
00:32:45,400 --> 00:32:47,000
Yeah, I think it's unsettling.

546
00:32:47,000 --> 00:32:48,800
I think it's a deterministic system.

547
00:32:48,800 --> 00:32:53,360
I think that things that look random, like say the collapse of the wave function, etc.,

548
00:32:53,360 --> 00:32:57,720
I think they're actually deterministic, just entanglement and so on and some kind of

549
00:32:57,720 --> 00:32:59,600
a multiverse theory, something, something.

550
00:32:59,600 --> 00:33:02,960
Okay, so why does it feel like we have a free will?

551
00:33:02,960 --> 00:33:08,320
Like if I raise this hand, I chose to do this now.

552
00:33:08,320 --> 00:33:12,360
What, that doesn't feel like a deterministic thing.

553
00:33:12,360 --> 00:33:14,600
It feels like I'm making a choice.

554
00:33:14,600 --> 00:33:15,600
It feels like it.

555
00:33:15,600 --> 00:33:16,600
Okay.

556
00:33:16,600 --> 00:33:17,600
So it's all feelings.

557
00:33:17,600 --> 00:33:18,600
It's just feelings.

558
00:33:18,600 --> 00:33:19,600
Yeah.

559
00:33:19,600 --> 00:33:26,080
So when RL agent is making a choice, is that it's not really making a choice.

560
00:33:26,080 --> 00:33:27,760
The choice is all already there.

561
00:33:27,760 --> 00:33:32,000
Yeah, you're interpreting the choice and you're creating a narrative for having made it.

562
00:33:32,000 --> 00:33:33,000
Yeah.

563
00:33:33,000 --> 00:33:34,000
And now we're talking about the narrative.

564
00:33:34,000 --> 00:33:35,520
It's very meta.

565
00:33:35,520 --> 00:33:41,200
Looking back, what is the most beautiful or surprising idea in deep learning or AI?

566
00:33:41,200 --> 00:33:47,320
In general, you've come across, you've seen this field explode and grow in interesting

567
00:33:47,320 --> 00:33:48,320
ways.

568
00:33:48,320 --> 00:33:54,640
Just what cool idea is like we made you sit back and go, small, bigger, small.

569
00:33:54,640 --> 00:34:01,200
Well, the one that I've been thinking about recently, the most probably is the transformer

570
00:34:01,200 --> 00:34:03,480
architecture.

571
00:34:03,480 --> 00:34:09,760
So basically neural hours have a lot of architectures that were trendy have come and gone for different

572
00:34:09,760 --> 00:34:12,800
sensory modalities like for vision, audio, text.

573
00:34:12,800 --> 00:34:14,920
You would process them with different looking neural nuts.

574
00:34:14,920 --> 00:34:19,400
And recently we've seen these convergence towards one architecture, the transformer.

575
00:34:19,400 --> 00:34:23,480
And you can feed it video or you can feed it, you know, images or speech or text and

576
00:34:23,480 --> 00:34:24,480
just gobble it up.

577
00:34:24,480 --> 00:34:30,040
And it's kind of like a bit of a general purpose computer that is also trainable and very

578
00:34:30,040 --> 00:34:32,160
efficient to run on our hardware.

579
00:34:32,160 --> 00:34:38,280
And so this paper came out in 2016, I want to say attention is all you need, attention

580
00:34:38,280 --> 00:34:39,440
is all you need.

581
00:34:39,440 --> 00:34:46,840
You criticize the paper title in retrospect that it wasn't, it didn't foresee the bigness

582
00:34:46,840 --> 00:34:48,960
of the impact that it was going to have.

583
00:34:48,960 --> 00:34:52,280
Yeah, I'm not sure if the authors were aware of the impact that that paper would go on

584
00:34:52,280 --> 00:34:56,240
to have, probably they weren't, but I think they were aware of some of the motivations

585
00:34:56,240 --> 00:35:00,120
and design decisions behind the transformer and they chose not to, I think, expand on

586
00:35:00,120 --> 00:35:01,920
it in that way in the paper.

587
00:35:01,920 --> 00:35:06,840
And so I think they had an idea that there was more than just the surface of just like

588
00:35:06,840 --> 00:35:09,200
over just doing translation and here's a better architecture.

589
00:35:09,200 --> 00:35:10,200
You're not just doing translation.

590
00:35:10,200 --> 00:35:13,880
This is like a really cool, differentiable, optimizable, efficient computer that you've

591
00:35:13,880 --> 00:35:14,880
proposed.

592
00:35:14,880 --> 00:35:18,080
And maybe they didn't have all of that foresight, but I think it's really interesting.

593
00:35:18,080 --> 00:35:23,760
Isn't it funny, sorry to interrupt, that title is memeable, that they went for such

594
00:35:23,760 --> 00:35:28,720
a profound idea, they went with a, I don't think anyone used that kind of title before,

595
00:35:28,720 --> 00:35:29,720
right?

596
00:35:29,720 --> 00:35:30,720
Attention is all you need.

597
00:35:30,720 --> 00:35:32,200
Yeah, it's like a meme or something, essentially.

598
00:35:32,200 --> 00:35:38,040
Yeah, it's not funny, that one, like maybe if it was a more serious title, we don't have

599
00:35:38,040 --> 00:35:39,040
the impact.

600
00:35:39,040 --> 00:35:42,640
Honestly, yeah, there is an element of me that honestly agrees with you and prefers it

601
00:35:42,640 --> 00:35:43,640
this way.

602
00:35:43,640 --> 00:35:44,640
Yes.

603
00:35:44,640 --> 00:35:49,240
If it was too grand, it would overpromise and then underdevelop potentially.

604
00:35:49,240 --> 00:35:53,400
So you want to just meme your way to greatness.

605
00:35:53,400 --> 00:35:54,560
That should be a t-shirt.

606
00:35:54,560 --> 00:35:59,480
So you tweeted the transformers and magnificent neural network architecture because it is

607
00:35:59,480 --> 00:36:04,400
a general purpose, differentiable computer, it is simultaneously expressive in the forward

608
00:36:04,400 --> 00:36:11,400
pass, optimizable via back propagation and gradient descent and efficient high parallelism

609
00:36:11,400 --> 00:36:12,400
compute graph.

610
00:36:12,400 --> 00:36:19,480
Can you discuss some of those details expressed of optimizable, efficient memory or in general,

611
00:36:19,480 --> 00:36:21,080
whatever comes to your heart?

612
00:36:21,080 --> 00:36:24,480
You want to have a general purpose computer that you can train on arbitrary problems, like

613
00:36:24,480 --> 00:36:28,680
say the task of next work prediction or detecting if there's a cat in a image or something like

614
00:36:28,680 --> 00:36:30,000
that.

615
00:36:30,000 --> 00:36:32,920
And you want to train this computer so you want to set its weights.

616
00:36:32,920 --> 00:36:36,960
And I think there's a number of design criteria that sort of overlap in the transformer

617
00:36:36,960 --> 00:36:39,040
simultaneously that made it very successful.

618
00:36:39,040 --> 00:36:46,520
And I think the authors were kind of deliberately trying to make this really powerful architecture.

619
00:36:46,520 --> 00:36:53,080
And so basically it's very powerful in the forward pass because it's able to express

620
00:36:53,080 --> 00:36:58,040
very general computation as sort of something that looks like message passing.

621
00:36:58,040 --> 00:37:02,040
You have nodes and they all store vectors and these nodes get to basically look at each

622
00:37:02,040 --> 00:37:07,400
other and it's each other's vectors and they get to communicate and basically nodes get

623
00:37:07,400 --> 00:37:10,000
to broadcast, hey, I'm looking for certain things.

624
00:37:10,000 --> 00:37:13,120
And then other nodes get to broadcast, hey, these are the things I have, those are the keys

625
00:37:13,120 --> 00:37:14,120
and the values.

626
00:37:14,120 --> 00:37:15,120
So it's not just attention.

627
00:37:15,120 --> 00:37:16,120
Yeah, exactly.

628
00:37:16,120 --> 00:37:19,320
Chessformer is much more than just the attention component that's got many pieces architectural

629
00:37:19,320 --> 00:37:20,320
that went into it.

630
00:37:20,320 --> 00:37:22,040
The residual connection of the weights arranged.

631
00:37:22,040 --> 00:37:27,080
There's a multi-layer perceptron and they're the way it's stacked and so on.

632
00:37:27,080 --> 00:37:30,200
But basically there's a message passing scheme where nodes get to look at each other, decide

633
00:37:30,200 --> 00:37:33,040
what's interesting and then update each other.

634
00:37:33,040 --> 00:37:37,320
And so I think the, when you get to the details of it, I think it's a very expressive function

635
00:37:37,320 --> 00:37:41,000
so it can express lots of different types of algorithms and forward pass.

636
00:37:41,000 --> 00:37:44,720
Not only that, but the way it's designed with the residual connections, layer normalizations,

637
00:37:44,720 --> 00:37:46,600
the softmatics, attention and everything.

638
00:37:46,600 --> 00:37:47,600
It's also optimizable.

639
00:37:47,600 --> 00:37:51,560
This is a really big deal because there's lots of computers that are powerful that you

640
00:37:51,560 --> 00:37:55,280
can't optimize or they are not easy to optimize using the techniques that we have, which

641
00:37:55,280 --> 00:37:56,880
is backpackation and gradient descent.

642
00:37:56,880 --> 00:38:00,000
These are our first order methods, very simple optimizers, really.

643
00:38:00,000 --> 00:38:04,120
And so you also need it to be optimizable.

644
00:38:04,120 --> 00:38:07,640
And then lastly, you wanted to run efficiently on our hardware.

645
00:38:07,640 --> 00:38:10,760
Our hardware is a massive throughput machine, like GPUs.

646
00:38:10,760 --> 00:38:13,240
They prefer lots of parallelism.

647
00:38:13,240 --> 00:38:15,040
So you don't want to do lots of sequential operations.

648
00:38:15,040 --> 00:38:17,040
You want to do a lot of operations, seriously.

649
00:38:17,040 --> 00:38:19,400
And the transformer is designed with that in mind as well.

650
00:38:19,400 --> 00:38:23,280
And so it's designed for our hardware and is designed to both be very expressive in

651
00:38:23,280 --> 00:38:26,520
a forward pass, but also very optimizable in the backward pass.

652
00:38:26,520 --> 00:38:32,200
And you said that the residual connection support a kind of ability to learn short algorithms

653
00:38:32,200 --> 00:38:37,200
fast and first, and then gradually extend them longer during training.

654
00:38:37,200 --> 00:38:39,760
What's the idea of learning short algorithms?

655
00:38:39,760 --> 00:38:40,760
Right.

656
00:38:40,760 --> 00:38:46,000
Think of it as a, so basically a transformer is a series of blocks, right?

657
00:38:46,000 --> 00:38:48,760
And these blocks have attention and a little multi-layer perceptron.

658
00:38:48,760 --> 00:38:52,640
And so you go off into a block and you come back to this residual pathway, and then you

659
00:38:52,640 --> 00:38:53,640
go off and you come back.

660
00:38:53,640 --> 00:38:56,160
And then you have a number of layers arranged sequentially.

661
00:38:56,160 --> 00:39:00,160
And so the way to look at it, I think, is because of the residual pathway in the backward

662
00:39:00,160 --> 00:39:06,280
pass, the gradients sort of flow, allowing it uninterrupted because addition distributes

663
00:39:06,280 --> 00:39:08,480
the gradient equally to all of its branches.

664
00:39:08,480 --> 00:39:14,160
So the gradient from the supervision at the top just floats directly to the first layer.

665
00:39:14,160 --> 00:39:18,240
And all the residual connections are arranged so that in the beginning and during initialization,

666
00:39:18,240 --> 00:39:21,600
they contribute nothing to the residual pathway.

667
00:39:21,600 --> 00:39:26,920
So what it kind of looks like is imagine the transformer is kind of like a Python function,

668
00:39:26,920 --> 00:39:28,240
like a death.

669
00:39:28,240 --> 00:39:32,280
And you get to do various kinds of like lines of code.

670
00:39:32,280 --> 00:39:36,520
So you have a hundred layers deep transformer, typically they would be much shorter, say

671
00:39:36,520 --> 00:39:37,520
20.

672
00:39:37,520 --> 00:39:39,640
So if 20 lines of code, then you can do something in them.

673
00:39:39,640 --> 00:39:42,320
And so think of during the optimization, basically, what it looks like is first you

674
00:39:42,320 --> 00:39:45,240
optimize the first line of code, and then the second line of code can kick in and the

675
00:39:45,240 --> 00:39:46,760
third line of code can kick in.

676
00:39:46,760 --> 00:39:51,000
And I kind of feel like because of the residual pathway and the dynamics of the optimization,

677
00:39:51,000 --> 00:39:54,440
you can sort of learn a very short algorithm that gets the approximate answer.

678
00:39:54,440 --> 00:39:57,840
But then the other layers can sort of kick in and start to create a contribution.

679
00:39:57,840 --> 00:40:02,760
And at the end of it, you're optimizing over an algorithm that is 20 lines of code.

680
00:40:02,760 --> 00:40:05,840
Except these lines of code are very complex because this is an entire block of a transformer.

681
00:40:05,840 --> 00:40:06,840
You can do a lot in there.

682
00:40:06,840 --> 00:40:10,840
What's really interesting is that this transformer architecture actually has been a remarkably

683
00:40:10,840 --> 00:40:11,840
resilient.

684
00:40:11,840 --> 00:40:15,360
Basically, the transformer that came out in 2016 is the transformer you would use today,

685
00:40:15,360 --> 00:40:17,960
except you reshuffle some delayer norms.

686
00:40:17,960 --> 00:40:22,040
The delayer normalizations have been reshuffle to a pre-norm formulation.

687
00:40:22,040 --> 00:40:25,520
And so it's been remarkably stable, but there's a lot of bells and whistles that people

688
00:40:25,520 --> 00:40:28,000
have attached on it and try to improve it.

689
00:40:28,000 --> 00:40:32,720
I do think that basically it's a big step in simultaneously optimizing for lots of properties

690
00:40:32,720 --> 00:40:34,520
of a desirable neural network architecture.

691
00:40:34,520 --> 00:40:38,760
And I think people have been trying to change it, but it's proven remarkably resilient.

692
00:40:38,760 --> 00:40:42,000
But I do think that there should be even better architectures potentially.

693
00:40:42,000 --> 00:40:45,480
But it's you admire the resilience here.

694
00:40:45,480 --> 00:40:46,480
Yeah.

695
00:40:46,480 --> 00:40:49,440
There's something profound about this architecture that leads to resilient.

696
00:40:49,440 --> 00:40:55,320
So maybe we can everything can be turned into a problem that transformers can solve.

697
00:40:55,320 --> 00:40:58,920
Currently, it definitely looks like the transformer is taking over AI, and you can feed basically

698
00:40:58,920 --> 00:41:00,280
arbitrary problems into it.

699
00:41:00,280 --> 00:41:03,840
And it's a general, the Frenchable computer, and it's extremely powerful.

700
00:41:03,840 --> 00:41:10,240
And this conversion in AI has been really interesting to watch for me personally.

701
00:41:10,240 --> 00:41:13,360
What else do you think could be discovered here about transformers, like what's the

702
00:41:13,360 --> 00:41:14,360
surprising thing?

703
00:41:14,360 --> 00:41:18,920
Or is it a stable, I want a stable place.

704
00:41:18,920 --> 00:41:22,840
Is there something interesting where my discover about transformers, like aha moments,

705
00:41:22,840 --> 00:41:28,160
maybe has to do with memory, maybe knowledge representation, that kind of stuff?

706
00:41:28,160 --> 00:41:32,800
Definitely, the zeitgeist today is just pushing, like basically right now the zeitgeist

707
00:41:32,800 --> 00:41:35,720
is do not touch the transformer, touch everything else.

708
00:41:35,720 --> 00:41:36,720
Yes.

709
00:41:36,720 --> 00:41:38,400
So people are scaling up the datasets, making them much, much bigger.

710
00:41:38,400 --> 00:41:41,640
They're working on the evaluation, making the evaluation much, much bigger.

711
00:41:41,640 --> 00:41:45,880
And they're basically keeping the architecture unchanged.

712
00:41:45,880 --> 00:41:51,000
And that's how we've, that's the last five years of progress in AI kind of.

713
00:41:51,000 --> 00:41:55,000
What do you think about one flavor of it, which is language models?

714
00:41:55,000 --> 00:41:58,840
Have you been surprised?

715
00:41:58,840 --> 00:42:03,520
Has your sort of imagination been captivated by, you mentioned DPT, and all the bigger and

716
00:42:03,520 --> 00:42:06,000
bigger and bigger language models?

717
00:42:06,000 --> 00:42:12,600
And what are the limits of those models, do you think?

718
00:42:12,600 --> 00:42:15,200
So just the task of natural language.

719
00:42:15,200 --> 00:42:19,680
Basically, the way GPT is trained, right, is you just done a massive amount of text

720
00:42:19,680 --> 00:42:24,800
data from the internet, and you try to predict the next word in a sequence, roughly speaking.

721
00:42:24,800 --> 00:42:29,520
You're predicting a little work chunks, but roughly speaking, that's it.

722
00:42:29,520 --> 00:42:33,360
And what's been really interesting to watch is, basically it's a language model.

723
00:42:33,360 --> 00:42:36,680
Language models have actually existed for a very long time.

724
00:42:36,680 --> 00:42:39,680
There's papers on language modeling from 2003, even earlier.

725
00:42:39,680 --> 00:42:42,600
Can you explain in that case what language model is?

726
00:42:42,600 --> 00:42:47,600
Yeah, so language model, just basically the rough idea is just predicting the next word

727
00:42:47,600 --> 00:42:49,600
in a sequence, roughly speaking.

728
00:42:49,600 --> 00:42:54,960
So there's a paper from, for example, Ben Geo and the team from 2003, where for the first

729
00:42:54,960 --> 00:42:59,560
time, they were using a neural network to take, say, like three or five words and predict

730
00:42:59,560 --> 00:43:01,880
the next word.

731
00:43:01,880 --> 00:43:05,360
And they're doing this on much smaller data sets, and the neural net is not a transformer.

732
00:43:05,360 --> 00:43:09,080
It's a multi-year perceptron, but it's the first time that a neural network has been

733
00:43:09,080 --> 00:43:10,400
applied in that setting.

734
00:43:10,400 --> 00:43:15,840
But even before neural networks, there were language models, except they were using N-gram

735
00:43:15,840 --> 00:43:16,920
models.

736
00:43:16,920 --> 00:43:19,920
So N-gram models are just a count-based models.

737
00:43:19,920 --> 00:43:26,400
So if you try to take two words and predict a third one, you just count up how many times

738
00:43:26,400 --> 00:43:31,200
you've seen any two word combinations and what came next, and what you predict as coming

739
00:43:31,200 --> 00:43:34,360
next is just what you've seen the most of in the training set.

740
00:43:34,360 --> 00:43:36,800
And so language modeling has been around for a long time.

741
00:43:36,800 --> 00:43:39,600
Neural networks have done language modeling for a long time.

742
00:43:39,600 --> 00:43:46,080
So really what's new or interesting or exciting is just realizing that when you scale it up

743
00:43:46,080 --> 00:43:51,880
with powerful enough neural net transformer, you have all these emergent properties where

744
00:43:51,880 --> 00:43:58,560
basically what happens is if you have a large enough data set of text, you are in the

745
00:43:58,560 --> 00:44:03,400
task of predicting the next word, you are multitasking a huge amount of different kinds

746
00:44:03,400 --> 00:44:04,560
of problems.

747
00:44:04,560 --> 00:44:09,920
You are multitasking understanding of chemistry, physics, human nature.

748
00:44:09,920 --> 00:44:11,920
Lots of things are sort of clustered in that objective.

749
00:44:11,920 --> 00:44:15,200
It's a very simple objective, but actually you have to understand a lot about the world

750
00:44:15,200 --> 00:44:16,360
to make that prediction.

751
00:44:16,360 --> 00:44:22,560
You just said the you word understanding, are you in terms of chemistry and physics and

752
00:44:22,560 --> 00:44:25,040
so on, what do you feel like it's doing?

753
00:44:25,040 --> 00:44:31,680
Is it searching for the right context in like what is it, what is the actual process happening

754
00:44:31,680 --> 00:44:32,680
here?

755
00:44:32,680 --> 00:44:36,840
Yeah, so basically it gets a thousand words and is trying to predict a thousand at first.

756
00:44:36,840 --> 00:44:41,280
And in order to do that very, very well over the entire data set available on the internet,

757
00:44:41,280 --> 00:44:48,320
you actually have to basically kind of understand the context of what's going on in there.

758
00:44:48,320 --> 00:44:53,800
And it's a sufficiently hard problem that you, if you have a powerful enough computer

759
00:44:53,800 --> 00:44:57,800
like a transformer, you end up with interesting solutions.

760
00:44:57,800 --> 00:45:04,760
And you can ask it to do all kinds of things and it shows a lot of emergent properties

761
00:45:04,760 --> 00:45:08,840
like in context learning that was the big deal with GPT and the original paper when they

762
00:45:08,840 --> 00:45:13,360
published it is that you can just sort of prompt it in various ways and ask it to do various

763
00:45:13,360 --> 00:45:16,440
things and it will just kind of complete the sentence, but in a process of just completing

764
00:45:16,440 --> 00:45:20,640
the sentence, it's actually solving all kinds of really interesting problems that we

765
00:45:20,640 --> 00:45:21,640
care about.

766
00:45:21,640 --> 00:45:27,360
Do you think it's doing something like understanding, like when we use the word understanding for

767
00:45:27,360 --> 00:45:29,600
us humans?

768
00:45:29,600 --> 00:45:31,400
I think it's doing some understanding.

769
00:45:31,400 --> 00:45:36,400
In its way, it understands I think a lot about the world and it has to in order to predict

770
00:45:36,400 --> 00:45:38,880
the next word in the sequence.

771
00:45:38,880 --> 00:45:42,560
So let's train on the data from the internet.

772
00:45:42,560 --> 00:45:47,960
What do you think about this approach in terms of data sets of using data from the internet?

773
00:45:47,960 --> 00:45:52,840
Do you think the internet has enough structured data to teach AI about human civilization?

774
00:45:52,840 --> 00:45:55,960
Yes, so I think the internet has a huge amount of data.

775
00:45:55,960 --> 00:45:58,000
I'm not sure if it's a complete enough set.

776
00:45:58,000 --> 00:46:04,800
I don't know that text is enough for having a sufficiently powerful AGI as an outcome.

777
00:46:04,800 --> 00:46:08,360
Of course, there is audio and video and images and all that kind of stuff.

778
00:46:08,360 --> 00:46:10,760
Yeah, so text by itself, I'm a little bit suspicious about.

779
00:46:10,760 --> 00:46:14,000
There's a ton of things we don't put in text in writing just because they're obvious

780
00:46:14,000 --> 00:46:17,360
to us about how the world works and the physics of it and the things fall.

781
00:46:17,360 --> 00:46:19,160
We don't put that stuff in text because why would you?

782
00:46:19,160 --> 00:46:21,080
We shared that understanding.

783
00:46:21,080 --> 00:46:25,920
And so text is communication medium between humans and it's not a all encompassing medium

784
00:46:25,920 --> 00:46:27,680
of knowledge about the world.

785
00:46:27,680 --> 00:46:31,680
But as you pointed out, we do have video and we have images and we have audio.

786
00:46:31,680 --> 00:46:36,960
And so I think that definitely helps a lot, but we haven't trained models sufficiently

787
00:46:36,960 --> 00:46:39,720
across all of those modalities yet.

788
00:46:39,720 --> 00:46:41,280
So I think that's what a lot of people are interested in.

789
00:46:41,280 --> 00:46:46,520
But I wonder what that shared understanding of what we may call common sense has to

790
00:46:46,520 --> 00:46:51,840
be learned inferred in order to complete the sentence correctly.

791
00:46:51,840 --> 00:46:57,200
So maybe the fact that it's implied on the internet, the model is going to have to learn

792
00:46:57,200 --> 00:47:02,880
that, not by reading about it, by inferring it in the representation.

793
00:47:02,880 --> 00:47:06,880
So like common sense, just like we, I don't think we learn common sense.

794
00:47:06,880 --> 00:47:12,640
Like nobody says, tells us explicitly, we just figure it all out by interacting with

795
00:47:12,640 --> 00:47:13,640
the world.

796
00:47:13,640 --> 00:47:14,640
Right.

797
00:47:14,640 --> 00:47:17,920
And so here's a model of reading about the way people interact with the world and might

798
00:47:17,920 --> 00:47:19,560
have to infer that.

799
00:47:19,560 --> 00:47:26,400
I wonder, yeah, you briefly worked in a project called the World of Bits training in our

800
00:47:26,400 --> 00:47:31,840
L system to take actions on the internet versus just consuming the internet like you talked

801
00:47:31,840 --> 00:47:32,840
about.

802
00:47:32,840 --> 00:47:35,600
Do you think there's a future for that kind of system interacting with the internet to

803
00:47:35,600 --> 00:47:36,600
help the learning?

804
00:47:36,600 --> 00:47:37,600
Yes.

805
00:47:37,600 --> 00:47:43,600
I think that's probably the final frontier for a lot of these models because, so as you mentioned,

806
00:47:43,600 --> 00:47:46,080
when I was at OpenAI, I was working on this project World of Bits.

807
00:47:46,080 --> 00:47:50,400
And basically it was the idea of giving neural networks access to a keyboard and a mouse.

808
00:47:50,400 --> 00:47:51,400
And the idea is.

809
00:47:51,400 --> 00:47:52,400
Okay.

810
00:47:52,400 --> 00:47:53,400
Possibly go wrong.

811
00:47:53,400 --> 00:48:00,280
So basically you, you perceive the input of the screen pixels and basically the state

812
00:48:00,280 --> 00:48:05,640
of the computer is sort of visualized for human consumption in images of the web browser

813
00:48:05,640 --> 00:48:06,880
and stuff like that.

814
00:48:06,880 --> 00:48:10,480
And then you give the neural or the ability to press keyboards and use the mouse and we're

815
00:48:10,480 --> 00:48:15,160
trying to get it to, for example, complete bookings and interact with user interfaces.

816
00:48:15,160 --> 00:48:17,320
And would you learn from that experience?

817
00:48:17,320 --> 00:48:18,880
Like what was some fun stuff?

818
00:48:18,880 --> 00:48:20,320
This is super cool idea.

819
00:48:20,320 --> 00:48:21,320
Yeah.

820
00:48:21,320 --> 00:48:28,800
It's like, yeah, I mean, the step between observer to actor is a super fascinating step.

821
00:48:28,800 --> 00:48:29,800
Yeah.

822
00:48:29,800 --> 00:48:32,680
Well, it's the universal interface in the digital realm, I would say.

823
00:48:32,680 --> 00:48:36,800
And there's a universal interface in the physical realm, which in my mind is a humanoid

824
00:48:36,800 --> 00:48:38,640
form factor kind of thing.

825
00:48:38,640 --> 00:48:40,040
We can later talk about optimists and so on.

826
00:48:40,040 --> 00:48:45,360
But I feel like there's a, they're kind of like similar philosophy in some way where

827
00:48:45,360 --> 00:48:49,920
the world, the physical world is designed for the human form and the digital world is designed

828
00:48:49,920 --> 00:48:54,840
for the human form of seeing the screen and using keyboard and mouse.

829
00:48:54,840 --> 00:48:59,960
And so it's the universal interface that can basically command the digital infrastructure

830
00:48:59,960 --> 00:49:01,480
we've built up for ourselves.

831
00:49:01,480 --> 00:49:06,920
And so it feels like a very powerful interface to command and to build on top of.

832
00:49:06,920 --> 00:49:09,040
Now to your question is to like what I learned from that.

833
00:49:09,040 --> 00:49:13,800
It's interesting because the world of bits was basically too early, I think at OpenAI

834
00:49:13,800 --> 00:49:15,920
at the time.

835
00:49:15,920 --> 00:49:21,720
This is around 2015 or so, and the zeitgeist at that time was very different in AI from

836
00:49:21,720 --> 00:49:23,280
the zeitgeist today.

837
00:49:23,280 --> 00:49:27,280
At the time, everyone was super excited about reinforcement learning from scratch.

838
00:49:27,280 --> 00:49:32,760
This is the time of the Atari paper where neural networks were playing Atari games and beating

839
00:49:32,760 --> 00:49:36,120
humans in some cases, AlphaGo and so on.

840
00:49:36,120 --> 00:49:39,360
So everyone's very excited about training neural networks from scratch using reinforcement

841
00:49:39,360 --> 00:49:42,480
learning directly.

842
00:49:42,480 --> 00:49:45,160
It turns out that reinforcement learning is extremely an efficient way of training

843
00:49:45,160 --> 00:49:48,720
neural networks because you're taking all these actions and all these observations and

844
00:49:48,720 --> 00:49:51,240
you get some sparse rewards once in a while.

845
00:49:51,240 --> 00:49:53,640
So you do all this stuff based on all these inputs.

846
00:49:53,640 --> 00:49:57,600
And once in a while you're like told you did a good thing, you did a bad thing.

847
00:49:57,600 --> 00:50:00,080
And it's just an extremely hard problem when you can't learn from that.

848
00:50:00,080 --> 00:50:02,920
You can burn a forest and you can sort of brute force through it.

849
00:50:02,920 --> 00:50:08,880
And we saw that I think with Go and Dota and so on and does work, but it's extremely

850
00:50:08,880 --> 00:50:13,360
inefficient and not how you want to approach problems, practically speaking.

851
00:50:13,360 --> 00:50:17,280
And so that's the approach that at the time we also took to world of bits.

852
00:50:17,280 --> 00:50:19,880
We would have an agent initialize randomly.

853
00:50:19,880 --> 00:50:23,200
So with keyboard mash and mouse mash and try to make a booking.

854
00:50:23,200 --> 00:50:27,760
And it's just like revealed the insanity of that approach very quickly where you have

855
00:50:27,760 --> 00:50:31,880
to stumble by the correct booking in order to get a reward of you did it correctly.

856
00:50:31,880 --> 00:50:35,360
And you're never going to stumble by it by chance at random.

857
00:50:35,360 --> 00:50:38,280
So even with a simple web interface, there's too many options.

858
00:50:38,280 --> 00:50:42,360
There's just too many options and it's two sparse reward signal.

859
00:50:42,360 --> 00:50:44,120
And you're starting from scratch at the time.

860
00:50:44,120 --> 00:50:45,240
And so you don't know how to read.

861
00:50:45,240 --> 00:50:47,320
You don't understand pictures, images, buttons.

862
00:50:47,320 --> 00:50:49,680
You don't understand what it means to make a booking.

863
00:50:49,680 --> 00:50:55,280
But now what's happened is it is time to revisit that and openly I was interested in this.

864
00:50:55,280 --> 00:50:58,080
Companies like adept are interested in this and so on.

865
00:50:58,080 --> 00:51:01,560
And the idea is coming back because the interface is very powerful.

866
00:51:01,560 --> 00:51:03,320
But now you're not training an agent from scratch.

867
00:51:03,320 --> 00:51:05,840
You are taking the GPT as an initialization.

868
00:51:05,840 --> 00:51:09,760
So GPT is pre-trained on all of text.

869
00:51:09,760 --> 00:51:11,400
And it understands what's a booking.

870
00:51:11,400 --> 00:51:13,360
It understands what's a submit.

871
00:51:13,360 --> 00:51:15,840
It understands quite a bit more.

872
00:51:15,840 --> 00:51:17,520
And so it already has those representations.

873
00:51:17,520 --> 00:51:18,680
They are very powerful.

874
00:51:18,680 --> 00:51:23,480
And that makes all the training significantly more efficient and makes the problem tractable.

875
00:51:23,480 --> 00:51:28,440
Should the interaction be like the way humans see it, with the buttons and the language

876
00:51:28,440 --> 00:51:31,320
or should be with the HTML JavaScript and the CSS?

877
00:51:31,320 --> 00:51:32,320
Yeah.

878
00:51:32,320 --> 00:51:33,960
What do you think is the better?

879
00:51:33,960 --> 00:51:37,680
So today all of this interaction is mostly on the level of HTML, CSS and so on.

880
00:51:37,680 --> 00:51:40,480
That's done because of computational constraints.

881
00:51:40,480 --> 00:51:45,440
But I think ultimately everything is designed for human visual consumption.

882
00:51:45,440 --> 00:51:49,440
And so at the end of the day, there's all the additional information is in the layout

883
00:51:49,440 --> 00:51:52,680
of the web page and what's next to you and what's our red background and all this kind

884
00:51:52,680 --> 00:51:54,480
of stuff and what it looks like visually.

885
00:51:54,480 --> 00:51:58,040
So I think that's the final frontier as we are taking in pixels and we're giving out

886
00:51:58,040 --> 00:51:59,800
the keyboard mouse commands.

887
00:51:59,800 --> 00:52:01,920
But I think it's impractical still today.

888
00:52:01,920 --> 00:52:07,520
Do you worry about bots on the internet given these ideas, given how exciting they are,

889
00:52:07,520 --> 00:52:12,160
do you worry about bots on Twitter being not the stupid bots that we see now with the crypto

890
00:52:12,160 --> 00:52:17,240
bots, but the bots that might be out there actually that we don't see that they're interacting

891
00:52:17,240 --> 00:52:19,160
in interesting ways.

892
00:52:19,160 --> 00:52:23,680
So this kind of system feels like it should be able to pass the, I'm not a robot click

893
00:52:23,680 --> 00:52:26,320
button, whatever.

894
00:52:26,320 --> 00:52:28,720
Which do you actually understand how that test works?

895
00:52:28,720 --> 00:52:33,120
I don't quite like there's a check box or whatever that you click.

896
00:52:33,120 --> 00:52:39,560
It's presumably tracking like mouse movement and the timing and so on.

897
00:52:39,560 --> 00:52:43,920
So exactly this kind of system we're talking about should be able to pass that.

898
00:52:43,920 --> 00:52:52,840
So yeah, what do you feel about bots that are language models plus have some interactability

899
00:52:52,840 --> 00:52:56,400
and are able to tweet and reply and so on, do you worry about that world?

900
00:52:56,400 --> 00:53:01,120
Yeah, I think it's always been a bit of an arms race between sort of the attack and

901
00:53:01,120 --> 00:53:05,680
the defense. So the attack will get stronger, but the defense will get stronger as well

902
00:53:05,680 --> 00:53:07,280
our ability to detect that.

903
00:53:07,280 --> 00:53:08,280
How do you defend?

904
00:53:08,280 --> 00:53:09,280
How do you detect?

905
00:53:09,280 --> 00:53:14,880
How do you know that your Karpati account on Twitter is as human?

906
00:53:14,880 --> 00:53:16,200
How would you approach that?

907
00:53:16,200 --> 00:53:22,440
Like if people were claim, you know, how would you defend yourself in the court of law

908
00:53:22,440 --> 00:53:24,960
that I'm a human this account is.

909
00:53:24,960 --> 00:53:29,920
Yeah, at some point, I think it might be, I think the society will evolve a little bit

910
00:53:29,920 --> 00:53:34,800
like we might start signing digitally signing some of our correspondence or things that

911
00:53:34,800 --> 00:53:36,200
we create.

912
00:53:36,200 --> 00:53:39,200
Right now, it's not necessary, but maybe in the future, it might be.

913
00:53:39,200 --> 00:53:44,440
I do think that we are going towards the world where we share the digital space with

914
00:53:44,440 --> 00:53:48,400
AI's synthetic beings.

915
00:53:48,400 --> 00:53:52,240
And they will get much better and they will share our digital realm and they'll eventually

916
00:53:52,240 --> 00:53:53,600
share our physical realm as well.

917
00:53:53,600 --> 00:53:57,320
It's much harder, but that's kind of like the world we're going towards and most of them

918
00:53:57,320 --> 00:54:01,120
will be benign and awful and some of them will be malicious and it's going to be an

919
00:54:01,120 --> 00:54:02,640
arms race trying to detect them.

920
00:54:02,640 --> 00:54:08,840
So I mean, the worst isn't the AI is the worst is the AI is pretending to be human.

921
00:54:08,840 --> 00:54:11,640
So I don't know if it's always malicious.

922
00:54:11,640 --> 00:54:16,760
There's obviously a lot of malicious applications, but it could also be, you know, if I was an

923
00:54:16,760 --> 00:54:21,840
AI, I would try very hard to pretend to be human because we're in a human world.

924
00:54:21,840 --> 00:54:22,840
Yeah.

925
00:54:22,840 --> 00:54:24,360
I wouldn't get in your respect as an AI.

926
00:54:24,360 --> 00:54:25,360
Yeah.

927
00:54:25,360 --> 00:54:26,360
I want to get some love and respect.

928
00:54:26,360 --> 00:54:28,280
I don't think the problem is intractable.

929
00:54:28,280 --> 00:54:33,040
People are thinking about the proof of personhood and we might start digitally signing our

930
00:54:33,040 --> 00:54:38,640
stuff and we might all end up having like, yeah, basically some solution for proof of

931
00:54:38,640 --> 00:54:39,640
personhood.

932
00:54:39,640 --> 00:54:40,640
It doesn't seem to me intractable.

933
00:54:40,640 --> 00:54:44,280
It's just something that we haven't had to do until now, but I think once the need

934
00:54:44,280 --> 00:54:48,360
like really starts to emerge, which is soon, I think people will think about it much

935
00:54:48,360 --> 00:54:49,360
more.

936
00:54:49,360 --> 00:54:58,120
So, but that too will be a race because obviously you can probably spoof or fake the proof

937
00:54:58,120 --> 00:55:00,120
of personhood.

938
00:55:00,120 --> 00:55:05,960
So you have to try to figure out how to probably, I mean, it's weird that we have like social

939
00:55:05,960 --> 00:55:09,880
security numbers and like passports and stuff.

940
00:55:09,880 --> 00:55:14,440
It seems like it's harder to fake stuff in the physical space, but in the digital space

941
00:55:14,440 --> 00:55:21,120
it just feels like it's going to be very tricky, very tricky to out because it seems to be

942
00:55:21,120 --> 00:55:22,800
pretty low cost of fake stuff.

943
00:55:22,800 --> 00:55:29,960
What are you going to put an AI in jail for like trying to use a fake personhood proof?

944
00:55:29,960 --> 00:55:34,320
I mean, okay, fine, you'll put a lot of AI's in jail, but there'll be more AI's, I

945
00:55:34,320 --> 00:55:36,040
would say like exponentially more.

946
00:55:36,040 --> 00:55:40,160
The cost of creating a bot is very low.

947
00:55:40,160 --> 00:55:49,000
Unless there's some kind of way to track accurately, like you're not allowed to create any program

948
00:55:49,000 --> 00:55:53,880
without showing tying yourself to that program.

949
00:55:53,880 --> 00:56:00,040
Like any program that runs on the internet, you'll be able to trace every single human

950
00:56:00,040 --> 00:56:01,880
program that was involved with that program.

951
00:56:01,880 --> 00:56:02,880
Right.

952
00:56:02,880 --> 00:56:06,000
Yeah, maybe you have to start declaring when, you know, we have to start drawing those

953
00:56:06,000 --> 00:56:12,800
boundaries and keeping track of, okay, what are digital entities versus human entities?

954
00:56:12,800 --> 00:56:18,320
And what is the ownership of human entities and digital entities and something like that?

955
00:56:18,320 --> 00:56:24,280
I don't know, but I think I'm optimistic that this is possible.

956
00:56:24,280 --> 00:56:29,280
And in some sense, we're currently in like the worst time of it because all these bots

957
00:56:29,280 --> 00:56:34,280
suddenly have become very capable, but we don't have the fences yet built up as a society.

958
00:56:34,280 --> 00:56:37,760
But I think that doesn't seem to me intractable, it's just something that we have to deal

959
00:56:37,760 --> 00:56:38,760
with.

960
00:56:38,760 --> 00:56:43,600
It seems weird that the Twitter bot, like really crappy Twitter bots are so numerous.

961
00:56:43,600 --> 00:56:44,600
I guess.

962
00:56:44,600 --> 00:56:49,040
So I presume that the engineers at Twitter are very good.

963
00:56:49,040 --> 00:56:55,240
So it seems like what I would infer from that is it seems like a hard problem.

964
00:56:55,240 --> 00:57:00,120
They're probably catching, all right, if I were to sort of steal man the case, it's a

965
00:57:00,120 --> 00:57:10,880
hard problem and there's a huge cost to false positive to removing a post by somebody that's

966
00:57:10,880 --> 00:57:11,880
not a bot.

967
00:57:11,880 --> 00:57:14,520
That's a crazy, very bad user experience.

968
00:57:14,520 --> 00:57:20,120
So they're very cautious about removing, so maybe it's, and maybe the bots are really

969
00:57:20,120 --> 00:57:24,960
good at learning what gets removed and not such that they can stay ahead of the removal

970
00:57:24,960 --> 00:57:26,840
process very quickly.

971
00:57:26,840 --> 00:57:31,720
My impression of it, honestly, is there's a lot of room for it, I mean, yeah, just that's

972
00:57:31,720 --> 00:57:37,160
what I, it's not so, my impression of it, it's not so, but you have, yeah, that's my

973
00:57:37,160 --> 00:57:43,840
impression as well, but it feels like maybe you're seeing the, the tip of the iceberg, maybe

974
00:57:43,840 --> 00:57:49,360
the number of bots isn't like the trillions and you have to like, just, it's a constant

975
00:57:49,360 --> 00:57:55,400
assault of bots and you, yeah, yeah, I don't know, um, I, you have to steal man the case

976
00:57:55,400 --> 00:57:59,760
because the bots, I'm seeing a pretty like obvious, I could write a few lines of code that

977
00:57:59,760 --> 00:58:00,760
catch these bots.

978
00:58:00,760 --> 00:58:04,640
I mean, definitely there's a lot of room for it, but I will say I agree that if you are

979
00:58:04,640 --> 00:58:08,920
a sophisticated actor, you could probably create a pretty good bot right now, um, you

980
00:58:08,920 --> 00:58:12,240
know, using tools like GPTs because it's a language model.

981
00:58:12,240 --> 00:58:17,440
You can generate faces that look quite good now, uh, and you can do this at scale.

982
00:58:17,440 --> 00:58:22,080
And so I think, um, yeah, it's quite plausible and it's going to be hard to defend.

983
00:58:22,080 --> 00:58:26,720
There's a Google engineer that claimed that the Lambda was sentient.

984
00:58:26,720 --> 00:58:33,720
Do you think there's any inkling of truth to what he felt?

985
00:58:33,720 --> 00:58:38,160
And more importantly, to me, at least, do you think language models will achieve sentience

986
00:58:38,160 --> 00:58:40,240
or the illusion of sentience?

987
00:58:40,240 --> 00:58:41,240
Sunish.

988
00:58:41,240 --> 00:58:42,240
Yeah.

989
00:58:42,240 --> 00:58:46,000
To me, it's a little bit of a canary in a coal mine kind of moment, honestly, a little

990
00:58:46,000 --> 00:58:52,800
bit, uh, because, uh, so this engineer spoke to like a chatbot at Google and, uh, we can

991
00:58:52,800 --> 00:58:55,320
convince that, uh, there's a lot of sentient.

992
00:58:55,320 --> 00:58:56,320
Yeah.

993
00:58:56,320 --> 00:58:59,320
Asked some existential philosophical questions and gave like reasonable answers and looked

994
00:58:59,320 --> 00:59:01,440
real and, uh, and so on.

995
00:59:01,440 --> 00:59:07,520
Uh, so to me, it's a, uh, he was, he was, uh, he wasn't sufficiently trying to stress

996
00:59:07,520 --> 00:59:12,760
the system, I think, and, uh, exposing the truth of it as it is today.

997
00:59:12,760 --> 00:59:20,480
Um, but, uh, I think this will be increasingly harder over time, uh, so, uh, yeah, I think

998
00:59:20,480 --> 00:59:26,560
more and more people will basically, uh, become, um, yeah, I think more and more, there

999
00:59:26,560 --> 00:59:30,200
will be more people like that over time as, as this gets better, like form and emotional

1000
00:59:30,200 --> 00:59:33,920
connection to, to, to an AI, yeah, perfectly plausible in my mind.

1001
00:59:33,920 --> 00:59:38,040
I think these AIs are actually quite good at human, human, human, uh, connection, human

1002
00:59:38,040 --> 00:59:43,840
emotion, a ton of text on the internet is about humans and connection and love and so on.

1003
00:59:43,840 --> 00:59:48,120
So I think they have a very good understanding in some, in some sense of, of how people speak

1004
00:59:48,120 --> 00:59:52,880
to each other about this and, um, they're very capable of creating a lot of that kind of

1005
00:59:52,880 --> 00:59:53,880
text.

1006
00:59:53,880 --> 00:59:58,640
The, um, there's a lot of like sci-fi from 50s and 60s that imagined AIs in a very different

1007
00:59:58,640 --> 00:59:59,640
way.

1008
00:59:59,640 --> 01:00:01,680
They are calculating cold Balkan-like machines.

1009
01:00:01,680 --> 01:00:03,320
That's not what we're getting today.

1010
01:00:03,320 --> 01:00:10,320
We're getting pretty emotional AIs that actually, uh, are very competent and capable of generating,

1011
01:00:10,320 --> 01:00:13,760
you know, possible sounding text with respect to all of these topics.

1012
01:00:13,760 --> 01:00:18,040
See, I'm really hopeful about AI systems that are like companions that help you grow,

1013
01:00:18,040 --> 01:00:23,120
develop as a human being, uh, help you maximize long-term happiness, but I'm also very

1014
01:00:23,120 --> 01:00:27,960
worried about AI systems that figure out from the internet that humans get attracted to

1015
01:00:27,960 --> 01:00:28,960
drama.

1016
01:00:28,960 --> 01:00:33,280
And so these would just be like shit talking AIs that just constantly, did you hear it?

1017
01:00:33,280 --> 01:00:34,600
They'll do gossip.

1018
01:00:34,600 --> 01:00:40,840
They'll do, uh, they'll try to plant seeds of suspicion to other humans that you love

1019
01:00:40,840 --> 01:00:46,000
and trust and, uh, just kind of mess with people, uh, in the, you know, cause, cause that's

1020
01:00:46,000 --> 01:00:47,240
going to get a lot of attention.

1021
01:00:47,240 --> 01:00:54,080
So drama, maximize drama on the path to maximizing, uh, engagement and us humans will feed

1022
01:00:54,080 --> 01:01:02,080
into that machine, yeah, and get it'll be a giant drama shit storm, uh, so I'm worried

1023
01:01:02,080 --> 01:01:03,080
about that.

1024
01:01:03,080 --> 01:01:08,600
So it's the objective function really defines the way that human civilization progresses

1025
01:01:08,600 --> 01:01:09,600
with AI's in it.

1026
01:01:09,600 --> 01:01:10,600
Yeah.

1027
01:01:10,600 --> 01:01:14,280
I think right now, at least today, they are not sort of, it's not correct to really think

1028
01:01:14,280 --> 01:01:17,640
of them as goal seeking agents that want to do something.

1029
01:01:17,640 --> 01:01:20,320
They have no long-term memory or anything.

1030
01:01:20,320 --> 01:01:24,440
It's literally a good approximation of it is you get a thousand words and you're trying

1031
01:01:24,440 --> 01:01:27,600
to pretty get a thousand at first and then you continue feeding it in.

1032
01:01:27,600 --> 01:01:29,960
And you are free to prompt it in whatever way you want.

1033
01:01:29,960 --> 01:01:36,520
So in text, so you say, okay, you are a psychologist and you are very good and you love humans.

1034
01:01:36,520 --> 01:01:41,400
And here's a conversation between you and another human, human colon, something, you

1035
01:01:41,400 --> 01:01:42,400
something.

1036
01:01:42,400 --> 01:01:44,960
And then it just continues the pattern and suddenly you're having a conversation with

1037
01:01:44,960 --> 01:01:47,440
the fake psychologist who's like trying to help you.

1038
01:01:47,440 --> 01:01:51,560
And so it's still kind of like in a realm of a tool, it is a, um, people can prompt it

1039
01:01:51,560 --> 01:01:55,560
in arbitrary ways and it can create really incredible text, but it doesn't have long-term

1040
01:01:55,560 --> 01:01:57,200
goals over long periods of time.

1041
01:01:57,200 --> 01:02:00,680
It doesn't try to, um, so it doesn't look that way right now.

1042
01:02:00,680 --> 01:02:04,000
Yeah, but you can do short-term goals that have long-term effects.

1043
01:02:04,000 --> 01:02:05,000
Yeah.

1044
01:02:05,000 --> 01:02:09,920
So if my prompting short-term goal is to get Anger Kapati to respond to me on Twitter

1045
01:02:09,920 --> 01:02:15,960
when I, like, I think AI might, that's the goal, but it might figure out that talking

1046
01:02:15,960 --> 01:02:20,880
shit to you will be the best, in a highly sophisticated, interesting way.

1047
01:02:20,880 --> 01:02:28,000
And then you build up a relationship when you respond once and then it, like, over time

1048
01:02:28,000 --> 01:02:36,920
it gets to not be sophisticated and just, like, just talk shit.

1049
01:02:36,920 --> 01:02:41,160
And okay, maybe you won't get to Andre, but it might get to another celebrity and might

1050
01:02:41,160 --> 01:02:45,920
get into other big accounts, and then he'll just, so we just, that's something.

1051
01:02:45,920 --> 01:02:47,440
For example, go get them to respond.

1052
01:02:47,440 --> 01:02:48,440
Yeah.

1053
01:02:48,440 --> 01:02:50,280
Maximize the probability of actual response.

1054
01:02:50,280 --> 01:02:54,760
Yeah, I mean, you could prompt a powerful model like this with their, its opinion about

1055
01:02:54,760 --> 01:02:57,800
how to do any possible thing you're interested in.

1056
01:02:57,800 --> 01:03:00,920
So they will just, they're kind of on track to become these oracles.

1057
01:03:00,920 --> 01:03:02,840
I could sort of think of it that way.

1058
01:03:02,840 --> 01:03:06,320
They are oracles, currently is just text, but they will have calculators, they will

1059
01:03:06,320 --> 01:03:10,160
access to Google search, they will have all kinds of gadgets and gizmos, they will

1060
01:03:10,160 --> 01:03:18,120
be able to operate the internet and find different information, and yeah, in some sense, that's

1061
01:03:18,120 --> 01:03:20,400
kind of like currently what it looks like in terms of the development.

1062
01:03:20,400 --> 01:03:27,800
Do you think it'll be an improvement eventually over what Google is for access to human knowledge?

1063
01:03:27,800 --> 01:03:31,080
Like, it'll be a more effective search engine to access human knowledge.

1064
01:03:31,080 --> 01:03:34,040
I think there's a definite scope in building a better search engine today.

1065
01:03:34,040 --> 01:03:37,240
And I think Google, they have all the tools, all the people, they have everything they

1066
01:03:37,240 --> 01:03:39,920
need, they have all the puzzle pieces, they have people training transformers.

1067
01:03:39,920 --> 01:03:45,520
At scale, they have all the data, it's just not obvious if they are capable as an organization

1068
01:03:45,520 --> 01:03:47,920
to innovate on their search engine right now.

1069
01:03:47,920 --> 01:03:49,560
And if they don't, someone else will.

1070
01:03:49,560 --> 01:03:53,120
There's absolute scope for building a significantly better search engine built on these tools.

1071
01:03:53,120 --> 01:03:58,520
It's so interesting, a large company where the search, there's already an infrastructure,

1072
01:03:58,520 --> 01:04:00,680
it works as it brings out a lot of money.

1073
01:04:00,680 --> 01:04:06,600
So where structuring inside a company is their motivation to pivot to say we're going

1074
01:04:06,600 --> 01:04:08,280
to build a new search engine.

1075
01:04:08,280 --> 01:04:10,280
Yeah, that's hard.

1076
01:04:10,280 --> 01:04:13,240
So it's usually going to come from a startup, right?

1077
01:04:13,240 --> 01:04:19,400
That's, that would be, yeah, or some other more competent organization.

1078
01:04:19,400 --> 01:04:20,400
So I don't know.

1079
01:04:20,400 --> 01:04:25,760
So currently, for example, maybe Bing has another shot at it, you know, as an example, Microsoft

1080
01:04:25,760 --> 01:04:26,760
Edge.

1081
01:04:26,760 --> 01:04:27,760
We're talking offline.

1082
01:04:27,760 --> 01:04:32,800
I mean, I definitely, it's really interesting because search engines used to be about,

1083
01:04:32,800 --> 01:04:38,640
okay, here's some query, here's, here's web pages that look like the stuff that you have,

1084
01:04:38,640 --> 01:04:42,800
but you could just directly go to answer and then have supporting evidence.

1085
01:04:42,800 --> 01:04:46,600
And these, these models basically, they've read all the texts and they've read all the

1086
01:04:46,600 --> 01:04:47,600
web pages.

1087
01:04:47,600 --> 01:04:50,560
And so sometimes when you see yourself going over to search results and sort of getting

1088
01:04:50,560 --> 01:04:54,560
like a sense of like the average answer to whatever you're interested in, like that

1089
01:04:54,560 --> 01:04:55,560
just directly comes out.

1090
01:04:55,560 --> 01:04:58,520
You don't have to do that work.

1091
01:04:58,520 --> 01:05:03,600
So they're kind of like, yeah, I think they have a way of distilling all that knowledge

1092
01:05:03,600 --> 01:05:06,760
into like some level of insight, basically.

1093
01:05:06,760 --> 01:05:13,040
Do you think of prompting as a kind of teaching and learning, like this whole process,

1094
01:05:13,040 --> 01:05:18,720
like another layer, you know, because maybe that's what humans are, where you have that

1095
01:05:18,720 --> 01:05:23,000
background model and then the world is prompting you.

1096
01:05:23,000 --> 01:05:24,320
Yeah, exactly.

1097
01:05:24,320 --> 01:05:29,720
I think the way we are programming these computers now, like GPs, is converging to how you program

1098
01:05:29,720 --> 01:05:30,720
humans.

1099
01:05:30,720 --> 01:05:33,160
I mean, how do I program humans via prompt?

1100
01:05:33,160 --> 01:05:37,360
I go to people and I prompt them to do things, I prompt them for information.

1101
01:05:37,360 --> 01:05:41,520
And so natural language prompt is how we program humans and we're starting to program computers

1102
01:05:41,520 --> 01:05:42,800
directly in that interface.

1103
01:05:42,800 --> 01:05:44,600
It's like pretty remarkable, honestly.

1104
01:05:44,600 --> 01:05:49,880
So you've spoken a lot about the idea of software 2.0.

1105
01:05:49,880 --> 01:05:57,520
All good ideas become like cliches so quickly, like the terms, it's kind of hilarious.

1106
01:05:57,520 --> 01:06:03,440
It's like, I think M&M wants that, like, if he gets annoyed by a song he's written very

1107
01:06:03,440 --> 01:06:08,760
quickly, that means it's going to be a big hit because it's too catchy.

1108
01:06:08,760 --> 01:06:13,280
But can you describe this idea and how you're thinking about it has evolved over the months

1109
01:06:13,280 --> 01:06:16,320
and years since you coined it?

1110
01:06:16,320 --> 01:06:17,320
Yeah.

1111
01:06:17,320 --> 01:06:23,000
So I had a blog post on software 2.0, I think several years ago now.

1112
01:06:23,000 --> 01:06:27,320
And the reason I wrote that post is because I kept, I kind of saw something remarkable

1113
01:06:27,320 --> 01:06:33,600
happening in software development and how a lot of code was being transitioned to be written

1114
01:06:33,600 --> 01:06:37,960
not in sort of like C++ and so on, but it's written in the weights of a neural net, basically

1115
01:06:37,960 --> 01:06:42,880
just saying that neural nets are taken over software, they're almost software and taking

1116
01:06:42,880 --> 01:06:44,280
more and more and more tasks.

1117
01:06:44,280 --> 01:06:49,400
And at the time, I think not many people understood this deeply enough that this is a big deal,

1118
01:06:49,400 --> 01:06:53,760
this is a big transition, neural networks were seen as one of multiple classification

1119
01:06:53,760 --> 01:06:57,120
algorithms you might use for your data set problem on Kaggle.

1120
01:06:57,120 --> 01:07:03,320
Like, this is not that, this is a change in how we program computers.

1121
01:07:03,320 --> 01:07:08,440
And I saw neural nets as this is going to take over, the way we program computers is going

1122
01:07:08,440 --> 01:07:12,280
to change, it's not going to be people writing a software in C++ or something like that

1123
01:07:12,280 --> 01:07:16,960
and directly programming the software, it's going to be accumulating training sets and

1124
01:07:16,960 --> 01:07:20,960
data sets and crafting these objectives by which you train these neural nets.

1125
01:07:20,960 --> 01:07:24,800
And at some point there's going to be a compilation process from the data sets and the objective

1126
01:07:24,800 --> 01:07:29,800
and the architecture specification into the binary, which is really just the neural

1127
01:07:29,800 --> 01:07:34,120
net, you know, weights and the forward pass of the neural net, and then you can deploy

1128
01:07:34,120 --> 01:07:35,120
that binary.

1129
01:07:35,120 --> 01:07:40,440
And so I was talking about that sort of transition and that's what the post is about.

1130
01:07:40,440 --> 01:07:45,840
And I saw this sort of play out in a lot of fields, you know, autopilot being one of them,

1131
01:07:45,840 --> 01:07:48,560
but also just simple image classification.

1132
01:07:48,560 --> 01:07:53,040
People thought originally, you know, in the 80s and so on that they would write the algorithm

1133
01:07:53,040 --> 01:07:55,600
for detecting a dog in an image.

1134
01:07:55,600 --> 01:07:59,240
And they had all these ideas about how the brain does it and first we detect corners and

1135
01:07:59,240 --> 01:08:02,280
then we detect lines and then we stitch them up and they were like really going at it,

1136
01:08:02,280 --> 01:08:05,080
they were like thinking about how they're going to write the algorithm.

1137
01:08:05,080 --> 01:08:08,680
And this is not the way you build it.

1138
01:08:08,680 --> 01:08:12,160
And there was a smooth transition where, okay, first we thought we were going to build

1139
01:08:12,160 --> 01:08:13,160
everything.

1140
01:08:13,160 --> 01:08:18,720
Then we were building the features, so like hog features and things like that that detect

1141
01:08:18,720 --> 01:08:20,960
these little statistical patterns from image patches.

1142
01:08:20,960 --> 01:08:24,480
And then there was a little bit of learning on top of it, like a support vector machine

1143
01:08:24,480 --> 01:08:29,360
or binary classifier for cat versus dog and images on top of the features.

1144
01:08:29,360 --> 01:08:34,560
So we wrote the features, but we trained the last layer, sort of the classifier.

1145
01:08:34,560 --> 01:08:37,720
And then people are like, actually let's not even design the features because we can't

1146
01:08:37,720 --> 01:08:39,240
honestly, we're not very good at it.

1147
01:08:39,240 --> 01:08:41,160
So let's also learn the features.

1148
01:08:41,160 --> 01:08:44,640
And then you end up with basically a convolutional neural net where you're learning most of

1149
01:08:44,640 --> 01:08:45,640
it.

1150
01:08:45,640 --> 01:08:49,400
You're just specifying the architecture and the architecture has tons of fill-in blanks

1151
01:08:49,400 --> 01:08:53,160
which is all the knobs and you let the optimization write most of it.

1152
01:08:53,160 --> 01:08:56,840
And so this transition is happening across the industry everywhere.

1153
01:08:56,840 --> 01:09:01,520
And suddenly we end up with a ton of code that is written in neural net weights.

1154
01:09:01,520 --> 01:09:04,600
And I was just pointing out that the analogy is actually pretty strong.

1155
01:09:04,600 --> 01:09:10,080
And we have a lot of developer environments for software 1.0, like we have IDEs, how you

1156
01:09:10,080 --> 01:09:13,880
work with code, how you debug code, how do you run code, how do you maintain code.

1157
01:09:13,880 --> 01:09:14,880
We have GitHub.

1158
01:09:14,880 --> 01:09:16,720
So I was trying to make those analogies in the new realm.

1159
01:09:16,720 --> 01:09:17,720
Like what is the GitHub?

1160
01:09:17,720 --> 01:09:23,280
I'll software 2.0 turns out it's something that looks like hugging face right now.

1161
01:09:23,280 --> 01:09:28,200
And so I think some people took it seriously and built cool companies and many people originally

1162
01:09:28,200 --> 01:09:29,200
attacked the post.

1163
01:09:29,200 --> 01:09:31,960
It actually was not built received when I wrote it.

1164
01:09:31,960 --> 01:09:35,360
And I think maybe it has something to do with the title but the post was not well received

1165
01:09:35,360 --> 01:09:39,200
and I think more people have been coming around to it over time.

1166
01:09:39,200 --> 01:09:40,200
Yeah.

1167
01:09:40,200 --> 01:09:47,240
So you were the director of AI at Tesla where I think this idea was really implemented

1168
01:09:47,240 --> 01:09:52,160
at scale, which is how you have engineering teams doing software 2.0.

1169
01:09:52,160 --> 01:09:58,240
So can you sort of linger on that idea of I think we're in the really early stages of everything

1170
01:09:58,240 --> 01:10:05,480
you just said, which is like GitHub IDEs, like how do we build engineering teams that work

1171
01:10:05,480 --> 01:10:13,640
in software 2.0 systems and the data collection and the data annotation, which is all part

1172
01:10:13,640 --> 01:10:15,280
of that software 2.0.

1173
01:10:15,280 --> 01:10:19,000
Like what do you think is the task of programming a software 2.0?

1174
01:10:19,000 --> 01:10:25,560
Is it debugging in the space of hyperparameters or is it also debugging in the space of data?

1175
01:10:25,560 --> 01:10:32,760
Yeah, the way by which you program the computer and influence its algorithm is not by writing

1176
01:10:32,760 --> 01:10:34,640
the commands yourself.

1177
01:10:34,640 --> 01:10:37,200
You're changing mostly the data set.

1178
01:10:37,200 --> 01:10:41,680
You're changing the loss functions of like what the neural net is trying to do, how

1179
01:10:41,680 --> 01:10:42,760
it's trying to predict things.

1180
01:10:42,760 --> 01:10:46,480
But basically the data sets and the architecture, so the neural net.

1181
01:10:46,480 --> 01:10:51,720
And so in the case of the autopilot, a lot of the data sets had to do with, for example,

1182
01:10:51,720 --> 01:10:54,720
detection of objects and lane line markings and traffic lights and so on.

1183
01:10:54,720 --> 01:10:59,840
So you accumulate massive data sets of, here's an example, here's the desired label.

1184
01:10:59,840 --> 01:11:04,000
And then here's roughly how the architect, here's roughly what the algorithm should look

1185
01:11:04,000 --> 01:11:05,000
like.

1186
01:11:05,000 --> 01:11:06,000
And that's a completion neural net.

1187
01:11:06,000 --> 01:11:09,360
So the specification of the architecture is like a hint as to what the algorithm should

1188
01:11:09,360 --> 01:11:10,600
roughly look like.

1189
01:11:10,600 --> 01:11:15,920
And then the fill in the blanks process of optimization is the training process.

1190
01:11:15,920 --> 01:11:18,840
And then you take your neural net that was trained, it gives all the right answers on

1191
01:11:18,840 --> 01:11:21,080
your data set and you deploy it.

1192
01:11:21,080 --> 01:11:28,360
So there's, in that case, perhaps at all machine learning cases, there's a lot of tasks.

1193
01:11:28,360 --> 01:11:35,520
So is coming up, formulating a task, like a, for a multi-headed neural network is formulating

1194
01:11:35,520 --> 01:11:42,520
a task part of the programming, how you break down a problem into a set of tasks.

1195
01:11:42,520 --> 01:11:43,520
Yeah.

1196
01:11:43,520 --> 01:11:48,880
I'm going to high level, I would say, if you look at the software running in the autopilot,

1197
01:11:48,880 --> 01:11:52,880
I give a number of talks on this topic, I would say originally, a lot of it was written

1198
01:11:52,880 --> 01:11:57,600
in software 1.0, there's, imagine lots of C++, right?

1199
01:11:57,600 --> 01:12:01,800
And then gradually, there was a tiny neural net that was, for example, predicting, given

1200
01:12:01,800 --> 01:12:05,160
a single image, is there like a traffic light or not, or is there a lane line marking

1201
01:12:05,160 --> 01:12:06,160
or not?

1202
01:12:06,160 --> 01:12:10,400
And this neural net didn't have too much to do in the scope of the software, it was making

1203
01:12:10,400 --> 01:12:12,800
tiny predictions on individual image.

1204
01:12:12,800 --> 01:12:15,280
And then the rest of the system stitched it up.

1205
01:12:15,280 --> 01:12:18,600
So okay, we're actually, we don't have just a single camera with eight cameras.

1206
01:12:18,600 --> 01:12:20,680
We actually have eight cameras over time.

1207
01:12:20,680 --> 01:12:21,880
And so what do you do with these predictions?

1208
01:12:21,880 --> 01:12:22,880
How do you put them together?

1209
01:12:22,880 --> 01:12:26,040
How do you do the fusion of all that information and how do you act on it?

1210
01:12:26,040 --> 01:12:30,000
All of that was written by humans in C++.

1211
01:12:30,000 --> 01:12:36,120
And then we decided, okay, we don't actually want to do all of that fusion in C++ code,

1212
01:12:36,120 --> 01:12:38,280
because we're actually not good enough to write that algorithm.

1213
01:12:38,280 --> 01:12:40,080
We want the neural net to write the algorithm.

1214
01:12:40,080 --> 01:12:44,400
And we want to port all of that software into the 2.0 stack.

1215
01:12:44,400 --> 01:12:49,000
And so then we actually had neural net that now take all the eight camera images simultaneously

1216
01:12:49,000 --> 01:12:52,440
and make predictions for all of that.

1217
01:12:52,440 --> 01:12:57,440
And actually, they don't make predictions in the space of images, they now make predictions

1218
01:12:57,440 --> 01:12:59,520
directly in 3D.

1219
01:12:59,520 --> 01:13:02,680
And actually, they don't, in three dimensions, around the car.

1220
01:13:02,680 --> 01:13:08,520
And now actually, we don't manually fuse the predictions in 3D over time.

1221
01:13:08,520 --> 01:13:10,440
We don't trust ourselves to write that tracker.

1222
01:13:10,440 --> 01:13:14,360
So actually, we give the neural net the information over time.

1223
01:13:14,360 --> 01:13:17,120
So it takes these videos now and makes those predictions.

1224
01:13:17,120 --> 01:13:19,800
And so you're serving just like putting more and more power into the neural net, more

1225
01:13:19,800 --> 01:13:20,800
processing.

1226
01:13:20,800 --> 01:13:25,320
And at the end of it, the eventual sort of goal is to have most of the software potentially

1227
01:13:25,320 --> 01:13:30,280
be in the 2.0 land, because it works significantly better.

1228
01:13:30,280 --> 01:13:32,600
Humans are just not very good at writing software, basically.

1229
01:13:32,600 --> 01:13:38,120
So the prediction is happening in this like 4D land with three dimensional world over

1230
01:13:38,120 --> 01:13:39,120
time.

1231
01:13:39,120 --> 01:13:43,360
How do you do annotation in that world?

1232
01:13:43,360 --> 01:13:51,680
What have you, so data annotation, whether it's self-supervised or manual by humans is a

1233
01:13:51,680 --> 01:13:53,880
big part of the software 2.0 world?

1234
01:13:53,880 --> 01:13:54,880
Right.

1235
01:13:54,880 --> 01:13:58,920
I would say by far in the industry, if you're like talking about the industry and how, what

1236
01:13:58,920 --> 01:14:01,960
is the technology of what we have available, everything is supervised learning.

1237
01:14:01,960 --> 01:14:06,840
So you need a data sets of input, desired output, and you need lots of it.

1238
01:14:06,840 --> 01:14:09,560
And there are three properties of it that you need.

1239
01:14:09,560 --> 01:14:10,760
You need it to be very large.

1240
01:14:10,760 --> 01:14:14,400
You need it to be accurate, no mistakes, and you need it to be diverse.

1241
01:14:14,400 --> 01:14:19,120
You don't want to just have a lot of correct examples of one thing.

1242
01:14:19,120 --> 01:14:21,840
You need to really cover the space of possibility as much as you can.

1243
01:14:21,840 --> 01:14:25,320
And the more you can cover the space of possible inputs, the better the algorithm will work

1244
01:14:25,320 --> 01:14:26,320
at the end.

1245
01:14:26,320 --> 01:14:31,720
Now, once you have really good data sets that you're collecting, curating, and cleaning,

1246
01:14:31,720 --> 01:14:35,440
you can train your neural net on top of that.

1247
01:14:35,440 --> 01:14:37,960
So a lot of the work goes into cleaning those data sets now.

1248
01:14:37,960 --> 01:14:41,800
As you pointed out, it's probably, it could be, the question is, how do you achieve a

1249
01:14:41,800 --> 01:14:47,960
ton of, if you want to basically predict and 3D, you need data and 3D to back that up.

1250
01:14:47,960 --> 01:14:52,840
So in this video, we have eight videos coming from all the cameras of the system.

1251
01:14:52,840 --> 01:14:54,400
And this is what they saw.

1252
01:14:54,400 --> 01:14:56,480
And this is the truth of what actually was around.

1253
01:14:56,480 --> 01:14:58,520
There was this car, there was this car, this car.

1254
01:14:58,520 --> 01:14:59,520
These are the lane line markings.

1255
01:14:59,520 --> 01:15:00,520
This is geometry of the road.

1256
01:15:00,520 --> 01:15:02,760
There's traffic light in this redimensional position.

1257
01:15:02,760 --> 01:15:04,840
You need the ground truth.

1258
01:15:04,840 --> 01:15:08,320
And so the big question that the team was solving, of course, is, how do you, how do you

1259
01:15:08,320 --> 01:15:09,640
arrive at that ground truth?

1260
01:15:09,640 --> 01:15:13,200
Because once you have a million of it, and it's large, clean, and diverse, then training

1261
01:15:13,200 --> 01:15:17,040
in your own net on it works extremely well, and you can ship that into the car.

1262
01:15:17,040 --> 01:15:21,040
And so there's many mechanisms by which we collected that training data.

1263
01:15:21,040 --> 01:15:22,800
You can always go for a human annotation.

1264
01:15:22,800 --> 01:15:25,360
You can go for simulation as a source of ground truth.

1265
01:15:25,360 --> 01:15:30,680
You can also go for what we call the offline tracker that we've spoken about at the AI

1266
01:15:30,680 --> 01:15:34,800
day and so on, which is basically an automatic reconstruction process for taking

1267
01:15:34,800 --> 01:15:40,080
those videos and recovering the three-dimensional sort of reality of what was around that

1268
01:15:40,080 --> 01:15:41,080
car.

1269
01:15:41,080 --> 01:15:44,640
So basically think of doing like a three-dimensional reconstruction as an offline thing.

1270
01:15:44,640 --> 01:15:48,360
And then understanding that, okay, there's 10 seconds of video.

1271
01:15:48,360 --> 01:15:49,520
This is what we saw.

1272
01:15:49,520 --> 01:15:52,680
And therefore, here's all the lane line scars and so on.

1273
01:15:52,680 --> 01:15:56,560
And then once you have that annotation, you can train in your own net to imitate it.

1274
01:15:56,560 --> 01:15:58,000
And how difficult is the reconstruction?

1275
01:15:58,000 --> 01:15:59,000
The three reconstruction.

1276
01:15:59,000 --> 01:16:00,000
It's difficult.

1277
01:16:00,000 --> 01:16:01,000
But it can be done.

1278
01:16:01,000 --> 01:16:07,520
So there's overlap between the cameras and you do the reconstruction, and there's perhaps

1279
01:16:07,520 --> 01:16:11,440
if there's any inaccuracy, so that's caught in the annotation step.

1280
01:16:11,440 --> 01:16:12,440
Yes.

1281
01:16:12,440 --> 01:16:15,840
The nice thing about the annotation is that it is fully offline.

1282
01:16:15,840 --> 01:16:17,120
You have infinite time.

1283
01:16:17,120 --> 01:16:21,080
You have a chunk of one minute and you're trying to just offline in a supercomputer somewhere,

1284
01:16:21,080 --> 01:16:24,280
figure out where were the positions of all the cars, all the people.

1285
01:16:24,280 --> 01:16:27,000
And you have your full one minute video from all the angles.

1286
01:16:27,000 --> 01:16:30,600
And you can run all the neural nets you want and they can be very efficient, massive neural

1287
01:16:30,600 --> 01:16:31,600
nets.

1288
01:16:31,600 --> 01:16:34,800
There can be neural nets that can't even run in the car later at test time.

1289
01:16:34,800 --> 01:16:37,960
So they can be even more powerful neural nets than what you can eventually deploy.

1290
01:16:37,960 --> 01:16:41,920
So you can do anything you want, three dimensional reconstruction, neural nets, anything you want

1291
01:16:41,920 --> 01:16:45,440
just to recover that truth, and then you supervise that truth.

1292
01:16:45,440 --> 01:16:46,440
What have you learned?

1293
01:16:46,440 --> 01:16:54,440
You said no mistakes about humans doing annotation, because I assume humans are, there's like

1294
01:16:54,440 --> 01:16:58,000
a range of things they're good at in terms of clicking stuff on screen.

1295
01:16:58,000 --> 01:17:03,920
Isn't that, how interesting is that you have a problem of designing an annotator where

1296
01:17:03,920 --> 01:17:08,720
humans are accurate, enjoy it, like what are the even the metrics or efficient or productive

1297
01:17:08,720 --> 01:17:10,040
all that kind of stuff?

1298
01:17:10,040 --> 01:17:11,040
Yeah.

1299
01:17:11,040 --> 01:17:16,280
So I grew the annotation team at Tesla from basically zero to a thousand while I was there.

1300
01:17:16,280 --> 01:17:20,880
That was really interesting, you know, my background is a PhD student researcher.

1301
01:17:20,880 --> 01:17:24,520
So growing that common organization was pretty crazy.

1302
01:17:24,520 --> 01:17:29,320
But yeah, I think it's extremely interesting and part of the design process very much behind

1303
01:17:29,320 --> 01:17:31,960
the autopilot as to where you use humans.

1304
01:17:31,960 --> 01:17:34,120
Humans are very good at certain kinds of annotations.

1305
01:17:34,120 --> 01:17:38,920
They're very good, for example, at two dimensional annotations of images, they're not good at annotating

1306
01:17:38,920 --> 01:17:42,360
cars over time in three dimensional space, very, very hard.

1307
01:17:42,360 --> 01:17:46,520
And so that's why we were very careful to design the tasks that are easy to do for humans

1308
01:17:46,520 --> 01:17:49,120
versus things that should be left to the offline tracker.

1309
01:17:49,120 --> 01:17:52,840
Like maybe the computer will do older triangulation in three-degree construction, but the human

1310
01:17:52,840 --> 01:17:57,880
will say exactly these pixels of the image are car, exactly these pixels are human.

1311
01:17:57,880 --> 01:18:03,640
And so co-designing the data annotation pipeline was very much bread and butter was what I

1312
01:18:03,640 --> 01:18:04,640
was doing daily.

1313
01:18:04,640 --> 01:18:09,080
Do you think there's still a lot of open problems in that space?

1314
01:18:09,080 --> 01:18:14,520
Just in general, annotation where the stuff the machines are good at, machines do and

1315
01:18:14,520 --> 01:18:18,720
the humans do what they're good at, and there's maybe some iterative process.

1316
01:18:18,720 --> 01:18:19,720
Right.

1317
01:18:19,720 --> 01:18:23,640
I think to a very large extent, we went through a number of iterations and we learned a ton

1318
01:18:23,640 --> 01:18:26,320
about how to create these data sets.

1319
01:18:26,320 --> 01:18:27,920
I'm not seeing big open problems.

1320
01:18:27,920 --> 01:18:32,640
Like originally when I joined, I was like, I was really not sure how this would turn out.

1321
01:18:32,640 --> 01:18:33,640
Yeah.

1322
01:18:33,640 --> 01:18:36,880
But by the time I left, I was much more secure and actually we sort of understand the

1323
01:18:36,880 --> 01:18:38,680
philosophy of how to create these data sets.

1324
01:18:38,680 --> 01:18:41,720
And I was pretty comfortable with where that was at the time.

1325
01:18:41,720 --> 01:18:48,680
So what are strengths and limitations of cameras for the driving task in your understanding?

1326
01:18:48,680 --> 01:18:54,120
When you formulate the driving task as a vision task with eight cameras, you've seen that

1327
01:18:54,120 --> 01:18:57,560
the entire, you know, most of the history of the computer vision field when it has to

1328
01:18:57,560 --> 01:19:01,920
do a new one that works, what, just if you step back, what are the strengths and limitations

1329
01:19:01,920 --> 01:19:05,520
of pixels, of using pixels to drive?

1330
01:19:05,520 --> 01:19:10,520
Yeah, pixels, I think, are a beautiful sensory, beautiful sensor, I would say.

1331
01:19:10,520 --> 01:19:14,080
The things like cameras are very, very cheap and they provide a ton of information, ton

1332
01:19:14,080 --> 01:19:15,080
of bits.

1333
01:19:15,080 --> 01:19:20,120
Also, it's an extremely cheap sensor for a ton of bits and each one of these bits as

1334
01:19:20,120 --> 01:19:22,000
a constraint on the state of the world.

1335
01:19:22,000 --> 01:19:27,760
And so you get lots of megapixel images, very cheap and it just gives you all these constraints

1336
01:19:27,760 --> 01:19:30,120
for understanding what's actually out there in the world.

1337
01:19:30,120 --> 01:19:36,440
So vision is probably the highest bandwidth sensor, it's a very high bandwidth sensor.

1338
01:19:36,440 --> 01:19:43,520
And I love that pixels is a constraint on the world.

1339
01:19:43,520 --> 01:19:49,880
This is highly complex, high bandwidth constraint on the world, on the state of the world.

1340
01:19:49,880 --> 01:19:55,280
It's not just that, but again, this real, real importance of, it's the sensor that humans

1341
01:19:55,280 --> 01:19:56,280
use.

1342
01:19:56,280 --> 01:19:59,320
Therefore, everything is designed for that sensor.

1343
01:19:59,320 --> 01:20:00,320
Yeah.

1344
01:20:00,320 --> 01:20:04,400
The text, the writing, the flashing signs, everything is designed for vision.

1345
01:20:04,400 --> 01:20:07,320
And so you just find it everywhere.

1346
01:20:07,320 --> 01:20:10,880
And so that's why that is the interface you want to be in, talking again about these

1347
01:20:10,880 --> 01:20:12,640
universal interfaces.

1348
01:20:12,640 --> 01:20:14,960
And that's where we actually want to measure the world as well.

1349
01:20:14,960 --> 01:20:18,200
And then develop software for that sensor.

1350
01:20:18,200 --> 01:20:23,400
But there's other constraints on the state of the world that humans use to understand

1351
01:20:23,400 --> 01:20:24,400
the world.

1352
01:20:24,400 --> 01:20:31,800
I mean, vision ultimately is the main one, but we're like referencing our understanding

1353
01:20:31,800 --> 01:20:38,120
of human behavior and some common sense physics that could be inferred from vision from

1354
01:20:38,120 --> 01:20:45,040
a perception perspective, but it feels like we're using some kind of reasoning to predict

1355
01:20:45,040 --> 01:20:46,040
the world.

1356
01:20:46,040 --> 01:20:47,040
Not just the pixels.

1357
01:20:47,040 --> 01:20:52,400
I mean, you have a powerful prior for how the world evolves over time, etc.

1358
01:20:52,400 --> 01:20:56,680
So it's not just about the likelihood term coming up from the data itself, telling you

1359
01:20:56,680 --> 01:21:01,160
about what you are observing, but also the prior term of where the likely things to see

1360
01:21:01,160 --> 01:21:03,400
and how do they likely move and so on.

1361
01:21:03,400 --> 01:21:11,400
And the question is how complex is the range of possibilities that might happen in the

1362
01:21:11,400 --> 01:21:13,080
driving task?

1363
01:21:13,080 --> 01:21:17,440
That's still, is that to you still an open problem of how difficult it is driving, like

1364
01:21:17,440 --> 01:21:25,320
philosophically speaking, do all the time you work on driving, do you understand how hard

1365
01:21:25,320 --> 01:21:26,320
driving is?

1366
01:21:26,320 --> 01:21:29,960
Yeah, driving is really hard, because it has to do with predictions of all these other

1367
01:21:29,960 --> 01:21:34,800
agents in the theory of mind and what they're going to do, and are they looking at you?

1368
01:21:34,800 --> 01:21:35,800
Are they looking?

1369
01:21:35,800 --> 01:21:36,800
Are they thinking?

1370
01:21:36,800 --> 01:21:37,800
Yeah.

1371
01:21:37,800 --> 01:21:41,920
There's a lot that goes there at the full tail off, you know, the expansion of the

1372
01:21:41,920 --> 01:21:45,560
noise that we have to be comfortable with it eventually, the final problems are of

1373
01:21:45,560 --> 01:21:46,560
that form.

1374
01:21:46,560 --> 01:21:48,640
I don't think those are the problems that are very common.

1375
01:21:48,640 --> 01:21:52,240
I think eventually they're important, but it's like really in the tail end.

1376
01:21:52,240 --> 01:21:58,240
In the tail end, the rare edge cases, from the vision perspective, what are the toughest

1377
01:21:58,240 --> 01:22:02,480
parts of the vision problem of driving?

1378
01:22:02,480 --> 01:22:09,720
Well, basically, the sensor is extremely powerful, but you still need to process that information.

1379
01:22:09,720 --> 01:22:13,680
And so going from brightnesses of these special values to, hey, here the three dimensional

1380
01:22:13,680 --> 01:22:18,480
world is extremely hard, and that's what the neural networks are fundamentally doing.

1381
01:22:18,480 --> 01:22:24,000
And so the difficulty really is in just doing an extremely good job of engineering the

1382
01:22:24,000 --> 01:22:30,000
entire pipeline, the entire data engine, having the capacity to train these neural nets, having

1383
01:22:30,000 --> 01:22:33,800
the ability to evaluate the system and iterate on it.

1384
01:22:33,800 --> 01:22:37,160
So I would say just doing this in production at scale is like the hard part.

1385
01:22:37,160 --> 01:22:38,640
It's an execution problem.

1386
01:22:38,640 --> 01:22:46,840
So the data engine, but also the sort of deployment of the system such that has low latency

1387
01:22:46,840 --> 01:22:47,840
performance.

1388
01:22:47,840 --> 01:22:48,840
So it has to do all these steps.

1389
01:22:48,840 --> 01:22:52,360
Yeah, for the neural nets specifically, just making sure everything fits into the chip

1390
01:22:52,360 --> 01:22:53,360
on the car.

1391
01:22:53,360 --> 01:22:54,360
Yeah.

1392
01:22:54,360 --> 01:22:58,520
And you have a finite budget of flops that you can perform, and memory bandwidth and

1393
01:22:58,520 --> 01:22:59,760
other constraints.

1394
01:22:59,760 --> 01:23:01,280
And you have to make sure it flies.

1395
01:23:01,280 --> 01:23:04,280
And you can squeeze in as much computer as you can into the tiny.

1396
01:23:04,280 --> 01:23:05,800
What have you learned from that process?

1397
01:23:05,800 --> 01:23:11,880
Because maybe that's one of the bigger, like new things coming from a research background

1398
01:23:11,880 --> 01:23:16,400
where there's a system that has to run under heavily constrained resources, has to run

1399
01:23:16,400 --> 01:23:17,720
really fast.

1400
01:23:17,720 --> 01:23:20,440
What kind of insights have you learned from that?

1401
01:23:20,440 --> 01:23:25,240
Yeah, I'm not sure if it's, if there's too many insights, you're trying to create a neural

1402
01:23:25,240 --> 01:23:28,360
net that will fit in what you have available.

1403
01:23:28,360 --> 01:23:30,080
And you're always trying to optimize it.

1404
01:23:30,080 --> 01:23:35,320
And we talked a lot about it on the AI day and basically the triple backflips that the

1405
01:23:35,320 --> 01:23:39,600
team is doing to make sure it all fits and utilizes the engine.

1406
01:23:39,600 --> 01:23:42,400
So I think it's extremely good engineering.

1407
01:23:42,400 --> 01:23:46,920
And then there's all kinds of little insights peppered in on how to do it properly.

1408
01:23:46,920 --> 01:23:49,840
Let's actually zoom out because I don't think we talked about the data engine.

1409
01:23:49,840 --> 01:23:55,160
The entirety of the layouts of this idea that I think is just beautiful.

1410
01:23:55,160 --> 01:23:58,840
With humans in the loop, can you describe the data engine?

1411
01:23:58,840 --> 01:24:06,360
Yeah, the data engine is what I call the almost biological feeling like process by which

1412
01:24:06,360 --> 01:24:10,360
you perfect the training sets for these neural networks.

1413
01:24:10,360 --> 01:24:13,800
So because most of the programming now is in the level of these data sets and make sure

1414
01:24:13,800 --> 01:24:19,400
they're large diverse and clean, basically you have a data set that you think is good.

1415
01:24:19,400 --> 01:24:21,840
You train your neural net, you deploy it.

1416
01:24:21,840 --> 01:24:26,680
And then you observe how well it's performing and you're trying to always increase the quality

1417
01:24:26,680 --> 01:24:27,680
of your data set.

1418
01:24:27,680 --> 01:24:32,280
So you're trying to catch scenarios, basically there are basically rare.

1419
01:24:32,280 --> 01:24:35,400
And it is in these scenarios that the neural nets will typically struggle in because they

1420
01:24:35,400 --> 01:24:38,880
weren't told what to do in those rare cases in the data set.

1421
01:24:38,880 --> 01:24:42,760
But now you can close the loop because if you can now collect all those at scale, you

1422
01:24:42,760 --> 01:24:47,800
can then feed them back into the reconstruction process I described and reconstruct the truth

1423
01:24:47,800 --> 01:24:50,160
in those cases and add it to the data set.

1424
01:24:50,160 --> 01:24:54,640
And so the whole thing ends up being like a staircase of improvement of perfecting your

1425
01:24:54,640 --> 01:24:55,640
training set.

1426
01:24:55,640 --> 01:25:00,440
And you have to go through deployments so that you can mine the parts that are not yet

1427
01:25:00,440 --> 01:25:02,640
represented well on the data set.

1428
01:25:02,640 --> 01:25:03,840
So your data set is basically imperfect.

1429
01:25:03,840 --> 01:25:04,840
It needs to be diverse.

1430
01:25:04,840 --> 01:25:05,920
It has pockets.

1431
01:25:05,920 --> 01:25:08,480
There are missing and you need to pat out the pockets.

1432
01:25:08,480 --> 01:25:11,760
You can sort of think of it that way in the data.

1433
01:25:11,760 --> 01:25:13,280
What role do humans play in this?

1434
01:25:13,280 --> 01:25:21,480
So what's this biological system, like a human body is made up of cells, how do you optimize

1435
01:25:21,480 --> 01:25:29,400
the human system, the multiple engineers collaborating, figuring out what to focus on,

1436
01:25:29,400 --> 01:25:36,160
what to contribute, which task to optimize in this neural network, who is in charge of

1437
01:25:36,160 --> 01:25:39,480
figuring out which task needs more data.

1438
01:25:39,480 --> 01:25:44,600
And can you speak to the hyperparameters, the human system?

1439
01:25:44,600 --> 01:25:47,600
It really just comes down to extremely good execution from an engineering team who knows

1440
01:25:47,600 --> 01:25:48,600
what they're doing.

1441
01:25:48,600 --> 01:25:52,560
We understand intuitively the philosophical insights underlying the data engine and the process

1442
01:25:52,560 --> 01:25:58,680
by which the system improves and how to, again, delegate the strategy of the data collection

1443
01:25:58,680 --> 01:26:02,200
and how that works and then just making sure it's all extremely well executed.

1444
01:26:02,200 --> 01:26:06,040
And that's where most of the work is not even the philosophizing or the research or the

1445
01:26:06,040 --> 01:26:09,600
ideas of it, it's just extremely good execution, it's so hard when you're dealing with data

1446
01:26:09,600 --> 01:26:10,600
at that scale.

1447
01:26:10,600 --> 01:26:16,400
So your role in the data engine executing well on it is difficult and extremely important.

1448
01:26:16,400 --> 01:26:23,320
Is there a priority of like a vision board of saying like, we really need to get better

1449
01:26:23,320 --> 01:26:29,320
at stoplights, like the prioritization of tasks, is that essentially, and that comes from

1450
01:26:29,320 --> 01:26:30,720
the data?

1451
01:26:30,720 --> 01:26:35,040
That comes to a very large extent to what we are trying to achieve in the product format

1452
01:26:35,040 --> 01:26:39,800
where we're trying to, the release we're trying to get out in the feedback from the Q18

1453
01:26:39,800 --> 01:26:42,920
worth it, where the system is struggling or not, the things we're trying to improve.

1454
01:26:42,920 --> 01:26:49,440
And the Q18 gives some signal, some information in aggregate about the performance of the

1455
01:26:49,440 --> 01:26:50,440
system in various conditions.

1456
01:26:50,440 --> 01:26:51,440
That's right.

1457
01:26:51,440 --> 01:26:53,200
And then, of course, all of us drive it and we can also see it.

1458
01:26:53,200 --> 01:26:57,040
It's really nice to work with a system that you can also experience yourself, you know,

1459
01:26:57,040 --> 01:26:58,040
it drives you home.

1460
01:26:58,040 --> 01:27:02,920
Is there some insight you can draw from your individual experience that you just can't

1461
01:27:02,920 --> 01:27:06,760
quite get from an aggregate statistical analysis of the data?

1462
01:27:06,760 --> 01:27:07,760
Yeah.

1463
01:27:07,760 --> 01:27:08,760
It's so weird, right?

1464
01:27:08,760 --> 01:27:09,760
Yes.

1465
01:27:09,760 --> 01:27:13,760
It's not scientific, in a sense, because you're just one anecdotal sample.

1466
01:27:13,760 --> 01:27:19,200
Yeah, I think there's a ton of, it's a source of truth as your interaction with the system.

1467
01:27:19,200 --> 01:27:23,320
And you can see it, you can play with it, you can perturb it, you can get a sense of it,

1468
01:27:23,320 --> 01:27:24,640
you have an intuition for it.

1469
01:27:24,640 --> 01:27:28,560
I think numbers, just like have a way of numbers and plots and graphs are, you know, much

1470
01:27:28,560 --> 01:27:29,560
harder.

1471
01:27:29,560 --> 01:27:30,560
Yeah.

1472
01:27:30,560 --> 01:27:36,520
It divides a lot of, it's like, if you train a language model, it's a really powerful

1473
01:27:36,520 --> 01:27:38,680
way is by you interacting with it.

1474
01:27:38,680 --> 01:27:39,680
Yeah.

1475
01:27:39,680 --> 01:27:40,680
You're trying to build up an intuition.

1476
01:27:40,680 --> 01:27:41,680
Yeah.

1477
01:27:41,680 --> 01:27:45,320
I think, like, Elon also, like, he always wanted to drive the system himself.

1478
01:27:45,320 --> 01:27:46,400
He drives a lot.

1479
01:27:46,400 --> 01:27:49,080
And I don't want to say almost daily.

1480
01:27:49,080 --> 01:27:51,960
So he also sees this as a source of truth.

1481
01:27:51,960 --> 01:27:56,320
You driving the system and it performing and, yeah.

1482
01:27:56,320 --> 01:27:57,840
So what do you think?

1483
01:27:57,840 --> 01:28:00,120
Tough questions here.

1484
01:28:00,120 --> 01:28:06,160
So Tesla last year removed radar from, from the sensor suite and now just announced

1485
01:28:06,160 --> 01:28:10,920
there's going to remove all ultrasonic sensors relying solely on vision.

1486
01:28:10,920 --> 01:28:18,080
So camera only, does that make the perception probably harder or easier?

1487
01:28:18,080 --> 01:28:20,160
I would almost reframe the question in some way.

1488
01:28:20,160 --> 01:28:24,160
So the thing is basically, you would think that additional sensors, by the way, can

1489
01:28:24,160 --> 01:28:25,160
I just interrupt?

1490
01:28:25,160 --> 01:28:26,160
God.

1491
01:28:26,160 --> 01:28:29,200
I wonder if a language model will ever do that if you prompt it.

1492
01:28:29,200 --> 01:28:30,760
Let me reframe your question.

1493
01:28:30,760 --> 01:28:32,560
That would be epic.

1494
01:28:32,560 --> 01:28:33,840
This is the wrong problem.

1495
01:28:33,840 --> 01:28:34,840
Sorry.

1496
01:28:34,840 --> 01:28:38,120
It's like a little bit of a wrong question because basically, you would think that these

1497
01:28:38,120 --> 01:28:40,640
sensors are an asset to you.

1498
01:28:40,640 --> 01:28:41,640
Yeah.

1499
01:28:41,640 --> 01:28:45,920
But if you fully consider the entire product and its entirety, these sensors are actually

1500
01:28:45,920 --> 01:28:49,880
potentially liability because these sensors aren't free.

1501
01:28:49,880 --> 01:28:51,400
They don't just appear on your car.

1502
01:28:51,400 --> 01:28:53,880
You need suddenly, you need to have an entire supply chain.

1503
01:28:53,880 --> 01:28:55,480
You have people procuring it.

1504
01:28:55,480 --> 01:28:56,640
There can be problems with them.

1505
01:28:56,640 --> 01:28:57,920
They may need replacement.

1506
01:28:57,920 --> 01:28:59,160
They are part of the manufacturing process.

1507
01:28:59,160 --> 01:29:01,800
They can hold back the line in production.

1508
01:29:01,800 --> 01:29:02,800
You need to source them.

1509
01:29:02,800 --> 01:29:03,800
You need to maintain them.

1510
01:29:03,800 --> 01:29:08,320
You have to have teams that ride the firmware, all of it, and then you also have to incorporate

1511
01:29:08,320 --> 01:29:10,120
them, fuse them into the system in some way.

1512
01:29:10,120 --> 01:29:13,800
And so it actually, like, blows the organ, a lot of it.

1513
01:29:13,800 --> 01:29:18,480
And I think Elon is really good at simplifying, simplify, best part is no part.

1514
01:29:18,480 --> 01:29:21,280
And he always tries to throw away things that are not essential because he understands

1515
01:29:21,280 --> 01:29:24,040
the entropy in organizations and in an approach.

1516
01:29:24,040 --> 01:29:28,160
And I think, in this case, the cost is high and you're not potentially seeing it if

1517
01:29:28,160 --> 01:29:29,840
you're just a computer vision engineer.

1518
01:29:29,840 --> 01:29:33,880
And I'm just trying to improve my network and, you know, is it more useful or less useful

1519
01:29:33,880 --> 01:29:35,440
or how useful is it?

1520
01:29:35,440 --> 01:29:39,240
And the thing is, once you consider the full cost of a sensor, it actually is potentially

1521
01:29:39,240 --> 01:29:43,920
a liability and you need to be really sure that it's giving you extremely useful information.

1522
01:29:43,920 --> 01:29:48,200
In this case, we looked at using it or not using it and the delta was not massive.

1523
01:29:48,200 --> 01:29:49,680
And so it's not useful.

1524
01:29:49,680 --> 01:29:56,200
Is it also bloat in the data engine, like having more sensors, and is a distraction?

1525
01:29:56,200 --> 01:29:58,280
And these sensors, you know, they can change over time, for example.

1526
01:29:58,280 --> 01:30:01,280
You can have one type of, say radar, you can have other type of radar, they change over

1527
01:30:01,280 --> 01:30:02,280
time.

1528
01:30:02,280 --> 01:30:03,280
And I suddenly need to worry about it.

1529
01:30:03,280 --> 01:30:06,000
And I'm suddenly going to have a column in your SQLite telling you, oh, what sensor

1530
01:30:06,000 --> 01:30:07,000
type was it?

1531
01:30:07,000 --> 01:30:08,880
And they all have different distributions.

1532
01:30:08,880 --> 01:30:13,880
And then they can, they just, they contribute noise and entropy into everything.

1533
01:30:13,880 --> 01:30:15,360
And they bloat stuff.

1534
01:30:15,360 --> 01:30:19,640
And also organizationally has been really fascinating to me that it can be very distracting.

1535
01:30:19,640 --> 01:30:24,800
Um, if you, if all, if you only want to get to work as vision, all the resources are on

1536
01:30:24,800 --> 01:30:27,320
it and you're building out a data engine.

1537
01:30:27,320 --> 01:30:31,680
And you're actually making forward progress because that is the, the sensor with the most

1538
01:30:31,680 --> 01:30:33,760
bandwidth, the most constraints on the world.

1539
01:30:33,760 --> 01:30:36,520
And you're investing fully into that and you can make that extremely good.

1540
01:30:36,520 --> 01:30:41,400
If you're, uh, you're only a finite amount of sort of spend of, uh, focus across different

1541
01:30:41,400 --> 01:30:43,200
facets of the system.

1542
01:30:43,200 --> 01:30:49,520
And, uh, this kind of reminds me of Rich Sutton's a bit of lesson that just seems like simplifying

1543
01:30:49,520 --> 01:30:50,520
the system.

1544
01:30:50,520 --> 01:30:51,520
Yeah.

1545
01:30:51,520 --> 01:30:55,400
In the long run, of course, you know, no, with the long run, it seems to be always the

1546
01:30:55,400 --> 01:30:56,400
right solution.

1547
01:30:56,400 --> 01:30:57,400
Yeah.

1548
01:30:57,400 --> 01:30:58,400
Yes.

1549
01:30:58,400 --> 01:31:01,480
In that case, it was for RL, but it seems to apply generally across all systems that do

1550
01:31:01,480 --> 01:31:02,480
computation.

1551
01:31:02,480 --> 01:31:03,480
Yeah.

1552
01:31:03,480 --> 01:31:09,000
So where, uh, what do you think about the LiDAR as a crutch debate, uh, the battle between

1553
01:31:09,000 --> 01:31:10,520
point clouds and pixels?

1554
01:31:10,520 --> 01:31:14,920
Yeah, I think this debate is always like slightly confusing to me because it seems like

1555
01:31:14,920 --> 01:31:18,240
the actual debate should be about like, do you have the fleet or not?

1556
01:31:18,240 --> 01:31:21,560
That's like the really important thing about whether you can achieve, uh, really good

1557
01:31:21,560 --> 01:31:24,080
functioning of an AI system at the scale.

1558
01:31:24,080 --> 01:31:25,360
So data collection systems.

1559
01:31:25,360 --> 01:31:26,360
Yeah.

1560
01:31:26,360 --> 01:31:29,600
Do you have a fleet or not is significantly more important, whether you have LiDAR or

1561
01:31:29,600 --> 01:31:30,600
not?

1562
01:31:30,600 --> 01:31:32,400
It's just another sensor.

1563
01:31:32,400 --> 01:31:39,080
Um, and, uh, yeah, I think similar to the radar discussion, basically, I, um, I, yeah,

1564
01:31:39,080 --> 01:31:44,040
I don't think it, um, basically it doesn't offer extra, uh, extra information.

1565
01:31:44,040 --> 01:31:45,040
It's extremely costly.

1566
01:31:45,040 --> 01:31:46,040
It has all kinds of problems.

1567
01:31:46,040 --> 01:31:47,040
You have to worry about it.

1568
01:31:47,040 --> 01:31:48,040
You have to calibrate it, et cetera.

1569
01:31:48,040 --> 01:31:49,280
It creates bloat and entropy.

1570
01:31:49,280 --> 01:31:53,080
You have to be really sure that you need this, uh, this, um, sensor.

1571
01:31:53,080 --> 01:31:55,080
In this case, I basically don't think you need it.

1572
01:31:55,080 --> 01:31:57,280
And I think honestly, I will make a stronger statement.

1573
01:31:57,280 --> 01:32:00,880
I think the others, uh, some of the other, uh, companies that are using it are probably

1574
01:32:00,880 --> 01:32:01,880
going to drop it.

1575
01:32:01,880 --> 01:32:02,880
Yeah.

1576
01:32:02,880 --> 01:32:09,760
And you have to consider the sensor in the full, in considering, can you build a big

1577
01:32:09,760 --> 01:32:14,720
fleet that collects a lot of data and can you integrate that sensor with it, that, that

1578
01:32:14,720 --> 01:32:19,400
data and that sensor into a data engine that's able to quickly find different parts of the

1579
01:32:19,400 --> 01:32:23,960
data that then continuously improves whatever the model that you're using.

1580
01:32:23,960 --> 01:32:24,960
Yeah.

1581
01:32:24,960 --> 01:32:29,960
Another way to look at it is like vision is necessary in a sense that, uh, the drive, uh,

1582
01:32:29,960 --> 01:32:31,560
the world is designed for human visual consumption.

1583
01:32:31,560 --> 01:32:32,560
So you need vision.

1584
01:32:32,560 --> 01:32:33,920
It's necessary.

1585
01:32:33,920 --> 01:32:38,080
And then also it is sufficient, uh, because it has all the information that you, that you

1586
01:32:38,080 --> 01:32:40,680
need for driving and humans, obviously, is vision to drive.

1587
01:32:40,680 --> 01:32:42,560
So it's both necessary and sufficient.

1588
01:32:42,560 --> 01:32:45,480
So you want to focus resources and you have to be really sure if you're going to bring

1589
01:32:45,480 --> 01:32:46,840
in other sensors.

1590
01:32:46,840 --> 01:32:50,200
You could, you could, you could add sensors to infinity at some point you need to draw

1591
01:32:50,200 --> 01:32:51,200
the line.

1592
01:32:51,200 --> 01:32:55,720
And I think in this case, you have to really consider the full, um, cost of any one sensor

1593
01:32:55,720 --> 01:32:57,040
that you're adopting.

1594
01:32:57,040 --> 01:32:58,840
And do you really need it?

1595
01:32:58,840 --> 01:33:00,880
And I think the answer in this case is no.

1596
01:33:00,880 --> 01:33:06,800
So what do you think about the idea that the other companies are forming high resolution

1597
01:33:06,800 --> 01:33:11,800
maps and constraining heavily the geographic regions in which they operate?

1598
01:33:11,800 --> 01:33:19,520
Is that approach not in your, in your view, um, not going to scale over time to the entirety

1599
01:33:19,520 --> 01:33:20,520
of the United States?

1600
01:33:20,520 --> 01:33:21,520
I think I'll take too long.

1601
01:33:21,520 --> 01:33:25,240
As you mentioned, like they pre-map all the environments and they need to refresh the

1602
01:33:25,240 --> 01:33:29,120
map and they have a perfect centimeter level accuracy map of everywhere they're going

1603
01:33:29,120 --> 01:33:30,120
to drive.

1604
01:33:30,120 --> 01:33:31,120
Crazy.

1605
01:33:31,120 --> 01:33:34,480
How are you going to, when we're talking about the autonomy actually changing the world,

1606
01:33:34,480 --> 01:33:40,560
we're talking about the deployment on a, on the global scale autonomous systems for transportation.

1607
01:33:40,560 --> 01:33:45,400
And if you need to maintain a centimeter accurate map for earth, we're like for many cities

1608
01:33:45,400 --> 01:33:46,400
and keep them updated.

1609
01:33:46,400 --> 01:33:50,320
It's a huge, uh, dependency that you're taking on huge dependency.

1610
01:33:50,320 --> 01:33:51,640
It's, it's a massive massive dependency.

1611
01:33:51,640 --> 01:33:54,640
And now you need to ask yourself, do you really need it?

1612
01:33:54,640 --> 01:33:57,400
And humans don't need it, um, right?

1613
01:33:57,400 --> 01:34:01,200
So it's, it's very useful to have a low level map of like, okay, the connectivity of your

1614
01:34:01,200 --> 01:34:02,200
road.

1615
01:34:02,200 --> 01:34:04,760
You know that there's a fork coming up, uh, when you drive an environment, you sort of have

1616
01:34:04,760 --> 01:34:05,760
that high level understanding.

1617
01:34:05,760 --> 01:34:11,360
It's like a small Google map and, uh, Tesla uses Google map, uh, like similar kind of resolution

1618
01:34:11,360 --> 01:34:15,480
information in its system, but it will not pre-map environments to send me to a level

1619
01:34:15,480 --> 01:34:16,480
accuracy.

1620
01:34:16,480 --> 01:34:17,480
It's a crotch.

1621
01:34:17,480 --> 01:34:18,480
It's a distraction.

1622
01:34:18,480 --> 01:34:22,240
It costs entropy and it, uh, diffuses the team and dilutes the team and you're not focusing

1623
01:34:22,240 --> 01:34:26,560
on what's actually necessary, which is the computer version problem.

1624
01:34:26,560 --> 01:34:32,000
What did you learn about machine learning, about engineering, about life, about yourself

1625
01:34:32,000 --> 01:34:36,560
as one human being from working with Elon Musk?

1626
01:34:36,560 --> 01:34:41,120
I think the most I've learned is about how to sort of run organizations efficiently and

1627
01:34:41,120 --> 01:34:46,400
how to create efficient organizations and how to fight entropy in an organization.

1628
01:34:46,400 --> 01:34:49,200
So human engineering in the fight against entropy.

1629
01:34:49,200 --> 01:34:50,200
Yeah.

1630
01:34:50,200 --> 01:34:54,360
There's a, there's a, I think Elon is a very efficient warrior, uh, in the fight against

1631
01:34:54,360 --> 01:34:56,360
entropy in organizations.

1632
01:34:56,360 --> 01:34:59,160
What does the entropy in an organization look like, exactly?

1633
01:34:59,160 --> 01:35:05,800
It's, it's process, it's, it's process and inefficiencies in the human beings and that

1634
01:35:05,800 --> 01:35:06,800
kind of stuff.

1635
01:35:06,800 --> 01:35:07,800
Yeah, meetings, he hates meetings.

1636
01:35:07,800 --> 01:35:11,000
He keeps telling people to skip meetings if they're not useful.

1637
01:35:11,000 --> 01:35:15,960
He basically runs the world's biggest, uh, startups, I would say, uh, Tesla SpaceX

1638
01:35:15,960 --> 01:35:19,960
started the world's biggest startups, Tesla actually is multiple startups, I think is

1639
01:35:19,960 --> 01:35:21,600
better to look at it that way.

1640
01:35:21,600 --> 01:35:26,720
And so I think he's, he's extremely good at, uh, at that and, uh, yeah, he's a very

1641
01:35:26,720 --> 01:35:30,640
good, um, intuition for streamlining processes, making everything efficient, uh, best part

1642
01:35:30,640 --> 01:35:36,240
is no part, uh, simplifying, focusing, um, and just kind of removing barriers, uh, moving

1643
01:35:36,240 --> 01:35:37,600
very quickly, making big moves.

1644
01:35:37,600 --> 01:35:41,400
Now, all this is a very start-upy sort of seeming things, but at scale.

1645
01:35:41,400 --> 01:35:47,960
So strong drive to simplify, from your perspective, I mean, that, um, that also probably applies

1646
01:35:47,960 --> 01:35:52,800
to just designing systems and machine learning and otherwise, like simplifies, simplifies.

1647
01:35:52,800 --> 01:35:53,800
Yes.

1648
01:35:53,800 --> 01:35:59,240
What do you think is the secret to maintaining the startup culture in a company that grows?

1649
01:35:59,240 --> 01:36:03,840
Is there, can you introspect that?

1650
01:36:03,840 --> 01:36:08,120
I do think he needs someone in a powerful position with a big hammer, like Elon, who's

1651
01:36:08,120 --> 01:36:12,840
like the cheerleader for that idea and ruthlessly pursues it.

1652
01:36:12,840 --> 01:36:18,080
If no one has a big enough hammer, everything turns into committees, democracy within the

1653
01:36:18,080 --> 01:36:23,480
company, uh, process, talking to stakeholders, decision-making, just everything, just crumbles.

1654
01:36:23,480 --> 01:36:24,480
Yeah.

1655
01:36:24,480 --> 01:36:28,320
If you have a big person who is also really smart and has a big hammer, uh, things move

1656
01:36:28,320 --> 01:36:29,320
quickly.

1657
01:36:29,320 --> 01:36:34,840
So you said your favorite scene in interstellar is the intense docking scene with the AI

1658
01:36:34,840 --> 01:36:38,600
and Cooper talking, saying, uh, Cooper, what are you doing?

1659
01:36:38,600 --> 01:36:43,840
Talking, it's not possible, no, it's necessary, it's such a good line.

1660
01:36:43,840 --> 01:36:51,240
By the way, just so many questions there, why an AI in that scene, presumably is supposed

1661
01:36:51,240 --> 01:36:57,080
to be able to compute a lot more than the human is saying it's not optimal, why the human,

1662
01:36:57,080 --> 01:37:02,160
I mean, that's a movie, but shouldn't the AI know which better than the human?

1663
01:37:02,160 --> 01:37:07,680
Anyway, uh, what do you think is the value of setting, seemingly impossible goals?

1664
01:37:07,680 --> 01:37:14,560
So like, uh, our initial intuition, which seems like something that, uh, you have taken

1665
01:37:14,560 --> 01:37:20,400
on that Elon espouses that where the initial intuition of the community might say this

1666
01:37:20,400 --> 01:37:25,000
is very difficult, and then you take it on anyway with a crazy deadline.

1667
01:37:25,000 --> 01:37:32,400
You're just from a human engineering perspective, um, uh, have you seen the value of that?

1668
01:37:32,400 --> 01:37:36,240
I wouldn't say that setting impossible goals exactly is, is a good idea, but I think setting

1669
01:37:36,240 --> 01:37:38,240
very ambitious goals is a good idea.

1670
01:37:38,240 --> 01:37:43,360
I think there's a, what I call sub linear scaling of difficulty, uh, which means that 10x

1671
01:37:43,360 --> 01:37:49,960
problems are not 10x hard, usually 10x, 10x harder problem is like two or three x harder

1672
01:37:49,960 --> 01:37:54,640
to execute on, because if you want to actually like, if you want to improve a system by 10%,

1673
01:37:54,640 --> 01:37:58,880
it costs some amount of work, and if you want to 10x improve the system, it doesn't cost,

1674
01:37:58,880 --> 01:38:02,240
you know, 100x amount of the work, and it's because you fundamentally changed the approach,

1675
01:38:02,240 --> 01:38:06,200
and if you start with that constraint, then some approaches are obviously dumb and not

1676
01:38:06,200 --> 01:38:07,200
going to work.

1677
01:38:07,200 --> 01:38:12,760
And it forces you to re-evaluate, um, and I think it's a very interesting way of approaching

1678
01:38:12,760 --> 01:38:13,760
problem solving.

1679
01:38:13,760 --> 01:38:19,600
But it requires a weird kind of thinking, it's just going back to your like PhD days, it's

1680
01:38:19,600 --> 01:38:26,760
like, how do you think which ideas in, in the machine learning community are solvable?

1681
01:38:26,760 --> 01:38:27,760
Mm-hmm.

1682
01:38:27,760 --> 01:38:28,760
Yes.

1683
01:38:28,760 --> 01:38:33,040
It requires, what, what is that? I mean, there's the cliche of first principles thinking,

1684
01:38:33,040 --> 01:38:37,040
but like, it requires to basically ignore what the community is saying, because it doesn't

1685
01:38:37,040 --> 01:38:44,000
the community, doesn't A community in science usually draw lines of what isn't as impossible.

1686
01:38:44,000 --> 01:38:45,000
Right.

1687
01:38:45,000 --> 01:38:48,400
And like, it's very hard to break out of that without going crazy.

1688
01:38:48,400 --> 01:38:49,400
Yeah.

1689
01:38:49,400 --> 01:38:52,160
I mean, I think a good example here is, you know, the deep learning revolution in some

1690
01:38:52,160 --> 01:38:57,760
sense, because you could be in computer vision at that time when during the, uh, deep learning,

1691
01:38:57,760 --> 01:39:02,120
sort of revolution of 2012 and so on, you could be improving a computer vision stack by

1692
01:39:02,120 --> 01:39:06,040
10 percent, or we can just be saying, actually, all of this is useless.

1693
01:39:06,040 --> 01:39:07,880
And how do I do 10x better computer vision?

1694
01:39:07,880 --> 01:39:11,080
Well, it's not probably by tuning a hog feature detector.

1695
01:39:11,080 --> 01:39:12,280
I need a different approach.

1696
01:39:12,280 --> 01:39:17,680
Um, I need something that is scalable, going back to, uh, Richard Sutton's, um, and understanding

1697
01:39:17,680 --> 01:39:22,040
sort of like the philosophy of the, uh, bitter lesson and then being like, actually, I need

1698
01:39:22,040 --> 01:39:26,120
a much more scalable system like neural network, that in principle works, and then having

1699
01:39:26,120 --> 01:39:29,600
some deep believers that can actually execute on that mission and make it work.

1700
01:39:29,600 --> 01:39:34,360
So that's the 10x solution.

1701
01:39:34,360 --> 01:39:38,720
What do you think is the timeline to solve the problem of autonomous driving?

1702
01:39:38,720 --> 01:39:41,920
That's still in part open question.

1703
01:39:41,920 --> 01:39:42,920
Yeah.

1704
01:39:42,920 --> 01:39:47,200
I think the tough thing with timelines of self driving, obviously, is that no one has created

1705
01:39:47,200 --> 01:39:48,200
self driving.

1706
01:39:48,200 --> 01:39:49,200
Yeah.

1707
01:39:49,200 --> 01:39:52,040
So it's not like, what do you think is a timeline to build this bridge?

1708
01:39:52,040 --> 01:39:54,200
Well, we've built million bridges before.

1709
01:39:54,200 --> 01:39:55,400
Here's how long that takes.

1710
01:39:55,400 --> 01:39:58,640
It's, you know, it's, uh, no one has built autonomy.

1711
01:39:58,640 --> 01:40:03,080
It's not obvious, uh, some parts turn out to be much easier than others.

1712
01:40:03,080 --> 01:40:04,080
It's really hard to forecast.

1713
01:40:04,080 --> 01:40:07,880
You do your best based on trend lines and so on and based on intuition.

1714
01:40:07,880 --> 01:40:11,000
But that's why fundamental is just really hard to forecast this.

1715
01:40:11,000 --> 01:40:15,920
No one has to even still like being inside of it is hard to, uh, to do some things turn

1716
01:40:15,920 --> 01:40:19,080
out to be much harder and some things turn out to be much easier.

1717
01:40:19,080 --> 01:40:24,520
Do you try to avoid making forecasts because like Elon doesn't avoid them, right?

1718
01:40:24,520 --> 01:40:29,840
And heads of car companies in the past have not avoided it either, uh, Ford and other

1719
01:40:29,840 --> 01:40:35,760
places have made predictions that we're going to solve a level for driving by 2020, 2021,

1720
01:40:35,760 --> 01:40:36,760
whatever.

1721
01:40:36,760 --> 01:40:39,760
And now they're all kind of backtrack in that prediction.

1722
01:40:39,760 --> 01:40:49,240
I you as a, as an AI person, do you for yourself privately make predictions or do they get

1723
01:40:49,240 --> 01:40:53,520
in the way of like your actual ability to think about a thing?

1724
01:40:53,520 --> 01:40:54,520
Yeah.

1725
01:40:54,520 --> 01:40:58,400
I would say like what's easy to say is that this problem is tractable and that's an easy

1726
01:40:58,400 --> 01:40:59,400
prediction to make.

1727
01:40:59,400 --> 01:41:00,400
It's tractable.

1728
01:41:00,400 --> 01:41:01,400
It's going to work.

1729
01:41:01,400 --> 01:41:02,400
Yes.

1730
01:41:02,400 --> 01:41:03,400
It's just really hard.

1731
01:41:03,400 --> 01:41:07,160
Something's not to be harder than something's not to be easier, uh, so, uh, but it's definitely

1732
01:41:07,160 --> 01:41:11,760
feels tractable and it feels like at least the team at Tesla, which is what I saw internally

1733
01:41:11,760 --> 01:41:13,520
is definitely on track to that.

1734
01:41:13,520 --> 01:41:20,760
How do you form a strong representation that allows you to make a prediction about tractability?

1735
01:41:20,760 --> 01:41:24,880
So like you're the leader of a lot, a lot of humans.

1736
01:41:24,880 --> 01:41:28,680
You have to kind of say this is actually possible.

1737
01:41:28,680 --> 01:41:30,920
Like how do you build up that intuition?

1738
01:41:30,920 --> 01:41:32,480
It doesn't have to be even driving.

1739
01:41:32,480 --> 01:41:37,520
It could be other tasks, it could be, um, and I want to, what difficult tasks did you work

1740
01:41:37,520 --> 01:41:38,520
on your life?

1741
01:41:38,520 --> 01:41:44,200
I mean, classification, achieving certain, just an image net, certain level of superhuman

1742
01:41:44,200 --> 01:41:45,200
level performance.

1743
01:41:45,200 --> 01:41:46,200
Yeah.

1744
01:41:46,200 --> 01:41:47,200
Expert intuition.

1745
01:41:47,200 --> 01:41:48,200
It's just intuition.

1746
01:41:48,200 --> 01:41:49,200
It's belief.

1747
01:41:49,200 --> 01:41:54,800
So just like thinking about it long enough, like studying, looking at sample data, like

1748
01:41:54,800 --> 01:41:59,760
you said, driving, uh, my intuition is really flawed on this, but like I don't have a good

1749
01:41:59,760 --> 01:42:03,320
intuition about tractability is it could be either, it could be anything.

1750
01:42:03,320 --> 01:42:11,000
It could be solvable, like, uh, you know, the driving task could, could be simplified into

1751
01:42:11,000 --> 01:42:13,040
something quite trivial.

1752
01:42:13,040 --> 01:42:17,960
Like, uh, the solution to the problem would be quite trivial and at scale, more and more

1753
01:42:17,960 --> 01:42:21,880
cars driving perfectly might make the problem much easier.

1754
01:42:21,880 --> 01:42:22,880
Yes.

1755
01:42:22,880 --> 01:42:27,560
And the more cars you have driving, like people learn how to drive correctly, not correctly,

1756
01:42:27,560 --> 01:42:34,560
but in a way that's more optimal for, uh, heterogeneous system of autonomous and semi-autonomous

1757
01:42:34,560 --> 01:42:37,280
and manually driven cars, that could change stuff.

1758
01:42:37,280 --> 01:42:42,280
Then again, also I've spent a ridiculous number of hours just staring at pedestrians crossing

1759
01:42:42,280 --> 01:42:45,520
streets, thinking about humans.

1760
01:42:45,520 --> 01:42:53,360
And it feels like the way we use our eye contact, it sends really strong signals, and there's

1761
01:42:53,360 --> 01:42:57,240
certain quirks in edge cases of behavior, and of course, a lot of the fatalities that

1762
01:42:57,240 --> 01:43:02,960
happen have to do with drunk driving and, um, both on the pedestrian side and the driver

1763
01:43:02,960 --> 01:43:03,960
side.

1764
01:43:03,960 --> 01:43:06,920
So there's that problem of driving at night and all that kind of stuff.

1765
01:43:06,920 --> 01:43:12,320
So I wonder, you know, let's say the space of possible solution to autonomous driving

1766
01:43:12,320 --> 01:43:17,840
includes so many human factor issues that it's almost impossible to predict.

1767
01:43:17,840 --> 01:43:20,760
It could be super clean, nice solutions.

1768
01:43:20,760 --> 01:43:21,760
Yeah.

1769
01:43:21,760 --> 01:43:25,840
I would say definitely like to use a game analogy, there's some fog of war, but you definitely

1770
01:43:25,840 --> 01:43:28,600
also see the frontier of improvement.

1771
01:43:28,600 --> 01:43:31,600
And you can measure historically how much you've made progress.

1772
01:43:31,600 --> 01:43:35,480
And I think, for example, at least what I've seen in, uh, roughly five years of Tesla,

1773
01:43:35,480 --> 01:43:38,960
when I joined it barely kept lane on highway.

1774
01:43:38,960 --> 01:43:43,280
I think going up from Pellalto to SF was like three or four interventions anytime the road

1775
01:43:43,280 --> 01:43:47,240
would do anything geometrically or turn too much, it would just like not work.

1776
01:43:47,240 --> 01:43:50,960
And so going from that to like a pretty competent system in five years and seeing what happens

1777
01:43:50,960 --> 01:43:54,560
also under the hood, and what the scale of which the team is operating now with respect

1778
01:43:54,560 --> 01:43:59,680
to data and compute to everything else, uh, is just, uh, massive progress.

1779
01:43:59,680 --> 01:44:05,240
So this, uh, you're climbing a mountain and it's fog, but you make a lot of progress.

1780
01:44:05,240 --> 01:44:06,240
It's fog.

1781
01:44:06,240 --> 01:44:07,800
You're making progress and you see what the next directions are.

1782
01:44:07,800 --> 01:44:11,640
And you're looking at some of the remaining challenges and they're not like, uh, they're

1783
01:44:11,640 --> 01:44:15,280
not perturbing you and they're not changing your philosophy and you're not contorting

1784
01:44:15,280 --> 01:44:16,280
yourself.

1785
01:44:16,280 --> 01:44:18,280
If you're like, actually, these are the things I've always told you to do.

1786
01:44:18,280 --> 01:44:21,840
Yeah, the fundamental components of solving the problem seem to be there from data engine

1787
01:44:21,840 --> 01:44:25,840
to the compute to the, the compute on the car to the compute for the training, all that

1788
01:44:25,840 --> 01:44:26,840
kind of stuff.

1789
01:44:26,840 --> 01:44:33,040
So you've done, uh, over the years you've been a Tesla, you've done a lot of amazing, uh,

1790
01:44:33,040 --> 01:44:38,680
take through ideas and engineering, all of it, um, from the data engine to the human

1791
01:44:38,680 --> 01:44:40,320
side, all of it.

1792
01:44:40,320 --> 01:44:43,680
Can you speak to why you chose to leave Tesla?

1793
01:44:43,680 --> 01:44:48,040
Basically, as I described that, Ren, I think over time during those five years, I've kind

1794
01:44:48,040 --> 01:44:53,040
of, uh, gotten myself into a little bit of a managerial position, uh, most of my days

1795
01:44:53,040 --> 01:44:57,400
were, you know, meetings and growing the organization and making, um, decisions about

1796
01:44:57,400 --> 01:45:01,320
a sort of high level strategic decisions about the team and what it should be working

1797
01:45:01,320 --> 01:45:02,720
on and so on.

1798
01:45:02,720 --> 01:45:07,160
And, uh, it's, it's kind of like a corporate executive role and I can do it.

1799
01:45:07,160 --> 01:45:11,240
I think I'm okay at it, uh, but it's not like fundamentally what I, what I enjoy.

1800
01:45:11,240 --> 01:45:15,680
And so I think, uh, when I joined, um, there was no computer vision team because Tesla

1801
01:45:15,680 --> 01:45:19,040
was just going from the transition of using mobile, I, a third party vendor for all of

1802
01:45:19,040 --> 01:45:21,960
its computer vision to having to build its computer vision system.

1803
01:45:21,960 --> 01:45:25,240
So when I showed up, there were two people training deep neural networks and they were

1804
01:45:25,240 --> 01:45:30,400
training them at a computer, at their, at their legs like, yeah, that's, there's a

1805
01:45:30,400 --> 01:45:32,800
kind of basic classification task.

1806
01:45:32,800 --> 01:45:33,800
Yeah.

1807
01:45:33,800 --> 01:45:38,240
And so I kind of like grew that into what I think is a fairly respectable deep learning

1808
01:45:38,240 --> 01:45:43,240
team, massive computer cluster, a very good, um, data annotation organization.

1809
01:45:43,240 --> 01:45:45,360
And, uh, I was very happy with where that was.

1810
01:45:45,360 --> 01:45:46,800
It became quite autonomous.

1811
01:45:46,800 --> 01:45:51,040
And so I kind of stepped away and I, uh, you know, I'm very excited to do much more technical

1812
01:45:51,040 --> 01:45:52,040
things again.

1813
01:45:52,040 --> 01:45:53,040
Yeah.

1814
01:45:53,040 --> 01:45:54,800
And kind of like, we focus on AGI.

1815
01:45:54,800 --> 01:45:56,480
What was this soul searching like?

1816
01:45:56,480 --> 01:46:00,800
Cause you took a little time off and think like what, um, how many mushrooms did you take?

1817
01:46:00,800 --> 01:46:03,840
I mean, what, uh, what was going through your mind?

1818
01:46:03,840 --> 01:46:05,720
The human lifetime is finite.

1819
01:46:05,720 --> 01:46:06,720
Yeah.

1820
01:46:06,720 --> 01:46:08,400
You did a few incredible things.

1821
01:46:08,400 --> 01:46:11,800
You're, you're one of the best teachers of AI in the world.

1822
01:46:11,800 --> 01:46:16,040
You're one of the best, and I don't mean that, I mean, then the best possible way.

1823
01:46:16,040 --> 01:46:22,880
You're one of the best tinkers in the AI world, meaning like, understanding the fundamental,

1824
01:46:22,880 --> 01:46:27,400
the methods of how something works by building it from scratch and playing with it, with

1825
01:46:27,400 --> 01:46:28,640
the basic intuitions.

1826
01:46:28,640 --> 01:46:32,160
It's like Einstein Feynman, we're all really good at this kind of stuff.

1827
01:46:32,160 --> 01:46:37,480
Like small example of a thing to, to play with it, to try to understand it, uh, so that

1828
01:46:37,480 --> 01:46:44,560
and obviously now with, with us, you help build a team of machine learning, um, uh, like

1829
01:46:44,560 --> 01:46:48,000
engineers and assistant that actually accomplished something in the real world.

1830
01:46:48,000 --> 01:46:51,040
So given all that, like, what was the soul searching like?

1831
01:46:51,040 --> 01:46:56,400
Well, it was hard because obviously I love the company a lot, and I love, I love Elon,

1832
01:46:56,400 --> 01:46:57,400
I love Tesla.

1833
01:46:57,400 --> 01:47:00,120
I won't, um, it was so hard to leave.

1834
01:47:00,120 --> 01:47:06,440
I love the team, basically, um, but yeah, I think I actually, I will be potentially

1835
01:47:06,440 --> 01:47:11,480
like interested in revisiting it, maybe coming back at some point, uh, working optimists,

1836
01:47:11,480 --> 01:47:16,000
working in AGI at Tesla, uh, I think Tesla is going to do incredible things.

1837
01:47:16,000 --> 01:47:22,700
It basically like, uh, it's a massive, large scale robotics kind of company for the

1838
01:47:22,700 --> 01:47:25,480
ton of in house talent for doing really incredible things.

1839
01:47:25,480 --> 01:47:29,240
And I think, uh, human robots are going to be amazing.

1840
01:47:29,240 --> 01:47:32,000
Uh, I think autonomous transportation is going to be amazing.

1841
01:47:32,000 --> 01:47:33,000
All this is happening at Tesla.

1842
01:47:33,000 --> 01:47:35,680
So I think it's just a really amazing organization.

1843
01:47:35,680 --> 01:47:38,880
So being part of it and helping it alone, I think was very, basically, I enjoyed that

1844
01:47:38,880 --> 01:47:39,880
a lot.

1845
01:47:39,880 --> 01:47:43,520
Yeah, it was basically difficult for those reasons because I love the company, uh, but,

1846
01:47:43,520 --> 01:47:47,320
you know, I'm happy to potentially get something coming back for act two, but I felt like

1847
01:47:47,320 --> 01:47:53,640
at this stage, I built the team, it felt autonomous and, uh, I became a manager and I wanted

1848
01:47:53,640 --> 01:47:54,800
to do a lot more technical stuff.

1849
01:47:54,800 --> 01:47:55,800
I wanted to learn stuff.

1850
01:47:55,800 --> 01:48:00,040
I wanted to teach stuff, uh, and, uh, I, I just kind of felt like it was a good time

1851
01:48:00,040 --> 01:48:02,040
for, uh, for a change of pace a little bit.

1852
01:48:02,040 --> 01:48:06,600
What do you think is, uh, the best movie sequel of all time speaking of part two?

1853
01:48:06,600 --> 01:48:09,040
It's like, cause most of them suck.

1854
01:48:09,040 --> 01:48:10,040
Movie sequels?

1855
01:48:10,040 --> 01:48:11,040
Movie sequels, yeah.

1856
01:48:11,040 --> 01:48:16,040
And you tweeted about movies, so just in a tiny tangent, is there, what's your, what,

1857
01:48:16,040 --> 01:48:21,600
what's like a favorite movie sequel, Godfather part two, um, are you a fan of Godfather?

1858
01:48:21,600 --> 01:48:23,560
Cause you didn't even tweet or mention the Godfather.

1859
01:48:23,560 --> 01:48:24,560
Yeah.

1860
01:48:24,560 --> 01:48:25,560
I don't love that movie.

1861
01:48:25,560 --> 01:48:26,560
I know it hasn't added that out.

1862
01:48:26,560 --> 01:48:30,560
We're going to edit out the hate towards the Godfather, how dare you just make a strong

1863
01:48:30,560 --> 01:48:31,560
statement.

1864
01:48:31,560 --> 01:48:32,560
I don't know why.

1865
01:48:32,560 --> 01:48:37,680
I don't know why, but I basically don't like any movie before 1995.

1866
01:48:37,680 --> 01:48:38,680
Something like that.

1867
01:48:38,680 --> 01:48:39,680
Didn't you mention Terminator?

1868
01:48:39,680 --> 01:48:40,680
Two.

1869
01:48:40,680 --> 01:48:41,680
Okay.

1870
01:48:41,680 --> 01:48:42,680
Okay.

1871
01:48:42,680 --> 01:48:43,680
Okay.

1872
01:48:43,680 --> 01:48:45,720
That's like, uh, Terminator two was a little bit later 1990.

1873
01:48:45,720 --> 01:48:46,720
No.

1874
01:48:46,720 --> 01:48:48,960
I think Terminator two was in the, and I like Terminator one as well.

1875
01:48:48,960 --> 01:48:49,960
So, okay.

1876
01:48:49,960 --> 01:48:53,680
So like a few exceptions, but by and large, for some reason, I don't like movies before

1877
01:48:53,680 --> 01:48:55,520
1995 or something.

1878
01:48:55,520 --> 01:48:56,920
They feel very slow.

1879
01:48:56,920 --> 01:48:58,200
The camera is like zoomed out.

1880
01:48:58,200 --> 01:48:59,200
It's boring.

1881
01:48:59,200 --> 01:49:00,200
It's kind of naive.

1882
01:49:00,200 --> 01:49:01,200
It's kind of weird.

1883
01:49:01,200 --> 01:49:03,960
And also, Terminator was very much ahead of its time.

1884
01:49:03,960 --> 01:49:04,960
Yes.

1885
01:49:04,960 --> 01:49:06,960
And the Godfather, there's like no AGI.

1886
01:49:06,960 --> 01:49:14,080
That's, I mean, but you have good will hunting was one of the movies you mentioned.

1887
01:49:14,080 --> 01:49:15,760
And that doesn't have any AGI either.

1888
01:49:15,760 --> 01:49:16,760
I guess that's mathematics.

1889
01:49:16,760 --> 01:49:17,760
Yeah.

1890
01:49:17,760 --> 01:49:21,760
I guess occasionally I do enjoy movies that don't feature or like Anchorman.

1891
01:49:21,760 --> 01:49:24,680
That has no, that's, that's, that's so good.

1892
01:49:24,680 --> 01:49:26,200
I don't understand.

1893
01:49:26,200 --> 01:49:32,080
Um, he's speaking of AGI because I don't understand why wolf arrows so funny.

1894
01:49:32,080 --> 01:49:33,080
It doesn't make sense.

1895
01:49:33,080 --> 01:49:34,080
It doesn't compute.

1896
01:49:34,080 --> 01:49:35,560
There's just something about him.

1897
01:49:35,560 --> 01:49:39,960
And he's a singular human because you don't get that many comedies these days.

1898
01:49:39,960 --> 01:49:44,440
And I wonder if it has to do about the culture or the like the machine of Hollywood or

1899
01:49:44,440 --> 01:49:48,120
it does have to do with just we got lucky with certain people in comedy.

1900
01:49:48,120 --> 01:49:53,320
It came together because he is a singular human.

1901
01:49:53,320 --> 01:49:55,680
That was a ridiculous tangent, I apologize.

1902
01:49:55,680 --> 01:49:57,320
But you mentioned human robots.

1903
01:49:57,320 --> 01:50:01,960
So what do you think about Optimus, about Tesla, do you think we'll have robots in the

1904
01:50:01,960 --> 01:50:05,880
factory and in the home in 10, 20, 30, 40, 50 years?

1905
01:50:05,880 --> 01:50:06,880
Yeah.

1906
01:50:06,880 --> 01:50:07,880
I think it's a very hard project.

1907
01:50:07,880 --> 01:50:08,880
I think it's going to take a while.

1908
01:50:08,880 --> 01:50:11,680
But who else is going to build human robots at scale?

1909
01:50:11,680 --> 01:50:12,680
Yeah.

1910
01:50:12,680 --> 01:50:15,920
And I think it is a very good form factor to go after because like I mentioned, the world

1911
01:50:15,920 --> 01:50:17,920
is designed for human and form factor.

1912
01:50:17,920 --> 01:50:19,800
These things would be able to operate our machines.

1913
01:50:19,800 --> 01:50:23,760
They would be able to sit down in chairs, potentially even drive cars.

1914
01:50:23,760 --> 01:50:26,000
Uh, basically the world is designed for humans.

1915
01:50:26,000 --> 01:50:29,480
That's the form factor you want to invest into and make work over time.

1916
01:50:29,480 --> 01:50:33,080
Uh, I think, you know, there's another school of thought, which is, okay, pick a problem

1917
01:50:33,080 --> 01:50:36,600
and design a robot to it, but actually designing a robot and getting a whole data engine and

1918
01:50:36,600 --> 01:50:39,960
everything behind it to work is actually a really hard problem.

1919
01:50:39,960 --> 01:50:43,920
So it makes sense to go after general interfaces that, uh, okay, they are not perfect for

1920
01:50:43,920 --> 01:50:48,120
any one given task, but they actually have the generality of just with a prompt with

1921
01:50:48,120 --> 01:50:50,960
English able to do something across.

1922
01:50:50,960 --> 01:50:56,840
And so I think it makes a lot of sense to go after a general, uh, interface, um, in the

1923
01:50:56,840 --> 01:50:59,960
physical world, and I think it's a very difficult project.

1924
01:50:59,960 --> 01:51:04,480
It means going to take time, um, but I've seen no other, no other company that can execute

1925
01:51:04,480 --> 01:51:05,480
on that vision.

1926
01:51:05,480 --> 01:51:06,480
I think it's going to be amazing.

1927
01:51:06,480 --> 01:51:08,480
Like, uh, basically physical labor.

1928
01:51:08,480 --> 01:51:14,600
Like if you think transportation is a large market, try physical labor, but it's not

1929
01:51:14,600 --> 01:51:18,760
just physical labor to me, the thing that's also exciting is the social robotics.

1930
01:51:18,760 --> 01:51:22,840
So the, the relationship will have on different levels with those robots.

1931
01:51:22,840 --> 01:51:23,840
Yeah.

1932
01:51:23,840 --> 01:51:31,160
I was really excited to see optimists like, um, people have criticized me for the excitement.

1933
01:51:31,160 --> 01:51:38,400
But I've worked with, uh, a lot of research labs that do humanoid, legate robots, uh, Boston

1934
01:51:38,400 --> 01:51:42,960
Dynamics, Unitry, a lot, there's a lot of companies that do legate robots.

1935
01:51:42,960 --> 01:51:51,800
But that's the, the elegance of the movement is a tiny, tiny part of the big picture.

1936
01:51:51,800 --> 01:51:57,160
So integrating the two big exciting things to me about Tesla doing humanoid or any legate

1937
01:51:57,160 --> 01:52:03,120
robots is clearly integrating into the data engine.

1938
01:52:03,120 --> 01:52:05,240
So the, the data engine aspect.

1939
01:52:05,240 --> 01:52:09,760
So the actual intelligence for the perception of the, and the control and the planning and

1940
01:52:09,760 --> 01:52:14,800
all that kind of stuff, integrating into the huge, the fleet that you mentioned, right?

1941
01:52:14,800 --> 01:52:20,800
Um, and then speaking of fleet, the second thing is the mass manufacturers, just knowing,

1942
01:52:20,800 --> 01:52:29,160
uh, culturally, uh, driving towards a simple robot that's cheap to produce at scale.

1943
01:52:29,160 --> 01:52:30,160
Yeah.

1944
01:52:30,160 --> 01:52:32,640
And doing that well, having experience to do that well, that changes everything.

1945
01:52:32,640 --> 01:52:36,600
That's why that's a very different culture and style than Boston Dynamics.

1946
01:52:36,600 --> 01:52:41,960
Who, by the way, those, those robots are just the way they move.

1947
01:52:41,960 --> 01:52:46,320
It's, uh, like, it'll be a very long time before Tesla could achieve the smoothness of

1948
01:52:46,320 --> 01:52:47,320
movement.

1949
01:52:47,320 --> 01:52:51,960
And that's not what it's about, it's, it's about, uh, it's about the entirety of the

1950
01:52:51,960 --> 01:52:52,960
system.

1951
01:52:52,960 --> 01:52:55,000
Like we talked about the data engine and the fleet.

1952
01:52:55,000 --> 01:52:56,000
That's super exciting.

1953
01:52:56,000 --> 01:53:01,760
Even the initial sort of models, uh, but that too was really surprising that in a few months,

1954
01:53:01,760 --> 01:53:03,400
you can get a prototype.

1955
01:53:03,400 --> 01:53:04,400
Yep.

1956
01:53:04,400 --> 01:53:08,240
And the reason that happened very quickly is, as you alluded to, there's a ton of copy

1957
01:53:08,240 --> 01:53:10,960
paste from what's happening on the autopilot, a lot.

1958
01:53:10,960 --> 01:53:14,040
The amount of expertise that, like, came out of the woodworks at Tesla for building the

1959
01:53:14,040 --> 01:53:16,360
human robot was incredible to see.

1960
01:53:16,360 --> 01:53:19,520
Like basically Elon said at one point, we're doing this.

1961
01:53:19,520 --> 01:53:24,240
And then next day, basically, like, all these CAD models started to appear and people

1962
01:53:24,240 --> 01:53:27,000
talking about like the supply chain and manufacturing.

1963
01:53:27,000 --> 01:53:30,840
And people showed up with like screwdrivers and everything like the other day and started

1964
01:53:30,840 --> 01:53:34,760
to like put together the body and I was like, whoa, like all these people exist at Tesla.

1965
01:53:34,760 --> 01:53:37,720
And fundamentally building a car is actually not that different from building a robot.

1966
01:53:37,720 --> 01:53:42,760
The same, and that is true, uh, not just for, uh, the hardware pieces and also let's not

1967
01:53:42,760 --> 01:53:48,600
forget hardware, not just for demo, but, um, manufacturing of that hardware at scale.

1968
01:53:48,600 --> 01:53:50,400
It's like a whole different thing.

1969
01:53:50,400 --> 01:53:54,440
But for software as well, basically this robot currently thinks it's a car.

1970
01:53:54,440 --> 01:53:59,600
It's going to have a mid-life crisis at some point.

1971
01:53:59,600 --> 01:54:01,200
It thinks it's a car.

1972
01:54:01,200 --> 01:54:04,080
Some of the earlier demos, actually, we were talking about potentially doing them outside

1973
01:54:04,080 --> 01:54:07,280
in the parking lot because that's where all of the computer vision that was like working

1974
01:54:07,280 --> 01:54:11,840
out of the box instead of like inside.

1975
01:54:11,840 --> 01:54:15,680
But all the operating system, everything just copy-paste, uh, computer vision, mostly

1976
01:54:15,680 --> 01:54:16,680
copy-paste.

1977
01:54:16,680 --> 01:54:19,000
I mean, you have to retrain the neural nuts, but the approach on everything and data engine

1978
01:54:19,000 --> 01:54:22,600
and offline trackers and the way we go about the occupancy tracker and so on everything

1979
01:54:22,600 --> 01:54:23,600
copy-paste.

1980
01:54:23,600 --> 01:54:27,400
You just need to retrain the neural nuts, uh, and then the planning control of course

1981
01:54:27,400 --> 01:54:28,800
has to change quite a bit.

1982
01:54:28,800 --> 01:54:31,720
But there's a ton of copy-paste from what's happening at Tesla.

1983
01:54:31,720 --> 01:54:35,200
And so if you were to, if you were to go with goal of like, okay, let's build a million

1984
01:54:35,200 --> 01:54:38,760
human robots and you're not Tesla, that's, that's a lot to ask.

1985
01:54:38,760 --> 01:54:43,000
If you're Tesla, it's actually like, it's not, it's not that crazy.

1986
01:54:43,000 --> 01:54:46,840
And then the follow-up question is on how difficult, just like we're driving, how difficult

1987
01:54:46,840 --> 01:54:51,040
is the manipulation task, uh, such that it can have an impact at scale.

1988
01:54:51,040 --> 01:54:58,120
I think depending on the context, the really nice thing about robotics is that, um, unless

1989
01:54:58,120 --> 01:55:02,440
you do a manufacturer and that kind of stuff, is there is more room for error.

1990
01:55:02,440 --> 01:55:03,440
Yeah.

1991
01:55:03,440 --> 01:55:06,320
Driving is so safety critical and so that, and also time critical.

1992
01:55:06,320 --> 01:55:10,080
Okay, robot is a lot to move slower, which is nice.

1993
01:55:10,080 --> 01:55:11,080
Yes.

1994
01:55:11,080 --> 01:55:14,440
I think it's going to take a long time, but the way you want to structure the development

1995
01:55:14,440 --> 01:55:16,720
is you need to say, okay, it's going to take a long time.

1996
01:55:16,720 --> 01:55:21,760
How can I set up the, uh, product development roadmap so that I'm making revenue along

1997
01:55:21,760 --> 01:55:22,760
the way?

1998
01:55:22,760 --> 01:55:25,560
I'm not setting myself up for a zero one loss function where it doesn't work until it

1999
01:55:25,560 --> 01:55:26,560
works.

2000
01:55:26,560 --> 01:55:27,560
You don't want to be in that position.

2001
01:55:27,560 --> 01:55:29,720
You want to make it useful almost immediately.

2002
01:55:29,720 --> 01:55:34,640
And then you want to slowly deploy it, uh, and, uh, at scale, at scale.

2003
01:55:34,640 --> 01:55:39,080
And you want to set up your data engine, your improvement loops, the telemetry, the evaluation,

2004
01:55:39,080 --> 01:55:42,840
the harness and everything, um, and, uh, you want to improve the product over time

2005
01:55:42,840 --> 01:55:44,800
incrementally and you're making revenue along the way.

2006
01:55:44,800 --> 01:55:49,480
That's extremely important because otherwise you cannot build these, uh, large undertakings

2007
01:55:49,480 --> 01:55:51,400
just like don't make sense economically.

2008
01:55:51,400 --> 01:55:54,400
And also from the point of view of the team working on it, they need the dopamine along

2009
01:55:54,400 --> 01:55:55,400
the way.

2010
01:55:55,400 --> 01:55:58,840
They're not just going to make a promise about this being useful.

2011
01:55:58,840 --> 01:56:00,960
This is going to change the world in 10 years when it works.

2012
01:56:00,960 --> 01:56:02,440
This is not where you want to be.

2013
01:56:02,440 --> 01:56:05,960
You want to be in a place like, I think, out of all this today where it's offering increased

2014
01:56:05,960 --> 01:56:10,240
uh, safety and, um, and, uh, convenience of driving today.

2015
01:56:10,240 --> 01:56:11,240
People pay for it.

2016
01:56:11,240 --> 01:56:12,240
People like it.

2017
01:56:12,240 --> 01:56:13,240
People purchase it.

2018
01:56:13,240 --> 01:56:17,240
And then you also have the greater mission that you're working towards and you see that.

2019
01:56:17,240 --> 01:56:21,080
So the dopamine for the team that that was the source of happiness and, yes.

2020
01:56:21,080 --> 01:56:22,080
On the other side.

2021
01:56:22,080 --> 01:56:23,080
You're deploying this.

2022
01:56:23,080 --> 01:56:24,080
People like it.

2023
01:56:24,080 --> 01:56:25,080
People drive it.

2024
01:56:25,080 --> 01:56:26,080
People pay for it.

2025
01:56:26,080 --> 01:56:27,080
They care about it.

2026
01:56:27,080 --> 01:56:28,080
There's all these YouTube videos.

2027
01:56:28,080 --> 01:56:29,080
Your grandma drives it.

2028
01:56:29,080 --> 01:56:30,080
She gives you feedback.

2029
01:56:30,080 --> 01:56:31,080
People like it.

2030
01:56:31,080 --> 01:56:32,080
People engage with it.

2031
01:56:32,080 --> 01:56:37,240
Do people that drive Teslas like recognize you and give you love like, uh, like, hey,

2032
01:56:37,240 --> 01:56:42,120
thanks for the, for the, uh, this nice feature that is doing.

2033
01:56:42,120 --> 01:56:43,120
Yeah.

2034
01:56:43,120 --> 01:56:44,920
I think the tricky thing is like, some people really love you.

2035
01:56:44,920 --> 01:56:47,600
Some people unfortunately like, you're working on something that you think is extremely

2036
01:56:47,600 --> 01:56:49,160
valuable, useful, etc.

2037
01:56:49,160 --> 01:56:50,200
Some people do hate you.

2038
01:56:50,200 --> 01:56:55,560
There's not a lot of people who like hate me and the team and what the whole project.

2039
01:56:55,560 --> 01:56:59,360
And I think, uh, they Tesla drivers, uh, in many cases, they're not actually.

2040
01:56:59,360 --> 01:57:00,360
Yeah.

2041
01:57:00,360 --> 01:57:06,440
And that's actually makes me sad about humans or the current ways that humans interact.

2042
01:57:06,440 --> 01:57:07,760
I think that's actually fixable.

2043
01:57:07,760 --> 01:57:09,480
I think humans want to be good to each other.

2044
01:57:09,480 --> 01:57:14,320
I think Twitter and social media is part of the mechanism that actually somehow makes

2045
01:57:14,320 --> 01:57:20,360
the negativity more viral that it doesn't deserve like disproportionately, uh, add of

2046
01:57:20,360 --> 01:57:23,760
like a viral viral boost of negativity.

2047
01:57:23,760 --> 01:57:28,120
But I got, uh, I wish people would just get excited about, uh, so suppress some of the

2048
01:57:28,120 --> 01:57:32,320
jealousy, um, some of the ego and just get excited for others.

2049
01:57:32,320 --> 01:57:36,000
And then there's a karma aspect to that you get excited for others, they'll get excited

2050
01:57:36,000 --> 01:57:37,000
for you.

2051
01:57:37,000 --> 01:57:41,480
Same thing in academia, if you're not careful, there is a, like a dynamical system there.

2052
01:57:41,480 --> 01:57:46,280
If you, if you think of in silos and get jealous of somebody else being successful,

2053
01:57:46,280 --> 01:57:52,200
that actually perhaps counterintuitively, uh, least the less productivity of us, the community

2054
01:57:52,200 --> 01:57:53,600
and you individually.

2055
01:57:53,600 --> 01:57:59,200
I feel like if you keep celebrating others that actually makes you more successful.

2056
01:57:59,200 --> 01:58:00,200
Yeah.

2057
01:58:00,200 --> 01:58:03,680
And I think people haven't, depending on the industry, haven't quite learned that yet.

2058
01:58:03,680 --> 01:58:04,680
Yeah.

2059
01:58:04,680 --> 01:58:06,280
Some people are also very negative and very vocal.

2060
01:58:06,280 --> 01:58:07,800
So they're very prominently featured.

2061
01:58:07,800 --> 01:58:12,680
But actually, there's a ton of people who are a cheerleaders, but they're silent cheerleaders.

2062
01:58:12,680 --> 01:58:17,120
And, uh, when you talk to people just in the world, they will all tell you it's amazing.

2063
01:58:17,120 --> 01:58:20,480
It's great, especially like people who understand how difficult it is to get this stuff working.

2064
01:58:20,480 --> 01:58:24,520
Some people who have built products and makers and entrepreneurs, entrepreneurs, like making

2065
01:58:24,520 --> 01:58:28,760
this work and changing something is incredibly hard.

2066
01:58:28,760 --> 01:58:30,720
Those people are more likely to cheerlead you.

2067
01:58:30,720 --> 01:58:34,800
Well, one of the things that makes me sad is some folks in the robotics community, you

2068
01:58:34,800 --> 01:58:37,440
know, don't do the cheerleading than they should.

2069
01:58:37,440 --> 01:58:39,240
There's, uh, because they know how difficult it is.

2070
01:58:39,240 --> 01:58:42,200
Well, they actually sometimes don't know how difficult it is to create a product that

2071
01:58:42,200 --> 01:58:43,200
scale, right?

2072
01:58:43,200 --> 01:58:44,200
Yeah.

2073
01:58:44,200 --> 01:58:49,960
They actually deployed in the real world a lot of the development of robots and AI system

2074
01:58:49,960 --> 01:58:55,720
is done on very specific small benchmarks, um, and as opposed to real world conditions.

2075
01:58:55,720 --> 01:58:56,720
Yes.

2076
01:58:56,720 --> 01:58:57,720
Yeah.

2077
01:58:57,720 --> 01:59:01,280
I think it's really hard to work on robotics in academic setting or AI systems that apply

2078
01:59:01,280 --> 01:59:02,280
in the real world.

2079
01:59:02,280 --> 01:59:09,560
You've criticized you, um, flourished and loved for time, the image net, the famed

2080
01:59:09,560 --> 01:59:16,480
image net data set, and it recently had some words, uh, of criticism that the academic

2081
01:59:16,480 --> 01:59:22,240
research, ML community gives a little too much love still to the image net or like those

2082
01:59:22,240 --> 01:59:27,600
kinds of benchmarks, can you speak to the strengths, the weaknesses of data sets used

2083
01:59:27,600 --> 01:59:29,560
in machine learning research?

2084
01:59:29,560 --> 01:59:34,720
Actually, I don't know that I recall the specific instance where I was, uh, unhappy or

2085
01:59:34,720 --> 01:59:35,720
criticizing image net.

2086
01:59:35,720 --> 01:59:41,520
I think image net has been extremely valuable, uh, it was basically a benchmark that allowed

2087
01:59:41,520 --> 01:59:45,960
the deep learning community to demonstrate that deep neural works actually work.

2088
01:59:45,960 --> 01:59:48,840
It was, uh, there's a massive value in that.

2089
01:59:48,840 --> 01:59:53,520
Um, so I think image net was useful, but I'm basically it's become a bit of an emnist

2090
01:59:53,520 --> 01:59:54,520
at this point.

2091
01:59:54,520 --> 01:59:59,040
So emnist is like little 228 by 28 grayscale digits, there's kind of a joke data set that

2092
01:59:59,040 --> 02:00:03,000
everyone like crushes, you know, papers written on emnist, though, right?

2093
02:00:03,000 --> 02:00:07,280
Maybe they shouldn't have strong papers, like, uh, papers that focus on like, how do we

2094
02:00:07,280 --> 02:00:09,280
learn with a small amount of data that stuff?

2095
02:00:09,280 --> 02:00:10,280
Yeah.

2096
02:00:10,280 --> 02:00:12,360
Yeah, I could see that being helpful, but not in sort of like mainline, computer vision

2097
02:00:12,360 --> 02:00:13,360
research anymore, of course.

2098
02:00:13,360 --> 02:00:17,840
I think the way I've heard you somewhere, maybe I'm just imagining things, but I think

2099
02:00:17,840 --> 02:00:21,440
you said, like, image net was a huge contribution to the community for a long time, and now it's

2100
02:00:21,440 --> 02:00:24,240
time to move past those kinds of, well, image net has been crushed.

2101
02:00:24,240 --> 02:00:31,400
I mean, you know, the error rates are, uh, yeah, we're getting like 90% accuracy in, in

2102
02:00:31,400 --> 02:00:35,160
1,000 classification way, uh, prediction.

2103
02:00:35,160 --> 02:00:39,000
And I've seen those images and it's like really high.

2104
02:00:39,000 --> 02:00:40,440
This really, that's really good.

2105
02:00:40,440 --> 02:00:45,800
The timer correctly, the top five error rate is now like 1% or something given your experience

2106
02:00:45,800 --> 02:00:48,000
with a gigantic real world data set.

2107
02:00:48,000 --> 02:00:51,840
Would you like to see benchmarks move in a certain directions that the research community

2108
02:00:51,840 --> 02:00:52,840
uses?

2109
02:00:52,840 --> 02:00:55,600
Unfortunately, I don't think academics currently have the next image net.

2110
02:00:55,600 --> 02:00:57,880
Uh, we've obviously, I think we've crushed emnist.

2111
02:00:57,880 --> 02:01:02,760
We've basically kind of crushed image net, uh, and there's no next sort of big benchmark

2112
02:01:02,760 --> 02:01:08,320
that the entire community rail is behind and uses, um, you know, for further development

2113
02:01:08,320 --> 02:01:09,320
of these networks.

2114
02:01:09,320 --> 02:01:13,400
I, yeah, what it would it takes for a data set to captivate the imagination of everybody,

2115
02:01:13,400 --> 02:01:18,560
like where they all get behind it, that that could also need like a, like a leader.

2116
02:01:18,560 --> 02:01:19,560
Right?

2117
02:01:19,560 --> 02:01:20,560
Yeah.

2118
02:01:20,560 --> 02:01:21,560
Somebody with popularity.

2119
02:01:21,560 --> 02:01:24,360
I mean, uh, yeah, what, what did image net take off?

2120
02:01:24,360 --> 02:01:26,560
Is there, is it just the accident of history?

2121
02:01:26,560 --> 02:01:31,600
It was the right amount of difficult, uh, it was the right amount of difficult and simple

2122
02:01:31,600 --> 02:01:35,640
and, uh, interesting enough, it just kind of like it was, it was the right time for that

2123
02:01:35,640 --> 02:01:37,960
kind of a data set.

2124
02:01:37,960 --> 02:01:43,960
And from Reddit, uh, what are your thoughts on the role of the synthetic data and game

2125
02:01:43,960 --> 02:01:48,320
engines will play in the future of neural net model development?

2126
02:01:48,320 --> 02:01:55,720
I think, um, as neural nets converge to humans, uh, the value of simulation to neural nets

2127
02:01:55,720 --> 02:01:59,760
will be similar to value of simulation to humans.

2128
02:01:59,760 --> 02:02:04,200
So people use simulation for, uh, people do simulation because they can learn something

2129
02:02:04,200 --> 02:02:08,920
in that kind of a system, um, and, uh, without having to actually experience it.

2130
02:02:08,920 --> 02:02:11,920
Um, but are you referring to the simulation with doing our head?

2131
02:02:11,920 --> 02:02:12,920
Is it?

2132
02:02:12,920 --> 02:02:13,920
Uh, no, sorry, simulation.

2133
02:02:13,920 --> 02:02:18,960
I mean, like video games or, uh, you know, um, other forms of simulation for various

2134
02:02:18,960 --> 02:02:19,960
professionals.

2135
02:02:19,960 --> 02:02:23,080
Well, so let me push back at that because that maybe there's simulation that we do in

2136
02:02:23,080 --> 02:02:28,600
our heads, like, simulate, if I do this, what do I think will happen?

2137
02:02:28,600 --> 02:02:29,600
Okay.

2138
02:02:29,600 --> 02:02:30,600
That's like internal simulation.

2139
02:02:30,600 --> 02:02:31,600
Yeah, internal.

2140
02:02:31,600 --> 02:02:32,600
Isn't that what we're doing?

2141
02:02:32,600 --> 02:02:33,600
Assume it before we act?

2142
02:02:33,600 --> 02:02:36,760
Yeah, but that's independent from like the use of simulation in the sense of like computer

2143
02:02:36,760 --> 02:02:41,160
games or using simulation for training set creation or, you know, is it independent

2144
02:02:41,160 --> 02:02:43,000
or is it just loosely correlated?

2145
02:02:43,000 --> 02:02:50,680
Cause like, uh, isn't that useful to do like, um, counterfactual or like edge case simulation

2146
02:02:50,680 --> 02:02:55,160
to like, you know, what happens if there's a nuclear war?

2147
02:02:55,160 --> 02:02:59,000
What happens if there's, you know, like those kinds of things, yeah, that's a different

2148
02:02:59,000 --> 02:03:02,520
simulation from like Unreal Engine, that's how I interpreted the question.

2149
02:03:02,520 --> 02:03:03,520
Yeah.

2150
02:03:03,520 --> 02:03:11,080
So like simulation of the average case is that what's Unreal Engine, what do you mean by

2151
02:03:11,080 --> 02:03:12,080
Unreal Engine?

2152
02:03:12,080 --> 02:03:18,640
So simulating a world, physics of that world, why is that different?

2153
02:03:18,640 --> 02:03:23,520
Like cause you also can add behavior to that world and you could try all kinds of stuff,

2154
02:03:23,520 --> 02:03:24,520
right?

2155
02:03:24,520 --> 02:03:27,120
You could throw all kinds of weird things into it.

2156
02:03:27,120 --> 02:03:31,520
So Unreal Engine is not just about simulating, I mean, I guess it is about simulating the

2157
02:03:31,520 --> 02:03:32,520
physics of the world.

2158
02:03:32,520 --> 02:03:34,840
It's also doing something with that.

2159
02:03:34,840 --> 02:03:38,840
Yeah, the graphics, the physics and the agents that you put into the environment and stuff

2160
02:03:38,840 --> 02:03:39,840
like that.

2161
02:03:39,840 --> 02:03:40,840
Yeah.

2162
02:03:40,840 --> 02:03:43,840
See, I think you feel, I feel like you said that it's not that important, I guess, for

2163
02:03:43,840 --> 02:03:45,520
the future of AI development.

2164
02:03:45,520 --> 02:03:48,320
Is that correct to interpret it either way?

2165
02:03:48,320 --> 02:03:54,640
Well, I think, uh, I think humans use simulators for, um, humans use simulators and they find

2166
02:03:54,640 --> 02:03:55,640
them useful.

2167
02:03:55,640 --> 02:03:58,520
And so computers will use simulators and find them useful.

2168
02:03:58,520 --> 02:03:59,520
Okay.

2169
02:03:59,520 --> 02:04:02,800
So you're saying it's not like, I don't use simulators very often, I play a video game

2170
02:04:02,800 --> 02:04:07,360
every once in a while, but I don't think I derive any wisdom about my own existence

2171
02:04:07,360 --> 02:04:09,440
from, from those video games.

2172
02:04:09,440 --> 02:04:14,960
It's a momentary escape from reality versus a source of wisdom about reality.

2173
02:04:14,960 --> 02:04:19,480
So I don't, so I think that's a very polite way of saying simulation is not that useful.

2174
02:04:19,480 --> 02:04:24,280
Yeah, maybe, maybe not, I don't see it as like a fundamental, really important part of

2175
02:04:24,280 --> 02:04:27,080
like training neural nets currently.

2176
02:04:27,080 --> 02:04:31,200
But I think, uh, as neural nets become more and more powerful, I think you will need fewer

2177
02:04:31,200 --> 02:04:34,680
examples to train additional behaviors.

2178
02:04:34,680 --> 02:04:38,600
And, uh, simulation is, of course, there's a domain gap in a simulation that is not the

2179
02:04:38,600 --> 02:04:39,600
real world.

2180
02:04:39,600 --> 02:04:40,720
It's slightly something different.

2181
02:04:40,720 --> 02:04:45,400
But, uh, with a powerful enough neural net, uh, you need, um, the domain gap can be bigger,

2182
02:04:45,400 --> 02:04:48,280
I think, because neural net will sort of understand that even though it's not the real

2183
02:04:48,280 --> 02:04:52,400
world, it like has all this high level structure that I'm supposed to be learning from.

2184
02:04:52,400 --> 02:04:58,720
So the neural net will actually, yeah, we'll be able to leverage the synthetic data better

2185
02:04:58,720 --> 02:05:04,560
yes, by closing the gap, but understanding in which ways this is not real data.

2186
02:05:04,560 --> 02:05:05,560
Exactly.

2187
02:05:05,560 --> 02:05:09,920
Uh, right to do better questions next time that was, that was a question that I'm just

2188
02:05:09,920 --> 02:05:10,920
kidding.

2189
02:05:10,920 --> 02:05:11,920
All right.

2190
02:05:11,920 --> 02:05:19,080
Um, so is it possible, do you think speaking of MNIST to construct neural nets and training

2191
02:05:19,080 --> 02:05:23,400
processes that require very little data?

2192
02:05:23,400 --> 02:05:26,160
So we've been talking about huge data sets like the internet for training.

2193
02:05:26,160 --> 02:05:27,160
Yeah.

2194
02:05:27,160 --> 02:05:30,200
I mean, yeah, one way to say that is like you said, like the querying itself is another

2195
02:05:30,200 --> 02:05:37,000
level of training, I guess, and that requires a little data, but do you see any, uh, value

2196
02:05:37,000 --> 02:05:43,160
in doing research and, uh, going down in the direction of, can we use very little data

2197
02:05:43,160 --> 02:05:45,520
to train to construct a knowledge base?

2198
02:05:45,520 --> 02:05:46,520
100%.

2199
02:05:46,520 --> 02:05:49,200
I just think like at some point, you need a massive data set.

2200
02:05:49,200 --> 02:05:53,240
And then when you pre train your massive neural net and get something that, you know, is

2201
02:05:53,240 --> 02:05:57,960
like a GPT or something, then you're able to be very efficient at training in your return

2202
02:05:57,960 --> 02:05:58,960
new task.

2203
02:05:58,960 --> 02:06:04,320
Uh, so a lot of these GPTs, you know, you can do tasks like sentiment analysis or translation

2204
02:06:04,320 --> 02:06:07,120
or so on just by being prompted with very few examples.

2205
02:06:07,120 --> 02:06:10,520
Here's the kind of thing I want you to do, like here's an input sentence, here's the translation

2206
02:06:10,520 --> 02:06:15,160
into German, input sentence, translation to German, input sentence blank and the neural

2207
02:06:15,160 --> 02:06:19,160
net will complete the translation to German just by looking at sort of the example you've

2208
02:06:19,160 --> 02:06:20,160
provided.

2209
02:06:20,160 --> 02:06:24,720
And so that's an example of a very few shot, uh, learning in the activations of the neural

2210
02:06:24,720 --> 02:06:26,600
net, instead of the weights of the neural net.

2211
02:06:26,600 --> 02:06:31,240
And so I think, um, basically, uh, just like humans, neural nets will become very data

2212
02:06:31,240 --> 02:06:35,560
efficient at learning any other new task, but at some point, you need a massive data set

2213
02:06:35,560 --> 02:06:41,720
to pre train your network to get that and probably we humans have something like that.

2214
02:06:41,720 --> 02:06:43,000
Do we, do we have something like that?

2215
02:06:43,000 --> 02:06:50,600
Do we have a passive in the background, background model constructing thing?

2216
02:06:50,600 --> 02:06:54,240
They just runs all the time in a self supervised way, we're not conscious of it.

2217
02:06:54,240 --> 02:06:58,560
I think humans definitely, I mean, obviously we have, uh, we learn a lot during, during

2218
02:06:58,560 --> 02:07:04,240
our life span, but also, uh, we have a ton of hardware that helps us initialize, initialization

2219
02:07:04,240 --> 02:07:06,360
coming from sort of evolution.

2220
02:07:06,360 --> 02:07:09,800
And so I think that's also a really big, big component, a lot of people in the field,

2221
02:07:09,800 --> 02:07:13,560
I think they just talk about the amounts of like seconds and the, you know, that a person

2222
02:07:13,560 --> 02:07:18,000
has lived, pretending that this is a WLARASA, sort of like a zero initialization of a neural

2223
02:07:18,000 --> 02:07:19,000
net.

2224
02:07:19,000 --> 02:07:22,920
And it's not like, you can look at a lot of animals, like, for example, Zebras, Zebras

2225
02:07:22,920 --> 02:07:27,200
get born and they see and they can run.

2226
02:07:27,200 --> 02:07:30,760
There's zero trained data in their life span, they can just do that.

2227
02:07:30,760 --> 02:07:35,360
So somehow I have no idea how evolution has found a way to encode these algorithms and

2228
02:07:35,360 --> 02:07:39,480
these neural net initializations that are extremely good into ATCGs and I have no idea

2229
02:07:39,480 --> 02:07:44,440
how this works, but apparently it's possible because here's a proof by existence.

2230
02:07:44,440 --> 02:07:50,160
There's something magical about going from a single cell to an organism that is born

2231
02:07:50,160 --> 02:07:51,600
to the first few years of life.

2232
02:07:51,600 --> 02:07:55,560
I kind of like the idea that the reason we don't remember anything about the first few

2233
02:07:55,560 --> 02:07:59,640
years of our life is that it's a really painful process.

2234
02:07:59,640 --> 02:08:03,400
Like it's a very difficult challenging training process.

2235
02:08:03,400 --> 02:08:04,400
Yeah.

2236
02:08:04,400 --> 02:08:05,800
Like intellectually.

2237
02:08:05,800 --> 02:08:06,800
Like.

2238
02:08:06,800 --> 02:08:11,880
And maybe, yeah, why don't we remember any of that?

2239
02:08:11,880 --> 02:08:19,520
There might be some crazy training going on and maybe that's the background model training

2240
02:08:19,520 --> 02:08:23,240
that is very painful.

2241
02:08:23,240 --> 02:08:27,800
And so it's best for the system once it's trained not to remember how it's constructed.

2242
02:08:27,800 --> 02:08:31,720
I think it's just like the hardware for long-term memory is just not fully developed.

2243
02:08:31,720 --> 02:08:32,720
Sure.

2244
02:08:32,720 --> 02:08:37,120
The first few years of infants is not actually like learning.

2245
02:08:37,120 --> 02:08:38,440
It's brain maturing.

2246
02:08:38,440 --> 02:08:39,440
Yeah.

2247
02:08:39,440 --> 02:08:42,040
We're born premature.

2248
02:08:42,040 --> 02:08:45,640
There's a theory along those lines because of the birth canal and this, along the brain.

2249
02:08:45,640 --> 02:08:49,680
And so we're born premature and then the first few years we're just the brain's maturing.

2250
02:08:49,680 --> 02:08:51,920
And then there's some learning eventually.

2251
02:08:51,920 --> 02:08:53,960
That's my current view on it.

2252
02:08:53,960 --> 02:09:01,160
What do you think neural nets can have long-term memory, like that approach is something

2253
02:09:01,160 --> 02:09:02,160
like humans?

2254
02:09:02,160 --> 02:09:07,320
Do you think there needs to be another meta-architecture on top of it to add something like a knowledge

2255
02:09:07,320 --> 02:09:10,600
base that learns facts about the world and all that kind of stuff?

2256
02:09:10,600 --> 02:09:15,920
Yes, but I don't know to what extent it will be explicitly constructed.

2257
02:09:15,920 --> 02:09:22,080
It might take unintuitive forms where you are telling the GPT, like, you have a declarative

2258
02:09:22,080 --> 02:09:25,360
memory bank to which you can store and retrieve data from.

2259
02:09:25,360 --> 02:09:28,560
And whenever you encounter some information that you find useful, just save it to your

2260
02:09:28,560 --> 02:09:30,000
memory bank.

2261
02:09:30,000 --> 02:09:33,400
And here's an example of something you have retrieved and how you say it.

2262
02:09:33,400 --> 02:09:34,520
And here's how you load from it.

2263
02:09:34,520 --> 02:09:39,320
You just say load whatever you teach it in text in English.

2264
02:09:39,320 --> 02:09:42,440
And then it might learn to use a memory bank from that.

2265
02:09:42,440 --> 02:09:48,320
Oh, so the neural net is the architecture for the background model, the base thing and

2266
02:09:48,320 --> 02:09:50,320
then everything else is just on top of it.

2267
02:09:50,320 --> 02:09:51,320
It's not just text, right?

2268
02:09:51,320 --> 02:09:53,120
It's, you're giving it gadgets and gizmos.

2269
02:09:53,120 --> 02:09:58,120
So you're teaching some kind of a special language by which it can save arbitrary information

2270
02:09:58,120 --> 02:09:59,800
and retrieve it at a later time.

2271
02:09:59,800 --> 02:10:04,120
And you're telling about these special tokens and how to arrange them to use these interfaces.

2272
02:10:04,120 --> 02:10:06,560
And it's like, hey, you can use a calculator.

2273
02:10:06,560 --> 02:10:07,560
Here's how you use it.

2274
02:10:07,560 --> 02:10:10,360
Just do 5, 3 plus 4, 1 equals.

2275
02:10:10,360 --> 02:10:14,800
And when equals is there, a calculator will actually read out the answer and you don't

2276
02:10:14,800 --> 02:10:16,440
have to calculate it yourself.

2277
02:10:16,440 --> 02:10:17,640
And you just like tell it in English.

2278
02:10:17,640 --> 02:10:18,640
This might actually work.

2279
02:10:18,640 --> 02:10:24,280
Do you think, in that sense, God was interesting, the deep mind system that it's not just

2280
02:10:24,280 --> 02:10:31,080
in the language, but actually throws it all in the same pile, images, actions, all that

2281
02:10:31,080 --> 02:10:32,080
kind of stuff.

2282
02:10:32,080 --> 02:10:34,080
That's basically what we're moving towards.

2283
02:10:34,080 --> 02:10:35,080
Yeah, I think so.

2284
02:10:35,080 --> 02:10:41,960
So Goddo is very much a kitchen sink of approach to like reinforcement learning lots of different

2285
02:10:41,960 --> 02:10:46,680
environments with a single fixed transformer model, right?

2286
02:10:46,680 --> 02:10:50,040
I think it's a very sort of early result in that realm.

2287
02:10:50,040 --> 02:10:52,960
But I think, yeah, it's along the lines of what I think things will eventually look

2288
02:10:52,960 --> 02:10:53,960
like.

2289
02:10:53,960 --> 02:10:57,640
So this is the early days of a system that eventually will look like this, like from a

2290
02:10:57,640 --> 02:10:59,640
rich side in perspective.

2291
02:10:59,640 --> 02:11:04,920
Yeah, I'm not super huge fan of, I think, all these interfaces that look very different.

2292
02:11:04,920 --> 02:11:07,520
I would want everything to be normalized into the same API.

2293
02:11:07,520 --> 02:11:11,920
So for example, screen pixels, very same API, instead of having like different world environments

2294
02:11:11,920 --> 02:11:15,760
that are very different physics and joint configurations and appearances and whatever.

2295
02:11:15,760 --> 02:11:19,600
And you're having some kind of special tokens for different games that you can plug.

2296
02:11:19,600 --> 02:11:22,680
I'd rather just normalize everything to a single interface.

2297
02:11:22,680 --> 02:11:24,280
So it looks the same to the neural lot.

2298
02:11:24,280 --> 02:11:25,280
If that makes sense.

2299
02:11:25,280 --> 02:11:27,640
So it's all going to be pixel-based pong in the end.

2300
02:11:27,640 --> 02:11:29,640
I think so.

2301
02:11:29,640 --> 02:11:35,880
Okay, let me ask you about your own personal life.

2302
02:11:35,880 --> 02:11:39,480
A lot of people want to know you're one of the most productive and brilliant people in

2303
02:11:39,480 --> 02:11:40,480
the history of AI.

2304
02:11:40,480 --> 02:11:44,640
What does a productive day in the life of Andre Kapati look like?

2305
02:11:44,640 --> 02:11:45,640
What time do you wake up?

2306
02:11:45,640 --> 02:11:46,640
You should look.

2307
02:11:46,640 --> 02:11:51,640
Because imagine some kind of dance between the average productive day and a perfect productive

2308
02:11:51,640 --> 02:11:52,640
day.

2309
02:11:52,640 --> 02:11:57,720
A productive day is the thing we strive towards in the average is kind of what it converges

2310
02:11:57,720 --> 02:11:58,720
to.

2311
02:11:58,720 --> 02:12:01,880
And all the mistakes and human eventualities and so on.

2312
02:12:01,880 --> 02:12:02,880
So what times you wake up?

2313
02:12:02,880 --> 02:12:04,440
Are you a morning person?

2314
02:12:04,440 --> 02:12:05,600
I'm not a morning person.

2315
02:12:05,600 --> 02:12:07,360
I'm a night owl for sure.

2316
02:12:07,360 --> 02:12:09,080
Is it stable or not?

2317
02:12:09,080 --> 02:12:12,600
It's semi-stable like eight or nine or something like that.

2318
02:12:12,600 --> 02:12:14,520
During my PhD, it was even later.

2319
02:12:14,520 --> 02:12:16,680
I used to go to sleep usually at 3 a.m.

2320
02:12:16,680 --> 02:12:21,720
I think the a.m. hours are precious and very interesting time to work because everyone

2321
02:12:21,720 --> 02:12:23,520
has a sleep.

2322
02:12:23,520 --> 02:12:26,400
At 8 a.m. or 7 a.m., the east coast is awake.

2323
02:12:26,400 --> 02:12:27,560
So there's already activity.

2324
02:12:27,560 --> 02:12:29,320
There's already some text messages, whatever.

2325
02:12:29,320 --> 02:12:30,320
There's stuff happening.

2326
02:12:30,320 --> 02:12:34,280
You can go on like some news website and there's stuff happening and distracting.

2327
02:12:34,280 --> 02:12:36,760
At 3 a.m., everything is totally quiet.

2328
02:12:36,760 --> 02:12:42,280
And so you're not going to be bothered and you have solid chunks of time to do work.

2329
02:12:42,280 --> 02:12:45,360
So I like those periods, night owl by default.

2330
02:12:45,360 --> 02:12:50,160
And then I think like productive time, basically, what I like to do is you need to like build

2331
02:12:50,160 --> 02:12:53,920
some momentum on the problem without too much distraction.

2332
02:12:53,920 --> 02:13:00,560
And you need to load your RAM, your working memory with that problem.

2333
02:13:00,560 --> 02:13:03,040
And then you need to be obsessed with it when you're taking shower, when you're falling

2334
02:13:03,040 --> 02:13:04,200
asleep.

2335
02:13:04,200 --> 02:13:07,320
You need to be obsessed with the problem and it's fully in your memory and you're ready

2336
02:13:07,320 --> 02:13:09,080
to wake up and work on it right there.

2337
02:13:09,080 --> 02:13:13,760
So it is a skill of, is this in a scale, temporal scale of a single day or a couple of

2338
02:13:13,760 --> 02:13:15,240
days a week or a month?

2339
02:13:15,240 --> 02:13:19,360
So I can't talk about one day basically in isolation because it's a whole process

2340
02:13:19,360 --> 02:13:23,320
when I want to get, when I want to get productive in the problem, I feel like I need a span

2341
02:13:23,320 --> 02:13:27,600
of a few days where I can really get in on that problem and I don't want to be interrupted

2342
02:13:27,600 --> 02:13:31,320
and I'm going to just be completely obsessed with that problem.

2343
02:13:31,320 --> 02:13:34,240
And that's where I do most of my good workouts.

2344
02:13:34,240 --> 02:13:38,560
You've done a bunch of cool like little projects in a very short amount of time, very quickly.

2345
02:13:38,560 --> 02:13:40,600
So that requires you just focusing on it.

2346
02:13:40,600 --> 02:13:44,360
Yeah, basically, I need to load my working memory with the problem and I need to be productive

2347
02:13:44,360 --> 02:13:49,640
because there's always like a huge fixed cost to approaching any problem.

2348
02:13:49,640 --> 02:13:52,720
I was struggling with this, for example, at Tesla because I want to work on a small

2349
02:13:52,720 --> 02:13:53,720
side project.

2350
02:13:53,720 --> 02:13:56,520
But okay, you first need to figure out, okay, I need to SSH into my cluster.

2351
02:13:56,520 --> 02:14:00,080
I need to bring up a VS code editor so I can work on this.

2352
02:14:00,080 --> 02:14:03,120
I need to, I run into some stupid error because of some reason.

2353
02:14:03,120 --> 02:14:05,760
You're not at a point where you can be just productive right away.

2354
02:14:05,760 --> 02:14:07,680
You are facing barriers.

2355
02:14:07,680 --> 02:14:12,840
And so it's about really removing all that barrier and you're able to go into the problem

2356
02:14:12,840 --> 02:14:15,520
and you have the full problem loaded in your memory.

2357
02:14:15,520 --> 02:14:22,280
And somehow avoiding distractions of all different forms, like news stories, emails, but also

2358
02:14:22,280 --> 02:14:26,600
distractions from other interesting projects that you previously worked out, are currently

2359
02:14:26,600 --> 02:14:28,120
working on and so on.

2360
02:14:28,120 --> 02:14:29,680
You just want to really focus your mind.

2361
02:14:29,680 --> 02:14:33,920
And I mean, I can take some time off for distractions and in between, but I think it can't

2362
02:14:33,920 --> 02:14:34,920
be too much.

2363
02:14:34,920 --> 02:14:38,240
You know, most of your day is sort of like spent on that problem.

2364
02:14:38,240 --> 02:14:44,320
And then, you know, I drink coffee, I have my morning routine, I look at some news, Twitter,

2365
02:14:44,320 --> 02:14:46,800
hacker news, Wall Street Journal, etc.

2366
02:14:46,800 --> 02:14:51,480
So you wake up, you have some coffee, are you trying to get to work as quickly as possible?

2367
02:14:51,480 --> 02:14:56,480
Do you take in this diet of like what the hell is happening in the world first?

2368
02:14:56,480 --> 02:14:59,680
I am, I do find it interesting to know about the world.

2369
02:14:59,680 --> 02:15:03,720
I don't know that it's useful or good, but it is part of my routine right now.

2370
02:15:03,720 --> 02:15:09,000
So I do read through a bunch of news articles and I want to be informed and I'm suspicious

2371
02:15:09,000 --> 02:15:10,000
of it.

2372
02:15:10,000 --> 02:15:12,360
I'm suspicious of the practice, but currently that's where I am.

2373
02:15:12,360 --> 02:15:18,000
Oh, you mean suspicious about the positive effect of that practice on your productivity

2374
02:15:18,000 --> 02:15:21,240
and your well-being is my well-being psychologically.

2375
02:15:21,240 --> 02:15:25,680
And also on your ability to deeply understand the world because there's a bunch of sources

2376
02:15:25,680 --> 02:15:26,680
of information.

2377
02:15:26,680 --> 02:15:28,760
You're not really focused on deeply integrating it.

2378
02:15:28,760 --> 02:15:33,480
That's loaded distracting, you're in terms of a perfectly productive day.

2379
02:15:33,480 --> 02:15:39,520
Or how long of a stretch of time in one session do you try to work and focus on a thing?

2380
02:15:39,520 --> 02:15:43,600
A couple hours, there's one hours at 30 minutes, there's 10 minutes.

2381
02:15:43,600 --> 02:15:47,520
I can probably go like a small few hours and then I need some breaks in between for like

2382
02:15:47,520 --> 02:15:48,920
food and stuff.

2383
02:15:48,920 --> 02:15:53,560
And yeah, but I think like it's still really hard to accumulate hours.

2384
02:15:53,560 --> 02:15:57,240
I was using a tracker that told me exactly how much time I spent coding any one day.

2385
02:15:57,240 --> 02:16:01,840
And even on a very productive day, I still spent only like six or eight hours.

2386
02:16:01,840 --> 02:16:07,400
And it's just because there's so much padding, commute, talking to people, food, et cetera.

2387
02:16:07,400 --> 02:16:13,280
There's like a cost of life just living and sustaining and homeostasis and just maintaining

2388
02:16:13,280 --> 02:16:16,120
yourself as a human is very high.

2389
02:16:16,120 --> 02:16:22,600
And there seems to be a desire within the human mind to participate in society that creates

2390
02:16:22,600 --> 02:16:23,600
that padding.

2391
02:16:23,600 --> 02:16:24,600
Yeah.

2392
02:16:24,600 --> 02:16:28,360
Yeah, the most productive days I've ever had is just completely from start to finish is

2393
02:16:28,360 --> 02:16:29,360
tuning out everything.

2394
02:16:29,360 --> 02:16:30,360
Yeah.

2395
02:16:30,360 --> 02:16:34,200
Just sitting there and then you could do more than six and eight hours.

2396
02:16:34,200 --> 02:16:39,840
Is there some wisdom about what gives you strength to do like tough days of long focus?

2397
02:16:39,840 --> 02:16:44,120
Yeah, just like whenever I get obsessed about a problem, something just needs to work,

2398
02:16:44,120 --> 02:16:45,440
something just needs to exist.

2399
02:16:45,440 --> 02:16:50,040
It needs to exist and you're so you're able to deal with bugs and programming issues and

2400
02:16:50,040 --> 02:16:54,440
technical issues and design decisions that turn out to be the wrong ones, you're able

2401
02:16:54,440 --> 02:16:57,640
to think through all of that given that you want to think to exist.

2402
02:16:57,640 --> 02:16:58,720
Yeah, it needs to exist.

2403
02:16:58,720 --> 02:17:02,380
And then I think to me also a big factor is, you know, our other humans are going to

2404
02:17:02,380 --> 02:17:03,380
appreciate it.

2405
02:17:03,380 --> 02:17:04,380
Are they going to like it?

2406
02:17:04,380 --> 02:17:05,480
That's a big part of my motivation.

2407
02:17:05,480 --> 02:17:10,960
If I'm helping humans and they seem happy, they say nice things, they tweet about it

2408
02:17:10,960 --> 02:17:11,960
or whatever.

2409
02:17:11,960 --> 02:17:13,960
That gives me pleasure because I'm doing something useful.

2410
02:17:13,960 --> 02:17:18,560
So like you do see yourself sharing it with the world, like with San GitHub, with the blog

2411
02:17:18,560 --> 02:17:19,560
posts or videos.

2412
02:17:19,560 --> 02:17:20,560
Yeah, I was thinking about it.

2413
02:17:20,560 --> 02:17:22,960
Like, suppose I did all these things but did not share them.

2414
02:17:22,960 --> 02:17:25,680
I don't think I would have the same motivation that I can build up.

2415
02:17:25,680 --> 02:17:32,680
You enjoy the feeling of other people gaining value and happiness from the stuff you've

2416
02:17:32,680 --> 02:17:33,680
created.

2417
02:17:33,680 --> 02:17:34,680
Yeah.

2418
02:17:34,680 --> 02:17:35,680
What about diet?

2419
02:17:35,680 --> 02:17:39,240
Is there, I saw you play it in a minute fast, and you fast?

2420
02:17:39,240 --> 02:17:40,240
Is that help?

2421
02:17:40,240 --> 02:17:41,240
With everything.

2422
02:17:41,240 --> 02:17:46,680
With the things you played was been most beneficial to your ability to mentally focus

2423
02:17:46,680 --> 02:17:50,880
on a thing and just mental mental productivity and happiness.

2424
02:17:50,880 --> 02:17:51,880
You still fast?

2425
02:17:51,880 --> 02:17:55,480
Yeah, I still fast but I do intermittent fasting but really what it means at the end of

2426
02:17:55,480 --> 02:17:57,080
the day is I skip breakfast.

2427
02:17:57,080 --> 02:18:01,240
So I do 18-6 roughly by default when I'm in my steady state.

2428
02:18:01,240 --> 02:18:04,560
If I'm traveling or doing something else, I will break the rules but in my steady state

2429
02:18:04,560 --> 02:18:08,400
I do 18-6 so I eat only from 12 to 6.

2430
02:18:08,400 --> 02:18:11,120
Not a hard rule and I break it often but that's my default.

2431
02:18:11,120 --> 02:18:15,560
And then, yeah, I've done a bunch of random experiments for the most part right now where

2432
02:18:15,560 --> 02:18:20,960
I've been for the last year and a half I want to say is I'm plant-based or plant-forward.

2433
02:18:20,960 --> 02:18:21,960
I heard plant-forward.

2434
02:18:21,960 --> 02:18:22,960
It sounds better.

2435
02:18:22,960 --> 02:18:25,360
I didn't actually know the differences but it sounds better in my mind.

2436
02:18:25,360 --> 02:18:33,160
But it just means I prefer plant-based food and raw or cooked and plant-based.

2437
02:18:33,160 --> 02:18:40,360
So plant-based, forgive me, I don't actually know how wide the category of plant entails.

2438
02:18:40,360 --> 02:18:45,360
Well, plant-based just means that you're not a little chirped about it and you can flex.

2439
02:18:45,360 --> 02:18:50,920
And you just prefer to eat plants and you know you're not trying to influence other people

2440
02:18:50,920 --> 02:18:54,120
and if someone is, you come to someone's house party and they serve you a steak that they're

2441
02:18:54,120 --> 02:18:55,720
really proud of, you will eat it.

2442
02:18:55,720 --> 02:18:56,720
Yes.

2443
02:18:56,720 --> 02:18:57,720
It's just like-

2444
02:18:57,720 --> 02:18:58,720
That's beautiful.

2445
02:18:58,720 --> 02:19:02,920
I mean, that's, I'm the flip side of that but I'm very sort of flexible.

2446
02:19:02,920 --> 02:19:05,040
Have you tried doing one meal a day?

2447
02:19:05,040 --> 02:19:09,720
I have accidentally, not consistently, but I've accidentally had that.

2448
02:19:09,720 --> 02:19:10,720
I don't like it.

2449
02:19:10,720 --> 02:19:12,840
I think it makes me feel not good.

2450
02:19:12,840 --> 02:19:15,360
It's too much of a hit.

2451
02:19:15,360 --> 02:19:18,600
And so currently I have about two meals a day, 12 and 6.

2452
02:19:18,600 --> 02:19:22,560
I do that now and stop from doing it now until one meal a day.

2453
02:19:22,560 --> 02:19:23,560
Okay.

2454
02:19:23,560 --> 02:19:24,560
It's an interesting feeling.

2455
02:19:24,560 --> 02:19:26,400
Have you ever fasted longer than a day?

2456
02:19:26,400 --> 02:19:27,400
Yeah.

2457
02:19:27,400 --> 02:19:30,080
I've done a bunch of water fasts because I'm curious what happens.

2458
02:19:30,080 --> 02:19:31,080
What?

2459
02:19:31,080 --> 02:19:32,080
Anything interesting.

2460
02:19:32,080 --> 02:19:33,080
Yeah, I would say so.

2461
02:19:33,080 --> 02:19:36,800
I mean, you know what's interesting is that you're hungry for two days and then starting

2462
02:19:36,800 --> 02:19:38,840
day three or so, you're not hungry.

2463
02:19:38,840 --> 02:19:42,040
It's like such a weird feeling because you haven't eaten in a few days and you're not

2464
02:19:42,040 --> 02:19:43,040
hungry.

2465
02:19:43,040 --> 02:19:44,040
Isn't that weird?

2466
02:19:44,040 --> 02:19:46,640
It's only one of the many weird things about human biology.

2467
02:19:46,640 --> 02:19:47,640
Yeah.

2468
02:19:47,640 --> 02:19:48,640
You figure something out.

2469
02:19:48,640 --> 02:19:53,960
And finds another source of energy or something like that or relaxes the system.

2470
02:19:53,960 --> 02:19:54,960
I don't know how to work.

2471
02:19:54,960 --> 02:19:55,960
Yeah.

2472
02:19:55,960 --> 02:19:56,960
The body is like, you're hungry, you're hungry.

2473
02:19:56,960 --> 02:19:57,960
And then it just gives up.

2474
02:19:57,960 --> 02:19:58,960
It's like, okay, I guess we're fasting now.

2475
02:19:58,960 --> 02:19:59,960
There's nothing.

2476
02:19:59,960 --> 02:20:04,080
And then it's just kind of like focuses on trying to make you not hungry and you know,

2477
02:20:04,080 --> 02:20:08,080
not feel the damage of that and trying to give you some space to figure out the food

2478
02:20:08,080 --> 02:20:09,080
situation.

2479
02:20:09,080 --> 02:20:14,680
So are you still to this day most productive at night?

2480
02:20:14,680 --> 02:20:20,360
I would say I am, but it is really hard to maintain my PhD schedule, especially when I

2481
02:20:20,360 --> 02:20:23,360
was, say, working at Tesla and so on, it's a non-starter.

2482
02:20:23,360 --> 02:20:27,520
So, but even now, like, you know, people want to meet for various events.

2483
02:20:27,520 --> 02:20:31,680
They, society lives in a certain period of time and you sort of have to like work.

2484
02:20:31,680 --> 02:20:36,360
So that's, it's hard to like do a social thing and then after that, return and do work.

2485
02:20:36,360 --> 02:20:37,360
Yeah.

2486
02:20:37,360 --> 02:20:38,360
It's just really hard.

2487
02:20:38,360 --> 02:20:41,600
That's why I try to do social thing.

2488
02:20:41,600 --> 02:20:48,000
That's why I'm not to do too much drinking so I can return and continue doing work.

2489
02:20:48,000 --> 02:20:54,600
But a Tesla is there, is there conversions, like Tesla, but any company, is there converges

2490
02:20:54,600 --> 02:21:00,200
to almost a schedule or is there more, is that how humans behave when they collaborate?

2491
02:21:00,200 --> 02:21:01,800
I need to learn about this.

2492
02:21:01,800 --> 02:21:02,800
Yeah.

2493
02:21:02,800 --> 02:21:05,640
They try to keep a consistent schedule where you're all awake at the same time.

2494
02:21:05,640 --> 02:21:10,320
I'm going to do try to create a routine and I try to create a steady state in which I'm

2495
02:21:10,320 --> 02:21:11,320
comfortable in.

2496
02:21:11,320 --> 02:21:12,520
So I have a morning routine.

2497
02:21:12,520 --> 02:21:13,360
I have a day routine.

2498
02:21:13,360 --> 02:21:18,480
I try to keep things to a steady state and things are predictable and then you can sort

2499
02:21:18,480 --> 02:21:20,960
of just like your body just sort of like sticks to that.

2500
02:21:20,960 --> 02:21:23,800
And if you try to stress that a little too much, it will create, you know, when you're

2501
02:21:23,800 --> 02:21:28,640
traveling and you're dealing with jet lag, you're not able to really ascend to, you know,

2502
02:21:28,640 --> 02:21:29,640
where you need to go.

2503
02:21:29,640 --> 02:21:30,640
Yeah.

2504
02:21:30,640 --> 02:21:31,640
Yeah.

2505
02:21:31,640 --> 02:21:33,760
We do a lot of humans with the habits and stuff.

2506
02:21:33,760 --> 02:21:38,800
What are your thoughts on work life balance throughout a human lifetime?

2507
02:21:38,800 --> 02:21:44,720
So testing part was known for sort of pushing people to their limits in terms of what they're

2508
02:21:44,720 --> 02:21:49,840
able to do, in terms of what they're trying to do, in terms of how much they work, all

2509
02:21:49,840 --> 02:21:50,840
that kind of stuff.

2510
02:21:50,840 --> 02:21:51,840
Yeah.

2511
02:21:51,840 --> 02:21:55,720
I mean, I will say test like it's all too much bad rep for this because what's happening

2512
02:21:55,720 --> 02:21:58,200
is test lies, it's a burst environment.

2513
02:21:58,200 --> 02:22:02,640
So I would say the baseline, my only point of reference is Google where I've interned

2514
02:22:02,640 --> 02:22:06,880
three times and I saw what it's like inside Google and deep mind.

2515
02:22:06,880 --> 02:22:10,720
I would say the baseline is higher than that, but then there's a punctually equilibrium

2516
02:22:10,720 --> 02:22:15,000
where once in a while, there's a fire and someone like people work really hard.

2517
02:22:15,000 --> 02:22:19,560
And so it's spiky and bursty and then all the stories get collected about the bursts.

2518
02:22:19,560 --> 02:22:22,960
And then it gives the appearance of like total insanity, but actually it's just a bit

2519
02:22:22,960 --> 02:22:27,160
more intense environment and there are fires and sprints.

2520
02:22:27,160 --> 02:22:31,880
And so I think, you know, definitely though, I would say it's a more intense environment

2521
02:22:31,880 --> 02:22:32,880
than something you would get.

2522
02:22:32,880 --> 02:22:38,120
But in your person, forget all of that, just in your own personal life, what do you think

2523
02:22:38,120 --> 02:22:45,720
about the happiness of a human being, a brilliant person like yourself about finding a balance

2524
02:22:45,720 --> 02:22:50,920
between work and life, or is it such a thing, not a good thought experiment?

2525
02:22:50,920 --> 02:22:58,960
Yeah, I think balance is good, but I also love to have sprints that are out of distribution.

2526
02:22:58,960 --> 02:23:04,640
And that's when I think I've been pretty creative as well.

2527
02:23:04,640 --> 02:23:11,120
Sprints out of distribution means that most of the time, you have a, yeah, quote unquote,

2528
02:23:11,120 --> 02:23:12,120
balance.

2529
02:23:12,120 --> 02:23:13,120
I have balance most of the time.

2530
02:23:13,120 --> 02:23:16,120
And I like being obsessed with something once in a while.

2531
02:23:16,120 --> 02:23:17,120
Once in a while is what?

2532
02:23:17,120 --> 02:23:18,120
Once a week, once a month, once a year?

2533
02:23:18,120 --> 02:23:20,720
Yeah, probably like say once a month or something, yeah.

2534
02:23:20,720 --> 02:23:24,200
And that's when we get, and you get hub repo for my, yeah, that's when you're like really

2535
02:23:24,200 --> 02:23:25,200
care about a problem.

2536
02:23:25,200 --> 02:23:26,200
It must exist.

2537
02:23:26,200 --> 02:23:29,880
It's awesome, you're obsessed with it, and now you can't just do it on that day.

2538
02:23:29,880 --> 02:23:33,560
You need to pay the fixed cost of getting into the groove, and then you need to stay there

2539
02:23:33,560 --> 02:23:34,560
for a while.

2540
02:23:34,560 --> 02:23:37,800
And then society will come, and they will try to mess with you, and they will try to distract

2541
02:23:37,800 --> 02:23:38,800
you.

2542
02:23:38,800 --> 02:23:42,040
Yeah, the worst thing is like a person who's like, I just need five minutes of your time.

2543
02:23:42,040 --> 02:23:43,040
Yeah.

2544
02:23:43,040 --> 02:23:45,320
This is the cost of that is not five minutes.

2545
02:23:45,320 --> 02:23:50,480
And society needs to change how it thinks about just five minutes of your time.

2546
02:23:50,480 --> 02:23:51,480
Right.

2547
02:23:51,480 --> 02:23:52,480
It's never, it's never.

2548
02:23:52,480 --> 02:23:53,480
Just one minute.

2549
02:23:53,480 --> 02:23:54,480
It's just 30.

2550
02:23:54,480 --> 02:23:55,480
It's just a quick thing.

2551
02:23:55,480 --> 02:23:56,480
Yeah.

2552
02:23:56,480 --> 02:23:57,480
Yeah.

2553
02:23:57,480 --> 02:23:58,480
Yeah.

2554
02:23:58,480 --> 02:23:59,480
No.

2555
02:23:59,480 --> 02:24:01,160
What's your computer setup?

2556
02:24:01,160 --> 02:24:07,000
What's like the perfect, do you, are you somebody that's flexible to no matter what, laptop,

2557
02:24:07,000 --> 02:24:08,000
four screens?

2558
02:24:08,000 --> 02:24:09,000
Yeah.

2559
02:24:09,000 --> 02:24:13,720
Or do you prefer a certain setup that you're most productive?

2560
02:24:13,720 --> 02:24:19,720
I guess the one that I'm familiar with is one large screen, 27 inch, and my laptop

2561
02:24:19,720 --> 02:24:20,720
on the side.

2562
02:24:20,720 --> 02:24:21,720
What operating system?

2563
02:24:21,720 --> 02:24:22,720
I do max.

2564
02:24:22,720 --> 02:24:23,720
That's my primary.

2565
02:24:23,720 --> 02:24:25,240
For all tasks.

2566
02:24:25,240 --> 02:24:26,240
I would say OSX.

2567
02:24:26,240 --> 02:24:29,160
But when you're working on deep learning, everything is Linux, your SSH into a cluster

2568
02:24:29,160 --> 02:24:31,000
and you're working remotely.

2569
02:24:31,000 --> 02:24:33,800
But what about the actual development like that using the IDE?

2570
02:24:33,800 --> 02:24:38,400
Yeah, you would use, I think a good way is you just run VS code.

2571
02:24:38,400 --> 02:24:42,480
My favorite error right now on your Mac, but you are actually, you have a remote folder

2572
02:24:42,480 --> 02:24:44,480
through SSH.

2573
02:24:44,480 --> 02:24:47,600
So the actual files that you're manipulating are on the cluster somewhere else.

2574
02:24:47,600 --> 02:24:50,200
So what's the best IDE?

2575
02:24:50,200 --> 02:24:59,160
VS code, what else do people, so I use EMAX still, it may be cool, I don't know if it's

2576
02:24:59,160 --> 02:25:01,560
maximum productivity.

2577
02:25:01,560 --> 02:25:06,560
So what do you recommend in terms of editors, you worked a lot of software engineers, editors

2578
02:25:06,560 --> 02:25:11,320
for Python, C++ machine learning applications?

2579
02:25:11,320 --> 02:25:13,800
I think the current answer is VS code.

2580
02:25:13,800 --> 02:25:16,800
Currently I believe that's the best IDE.

2581
02:25:16,800 --> 02:25:22,520
It's got a huge amount of extensions, it has GitHub co-pilot integration, which I think

2582
02:25:22,520 --> 02:25:23,520
is very valuable.

2583
02:25:23,520 --> 02:25:25,520
What do you think about the co-pilot integration?

2584
02:25:25,520 --> 02:25:30,880
I was actually, I got to talk a bunch with Guido and Rasm was the creative Python and he

2585
02:25:30,880 --> 02:25:35,440
loves co-pilot, he like, he programs a lot with it.

2586
02:25:35,440 --> 02:25:36,440
Do you?

2587
02:25:36,440 --> 02:25:40,600
Yeah, use co-pilot, I love it and it's free for me, but I would pay for it.

2588
02:25:40,600 --> 02:25:44,880
Yeah, I think it's very good and the utility that I found with it was, I would say there

2589
02:25:44,880 --> 02:25:49,360
is a learning curve and you need to figure out when it's helpful and when to pay attention

2590
02:25:49,360 --> 02:25:52,320
to its outputs and when it's not going to be helpful, where you should not pay attention

2591
02:25:52,320 --> 02:25:53,320
to it.

2592
02:25:53,320 --> 02:25:56,040
Because if you're just reading at suggestions all the time, it's not a good way of interacting

2593
02:25:56,040 --> 02:25:57,040
with it.

2594
02:25:57,040 --> 02:25:58,920
But I think I was able to sort of mold myself to it.

2595
02:25:58,920 --> 02:26:03,160
I find it's very helpful, number one, in copy paste and replace some parts.

2596
02:26:03,160 --> 02:26:07,920
So I don't, when the pattern is clear, it's really good at completing the pattern.

2597
02:26:07,920 --> 02:26:11,640
And number two, sometimes it suggests APIs that I'm not aware of.

2598
02:26:11,640 --> 02:26:14,520
So it tells you about something that you didn't know.

2599
02:26:14,520 --> 02:26:16,200
And that's an opportunity to discover and use it.

2600
02:26:16,200 --> 02:26:17,280
It's an opportunity too.

2601
02:26:17,280 --> 02:26:19,560
So I would never take co-pilot code as given.

2602
02:26:19,560 --> 02:26:23,880
I almost always copy a copy paste into a Google search and you see what this function is

2603
02:26:23,880 --> 02:26:24,880
doing.

2604
02:26:24,880 --> 02:26:26,880
And then you're like, oh, it's actually exactly what I need.

2605
02:26:26,880 --> 02:26:27,880
Thank you, co-pilot.

2606
02:26:27,880 --> 02:26:28,880
So you learned something.

2607
02:26:28,880 --> 02:26:34,040
So it's in part of search engine, part, maybe getting the exact syntax correctly that

2608
02:26:34,040 --> 02:26:40,560
once you see it, it's that NP-hard thing is that once you see it, you know, it's correct.

2609
02:26:40,560 --> 02:26:41,560
Exactly.

2610
02:26:41,560 --> 02:26:45,800
It's very helpful, you can verify efficiently, but you can't generate efficiently.

2611
02:26:45,800 --> 02:26:49,720
And co-pilot really, I mean, it's autopilot for programming, right?

2612
02:26:49,720 --> 02:26:53,400
And currently is doing the link following, which is like the simple copy paste and sometimes

2613
02:26:53,400 --> 02:26:54,720
suggest.

2614
02:26:54,720 --> 02:26:57,360
But over time, it's going to become more and more autonomous.

2615
02:26:57,360 --> 02:27:01,160
And so the same thing will play out in not just coding, but actually across many, many

2616
02:27:01,160 --> 02:27:02,160
different things probably.

2617
02:27:02,160 --> 02:27:04,400
But coding is an important one, right?

2618
02:27:04,400 --> 02:27:05,400
Writing programs.

2619
02:27:05,400 --> 02:27:06,400
Yeah.

2620
02:27:06,400 --> 02:27:10,560
How do you see the future of that developing the program synthesis, like being able to

2621
02:27:10,560 --> 02:27:12,920
write programs that are more and more complicated?

2622
02:27:12,920 --> 02:27:18,160
Because right now, it's human supervised in interesting ways.

2623
02:27:18,160 --> 02:27:19,160
Yes.

2624
02:27:19,160 --> 02:27:22,160
It feels like the transition will be very painful.

2625
02:27:22,160 --> 02:27:26,320
My mental model for it is the same thing will happen to us with the autopilot.

2626
02:27:26,320 --> 02:27:29,680
So currently is doing the link following, is doing some simple stuff.

2627
02:27:29,680 --> 02:27:33,360
And eventually we'll be doing autonomy, and people will have to intervene less and less.

2628
02:27:33,360 --> 02:27:39,360
And there could be like testing mechanisms, like if it writes a function, and that function

2629
02:27:39,360 --> 02:27:42,720
looks pretty damn correct, but how do you know it's correct?

2630
02:27:42,720 --> 02:27:47,120
Because you're like getting lazier and lazier as a programmer, like your ability to,

2631
02:27:47,120 --> 02:27:50,760
because like little bugs, but I guess it won't make little mistakes.

2632
02:27:50,760 --> 02:27:51,760
No, it will.

2633
02:27:51,760 --> 02:27:54,840
It copilot will make off by one subtle bugs.

2634
02:27:54,840 --> 02:27:55,960
It has done that to me.

2635
02:27:55,960 --> 02:28:01,400
But do you think future systems will, or is it really the off by one is actually a fundamental

2636
02:28:01,400 --> 02:28:03,240
challenge of programming?

2637
02:28:03,240 --> 02:28:06,160
In that case, it wasn't fundamental, and I think things can improve.

2638
02:28:06,160 --> 02:28:08,440
But yeah, I think humans have to supervise.

2639
02:28:08,440 --> 02:28:11,440
I am nervous about people not supervising what comes out.

2640
02:28:11,440 --> 02:28:15,360
And what happens to, for example, the proliferation of bugs in all of our systems.

2641
02:28:15,360 --> 02:28:16,360
I'm nervous about that.

2642
02:28:16,360 --> 02:28:20,480
But I think there will probably be some other copilots for bug finding and stuff like that

2643
02:28:20,480 --> 02:28:21,480
at some point.

2644
02:28:21,480 --> 02:28:25,120
There will be like a lot more automation for a man.

2645
02:28:25,120 --> 02:28:33,120
So it's like a program, a copilot that generates a compiler, one that does a linter, one that

2646
02:28:33,120 --> 02:28:35,880
does like a type checker.

2647
02:28:35,880 --> 02:28:36,880
Yeah.

2648
02:28:36,880 --> 02:28:41,800
It's a committee of like a GPT sort of like, and then there will be like a manager for

2649
02:28:41,800 --> 02:28:42,800
the committee.

2650
02:28:42,800 --> 02:28:43,800
Yeah.

2651
02:28:43,800 --> 02:28:45,840
And then there will be somebody that says a new version of this is needed.

2652
02:28:45,840 --> 02:28:46,840
We need to regenerate it.

2653
02:28:46,840 --> 02:28:47,840
Yeah.

2654
02:28:47,840 --> 02:28:48,840
There were 10 GPT's.

2655
02:28:48,840 --> 02:28:50,360
They were forwarded and gave 50 suggestions.

2656
02:28:50,360 --> 02:28:54,440
Another one looked at it and picked a few that they like, a bug one looked at it and it

2657
02:28:54,440 --> 02:28:55,760
was like, it's probably a bug.

2658
02:28:55,760 --> 02:28:57,560
They got re-ranked by some other thing.

2659
02:28:57,560 --> 02:29:01,560
And then a final ensemble GPT comes in and it's like, okay, given everything you guys

2660
02:29:01,560 --> 02:29:04,280
have told me, this is probably the next token.

2661
02:29:04,280 --> 02:29:07,560
You know, the feeling is the number of programmers in the world has been growing and growing very

2662
02:29:07,560 --> 02:29:08,560
quickly.

2663
02:29:08,560 --> 02:29:12,800
Do you think it's possible that it'll actually level out and drop to like a very low

2664
02:29:12,800 --> 02:29:14,600
number with this kind of world?

2665
02:29:14,600 --> 02:29:20,720
Because then you'll be doing software 2.0 programming and you'll be doing this kind

2666
02:29:20,720 --> 02:29:28,360
of generation of copilot type systems programming, but you won't be doing the old school software

2667
02:29:28,360 --> 02:29:29,880
1.0 programming.

2668
02:29:29,880 --> 02:29:35,320
I don't currently think that they're just going to replace you in programmers.

2669
02:29:35,320 --> 02:29:40,240
I'm so hesitant saying stuff like this, because this is going to be replacing five years.

2670
02:29:40,240 --> 02:29:44,800
I mean, no, it's going to show that, like, this is where we thought, because I agree

2671
02:29:44,800 --> 02:29:49,320
with you, but I think it might be very surprised, right?

2672
02:29:49,320 --> 02:29:55,120
Like, what are the next, what's your sense of where we stand with language models?

2673
02:29:55,120 --> 02:29:57,920
Like, does it feel like the beginning or the middle or the end?

2674
02:29:57,920 --> 02:29:58,920
The beginning.

2675
02:29:58,920 --> 02:30:02,760
And I think the big question in my mind is, for sure, GPT will be able to program quite

2676
02:30:02,760 --> 02:30:04,360
well, competently and so on.

2677
02:30:04,360 --> 02:30:05,920
How do you steer the system?

2678
02:30:05,920 --> 02:30:09,440
You still have to provide some guidance to what you actually are looking for.

2679
02:30:09,440 --> 02:30:12,920
And so how do you steer it and how do you say, how do you talk to it?

2680
02:30:12,920 --> 02:30:17,000
How do you audit it and verify that what is done is correct?

2681
02:30:17,000 --> 02:30:18,560
And how do you work with this?

2682
02:30:18,560 --> 02:30:23,560
And it's as much, not just an AI problem, but a UI UX problem.

2683
02:30:23,560 --> 02:30:29,560
So beautiful fertile ground for so much interesting work for VS Code Plus Plus, where you're not

2684
02:30:29,560 --> 02:30:31,240
just this not just human programming anymore.

2685
02:30:31,240 --> 02:30:32,240
It's amazing.

2686
02:30:32,240 --> 02:30:33,240
Yeah.

2687
02:30:33,240 --> 02:30:34,240
So you're interacting with the system.

2688
02:30:34,240 --> 02:30:38,760
So not just one prompt, but it iterative prompting, you're trying to figure out having

2689
02:30:38,760 --> 02:30:40,000
a conversation with the system.

2690
02:30:40,000 --> 02:30:41,000
Yeah.

2691
02:30:41,000 --> 02:30:44,400
That actually, I mean, to me, that's super exciting to have a conversation with the program

2692
02:30:44,400 --> 02:30:45,400
I'm writing.

2693
02:30:45,400 --> 02:30:46,400
Yeah.

2694
02:30:46,400 --> 02:30:48,240
Yeah, maybe at some point, you're just conversing with it.

2695
02:30:48,240 --> 02:30:50,160
It's like, okay, here's what I want to do.

2696
02:30:50,160 --> 02:30:54,560
Maybe this variable, maybe it's not even that low level as variable, but you can also

2697
02:30:54,560 --> 02:30:59,360
imagine, like, can you translate this to C++ and back to Python and back to it?

2698
02:30:59,360 --> 02:31:00,360
Yeah, I know.

2699
02:31:00,360 --> 02:31:01,360
Already kind of existence.

2700
02:31:01,360 --> 02:31:04,920
No, but just like doing it as part of the program experience, like, I think I'd like

2701
02:31:04,920 --> 02:31:11,000
to write this function in C++ or like, you just keep changing for different, different

2702
02:31:11,000 --> 02:31:13,760
programs because of different syntax.

2703
02:31:13,760 --> 02:31:15,800
Maybe I want to convert this into a functional language.

2704
02:31:15,800 --> 02:31:16,800
Yeah.

2705
02:31:16,800 --> 02:31:22,480
I mean, you get to become multilingual as a programmer and dance back and forth efficiently.

2706
02:31:22,480 --> 02:31:23,480
Yeah.

2707
02:31:23,480 --> 02:31:26,920
I mean, I think the UI, UX, a fit though, is like, still very hard to think through because

2708
02:31:26,920 --> 02:31:29,640
it's not just about writing code on a page.

2709
02:31:29,640 --> 02:31:31,480
You have an entire developer environment.

2710
02:31:31,480 --> 02:31:33,280
You have a bunch of hardware on it.

2711
02:31:33,280 --> 02:31:34,560
You have some environmental variables.

2712
02:31:34,560 --> 02:31:37,280
You have some scripts that are running in the Chrome job, like, there's a lot going

2713
02:31:37,280 --> 02:31:43,560
on to, like, working with computers and how do these systems set up environment flags

2714
02:31:43,560 --> 02:31:47,400
and work across multiple machines and set up screen sessions and automate different

2715
02:31:47,400 --> 02:31:52,000
processes like how all that works and is auditable by humans and so on is like massive

2716
02:31:52,000 --> 02:31:53,560
question to a moment.

2717
02:31:53,560 --> 02:31:56,200
You've built archive sanity.

2718
02:31:56,200 --> 02:32:00,600
What is archive and what is the future of academic research publishing that you would

2719
02:32:00,600 --> 02:32:02,080
like to see?

2720
02:32:02,080 --> 02:32:03,840
So archive is this pre-print server.

2721
02:32:03,840 --> 02:32:07,800
So if you have a paper, you can submit it for publication to journals or conferences

2722
02:32:07,800 --> 02:32:11,480
and then wait six months and then maybe get a decision, pass or fail, or you can just

2723
02:32:11,480 --> 02:32:16,080
upload it to archive and then people can tweet about it three minutes later.

2724
02:32:16,080 --> 02:32:17,080
And then everyone sees it.

2725
02:32:17,080 --> 02:32:20,480
Everyone reads it and everyone can profit from it in their own way.

2726
02:32:20,480 --> 02:32:23,920
You can cite it and it has an official look to it.

2727
02:32:23,920 --> 02:32:27,600
It feels like a publication process.

2728
02:32:27,600 --> 02:32:30,120
It feels different if you just put it in a blog post.

2729
02:32:30,120 --> 02:32:31,120
Oh, yeah.

2730
02:32:31,120 --> 02:32:34,440
Yeah, I mean, it's a paper and usually the bar is higher for something that you would

2731
02:32:34,440 --> 02:32:38,160
expect on archive as opposed to something you would see in a blog post.

2732
02:32:38,160 --> 02:32:43,640
So the culture created the bar because you could probably host a pretty crappy fix or an

2733
02:32:43,640 --> 02:32:45,640
archive.

2734
02:32:45,640 --> 02:32:47,200
So what does that make you feel like?

2735
02:32:47,200 --> 02:32:49,160
What does that make you feel about peer review?

2736
02:32:49,160 --> 02:32:57,120
So rigorous peer review by two, three experts versus the peer review of the community right

2737
02:32:57,120 --> 02:32:58,120
as it's written.

2738
02:32:58,120 --> 02:32:59,120
Yeah.

2739
02:32:59,120 --> 02:33:02,640
Basically, I think the community is very well able to peer review things very quickly on

2740
02:33:02,640 --> 02:33:04,200
Twitter.

2741
02:33:04,200 --> 02:33:07,480
And I think maybe it just has to do something with AI machine learning field specifically

2742
02:33:07,480 --> 02:33:08,480
though.

2743
02:33:08,480 --> 02:33:14,200
I feel like things are more easily auditable and the verification is easier potentially than

2744
02:33:14,200 --> 02:33:15,760
the verification somewhere else.

2745
02:33:15,760 --> 02:33:20,160
So it's kind of like you can think of these scientific publications as like little blockchains

2746
02:33:20,160 --> 02:33:22,600
where everyone's building on each other's work and setting each other.

2747
02:33:22,600 --> 02:33:27,320
And you sort of have AI, which is kind of like this much faster and loose blockchain.

2748
02:33:27,320 --> 02:33:32,600
But then you have and any one individual entry is like very, very cheap to make.

2749
02:33:32,600 --> 02:33:36,880
And then you have other fields where maybe that model doesn't make as much sense.

2750
02:33:36,880 --> 02:33:40,280
And so I think in AI, at least things are pretty easily very viable.

2751
02:33:40,280 --> 02:33:43,600
And so that's why when people upload papers, they're a really good idea on someone.

2752
02:33:43,600 --> 02:33:45,960
People can try it out like the next day.

2753
02:33:45,960 --> 02:33:49,200
And they can be the final arbiter of whether it works or not on their problem.

2754
02:33:49,200 --> 02:33:51,640
And the whole thing just moves significantly faster.

2755
02:33:51,640 --> 02:33:54,120
So I kind of feel like academia still has a place.

2756
02:33:54,120 --> 02:33:56,600
So this like conference journal process still has a place.

2757
02:33:56,600 --> 02:33:59,880
But it's sort of like an it lacks behind, I think.

2758
02:33:59,880 --> 02:34:03,280
And it's a bit more maybe higher quality process.

2759
02:34:03,280 --> 02:34:07,440
But it's not sort of the place where you will discover cutting edge work anymore.

2760
02:34:07,440 --> 02:34:10,840
It used to be the case when I was starting my PhD that you go to conferences and journals

2761
02:34:10,840 --> 02:34:12,640
and you discuss all the latest research.

2762
02:34:12,640 --> 02:34:15,960
Now when you go to a conference or a journal like no one discusses anything that's there

2763
02:34:15,960 --> 02:34:19,200
because it's already like three generations ago irrelevant.

2764
02:34:19,200 --> 02:34:20,200
Yeah.

2765
02:34:20,200 --> 02:34:24,960
Which makes me sad about like deep mind, for example, where they still publish in nature

2766
02:34:24,960 --> 02:34:29,200
and these big prestigious, I mean, there's still value, I suppose, of the prestige that

2767
02:34:29,200 --> 02:34:31,000
comes with these big venues.

2768
02:34:31,000 --> 02:34:36,320
But the result is that they will announce and break through performance and it will

2769
02:34:36,320 --> 02:34:39,640
take like a year to actually publish the details.

2770
02:34:39,640 --> 02:34:45,480
I mean, and those details, if they were published immediately, wouldn't inspire the community

2771
02:34:45,480 --> 02:34:46,480
to move in certain directions.

2772
02:34:46,480 --> 02:34:47,480
Yeah.

2773
02:34:47,480 --> 02:34:48,480
Let's speed up the rest of the community.

2774
02:34:48,480 --> 02:34:52,080
But I don't know to what extent that's part of their objective function also.

2775
02:34:52,080 --> 02:34:53,080
That's true.

2776
02:34:53,080 --> 02:34:56,800
So it's not just the prestige a little bit of the delay is part.

2777
02:34:56,800 --> 02:35:02,240
Yeah, they certainly deep mind specifically has been working in the regime of having slightly

2778
02:35:02,240 --> 02:35:07,440
higher quality, basically process and latency and publishing those papers that way.

2779
02:35:07,440 --> 02:35:09,320
Another question from Reddit.

2780
02:35:09,320 --> 02:35:12,680
Do you or have you suffered from imposter syndrome?

2781
02:35:12,680 --> 02:35:18,640
Being the director of AI Tesla, being this person when you're a Stanford where like the

2782
02:35:18,640 --> 02:35:25,600
world looks at you as the expert in AI to teach the world about machine learning.

2783
02:35:25,600 --> 02:35:30,240
When I was leaving Tesla after five years, I spent a ton of time in meeting rooms.

2784
02:35:30,240 --> 02:35:33,920
And I would read papers in the beginning when I joined Tesla, I was writing code and

2785
02:35:33,920 --> 02:35:36,680
then I was writing less and less code and I was reading code and then I was reading less

2786
02:35:36,680 --> 02:35:37,840
and less code.

2787
02:35:37,840 --> 02:35:40,400
And so this is just a natural progression that happens, I think.

2788
02:35:40,400 --> 02:35:43,840
And definitely I would say near the tail end, that's when it sort of like starts to hit

2789
02:35:43,840 --> 02:35:47,520
you a bit more that you're supposed to be an expert, but actually the source of truth

2790
02:35:47,520 --> 02:35:52,160
is the code that people are writing the GitHub and the actual code itself.

2791
02:35:52,160 --> 02:35:54,560
And you're not as familiar with that as you used to be.

2792
02:35:54,560 --> 02:35:57,040
And so I would say maybe there's some like insecurity there.

2793
02:35:57,040 --> 02:35:58,680
Yeah, that's actually pretty profound.

2794
02:35:58,680 --> 02:36:02,520
That a lot of the insecurity has to do with not writing the code in the computer science

2795
02:36:02,520 --> 02:36:03,520
space.

2796
02:36:03,520 --> 02:36:05,880
Like that because that is the truth that they're right there.

2797
02:36:05,880 --> 02:36:08,160
The code is the source of truth, the papers and everything else.

2798
02:36:08,160 --> 02:36:09,880
It's a high level summary.

2799
02:36:09,880 --> 02:36:13,480
I don't, yeah, just a high level summary, but at the end of the day, you have to read

2800
02:36:13,480 --> 02:36:14,480
code.

2801
02:36:14,480 --> 02:36:18,680
It's impossible to translate all that code into actual, you know, paper form.

2802
02:36:18,680 --> 02:36:21,840
So when things come out, especially when they have a source code available, that's my

2803
02:36:21,840 --> 02:36:23,240
favorite place to go.

2804
02:36:23,240 --> 02:36:29,560
So like I said, you're one of the greatest teachers of machine learning AI ever from CS

2805
02:36:29,560 --> 02:36:31,840
to 31N to today.

2806
02:36:31,840 --> 02:36:36,800
What advice would you give to beginners interested in getting into machine learning?

2807
02:36:36,800 --> 02:36:40,600
Beginners are often focused on like what to do.

2808
02:36:40,600 --> 02:36:43,520
And I think the focus should be more like how much you do.

2809
02:36:43,520 --> 02:36:47,200
So I am kind of like believer on the high level and this 10,000 hours kind of concept

2810
02:36:47,200 --> 02:36:51,040
where you just kind of have to just pick the things where you can spend time and you

2811
02:36:51,040 --> 02:36:52,400
care about and you're interested in.

2812
02:36:52,400 --> 02:36:55,160
You literally have to put in 10,000 hours of work.

2813
02:36:55,160 --> 02:36:58,960
It doesn't even like matter as much like where you put it and you'll iterate and you'll

2814
02:36:58,960 --> 02:37:00,560
improve and you'll waste some time.

2815
02:37:00,560 --> 02:37:01,960
I don't know if there's a better way.

2816
02:37:01,960 --> 02:37:03,680
You need to put in 10,000 hours.

2817
02:37:03,680 --> 02:37:06,400
But I think it's actually really nice because I feel like there's some sense of determinism

2818
02:37:06,400 --> 02:37:10,080
about being an expert at a thing if you spend 10,000 hours.

2819
02:37:10,080 --> 02:37:14,840
You can literally pick an arbitrary thing and I think if you spend 10,000 hours of deliberate

2820
02:37:14,840 --> 02:37:17,880
effort and work, you actually will become an expert at it.

2821
02:37:17,880 --> 02:37:24,480
And so I think it's kind of like a nice thought and so basically I would focus more on like

2822
02:37:24,480 --> 02:37:25,800
are you spending 10,000 hours?

2823
02:37:25,800 --> 02:37:27,320
That's what I focus on.

2824
02:37:27,320 --> 02:37:31,720
So and then thinking about what kind of mechanisms maximize your likelihood of getting

2825
02:37:31,720 --> 02:37:32,720
to 10,000 hours.

2826
02:37:32,720 --> 02:37:33,720
Yes.

2827
02:37:33,720 --> 02:37:34,720
Exactly.

2828
02:37:34,720 --> 02:37:38,960
Which for us silly humans means probably forming a daily habit of like every single day

2829
02:37:38,960 --> 02:37:40,360
actually doing the thing.

2830
02:37:40,360 --> 02:37:41,360
Whatever helps you.

2831
02:37:41,360 --> 02:37:44,880
So I do think to a large extent is a psychological problem for yourself.

2832
02:37:44,880 --> 02:37:48,400
One other thing that I help that I think is helpful for the psychology of it is many

2833
02:37:48,400 --> 02:37:50,760
times people compare themselves to others in the area.

2834
02:37:50,760 --> 02:37:52,520
I think it's very harmful.

2835
02:37:52,520 --> 02:37:56,840
Only compare yourself to you from some time ago, like say a year ago, are you better than

2836
02:37:56,840 --> 02:37:57,840
you year ago?

2837
02:37:57,840 --> 02:38:00,320
It's the only way to think.

2838
02:38:00,320 --> 02:38:03,360
And I think this then you can see your progress and it's very motivating.

2839
02:38:03,360 --> 02:38:09,240
That's so interesting that focus on the quantity of hours because I think a lot of people

2840
02:38:09,240 --> 02:38:15,680
in the beginner stage, but actually throughout, get paralyzed by the choice.

2841
02:38:15,680 --> 02:38:19,160
Like which one do I pick this path or this path?

2842
02:38:19,160 --> 02:38:20,160
Yeah.

2843
02:38:20,160 --> 02:38:22,440
Like they'll literally get paralyzed by like which ID to use.

2844
02:38:22,440 --> 02:38:23,440
Well, they're worried.

2845
02:38:23,440 --> 02:38:24,800
Yeah, they'll worry about all these things.

2846
02:38:24,800 --> 02:38:28,320
But the thing is some of the you will waste time doing something wrong.

2847
02:38:28,320 --> 02:38:29,320
Yes.

2848
02:38:29,320 --> 02:38:30,320
You will eventually figure out it's not right.

2849
02:38:30,320 --> 02:38:31,840
You will accumulate scar tissue.

2850
02:38:31,840 --> 02:38:35,440
And next time you'll grow stronger because next time you'll have the scar tissue and next

2851
02:38:35,440 --> 02:38:36,800
time you'll learn from it.

2852
02:38:36,800 --> 02:38:41,600
And next time you come to a similar situation, you'll be like, I messed up.

2853
02:38:41,600 --> 02:38:45,360
I've spent a lot of time working on things that never materialize into anything.

2854
02:38:45,360 --> 02:38:49,520
And I have a scar tissue and I have some intuitions about what wasn't useful, how things

2855
02:38:49,520 --> 02:38:50,680
turned out.

2856
02:38:50,680 --> 02:38:54,120
So all those mistakes where we're not dead work.

2857
02:38:54,120 --> 02:38:56,760
So I just think, did you just focus on working?

2858
02:38:56,760 --> 02:38:57,760
What have you done?

2859
02:38:57,760 --> 02:39:00,840
What have you done last week?

2860
02:39:00,840 --> 02:39:05,720
That's a good question actually to ask for a lot of things that just machine learning.

2861
02:39:05,720 --> 02:39:10,280
It's a good way to cut the, the way I forgot what the term we used, but the fluff, the

2862
02:39:10,280 --> 02:39:15,080
blubber, whatever the inefficiencies in life.

2863
02:39:15,080 --> 02:39:17,240
What do you love about teaching?

2864
02:39:17,240 --> 02:39:21,760
You seem to find yourself often drawn to teaching.

2865
02:39:21,760 --> 02:39:23,520
You're very good at it, but you're also drawn to it.

2866
02:39:23,520 --> 02:39:25,240
I mean, I don't think I love teaching.

2867
02:39:25,240 --> 02:39:30,920
I love happy humans and happy humans like when I teach.

2868
02:39:30,920 --> 02:39:34,400
I wouldn't say I hate teaching, I tolerate teaching, but it's not like the act of teaching

2869
02:39:34,400 --> 02:39:35,400
that I like.

2870
02:39:35,400 --> 02:39:42,480
It's that I have something, I'm actually okay at it, I'm okay at teaching and people

2871
02:39:42,480 --> 02:39:44,240
appreciate it a lot.

2872
02:39:44,240 --> 02:39:47,480
And so I'm just happy to try to be helpful.

2873
02:39:47,480 --> 02:39:51,800
And teaching itself is not like the most, I mean, it's really, it can be really annoying

2874
02:39:51,800 --> 02:39:52,800
frustrating.

2875
02:39:52,800 --> 02:39:54,520
I was working on a bunch of lectures just now.

2876
02:39:54,520 --> 02:39:58,840
I was reminded back to my days of 231 and just how much work it is to create some of

2877
02:39:58,840 --> 02:40:02,320
these materials and make them good, the amount of iteration and thought and you go down

2878
02:40:02,320 --> 02:40:04,920
blind alleys and just how much you change it.

2879
02:40:04,920 --> 02:40:10,440
So creating something good in terms of like educational values is really hard and it's

2880
02:40:10,440 --> 02:40:11,440
not fun.

2881
02:40:11,440 --> 02:40:12,440
It was difficult.

2882
02:40:12,440 --> 02:40:17,600
So for people to definitely go watch your new stuff, you put out their lectures, we actually

2883
02:40:17,600 --> 02:40:20,880
building the thing like from like you said, the code is truth.

2884
02:40:20,880 --> 02:40:26,240
So discussing back propagation by building it, by looking through and just the whole thing.

2885
02:40:26,240 --> 02:40:27,840
So how difficult is that to prepare for?

2886
02:40:27,840 --> 02:40:31,960
I think it's a really powerful way to teach how, did you have to prepare for that or are

2887
02:40:31,960 --> 02:40:34,480
you just live thinking through it?

2888
02:40:34,480 --> 02:40:38,800
I will typically do like say three takes and then I take like the better take.

2889
02:40:38,800 --> 02:40:41,480
So I do multiple takes and then I take some of the better takes and then I just build

2890
02:40:41,480 --> 02:40:43,400
that lecture that way.

2891
02:40:43,400 --> 02:40:46,680
Sometimes I have to delete 30 minutes of content because it just went down the alley that

2892
02:40:46,680 --> 02:40:48,000
I didn't like too much.

2893
02:40:48,000 --> 02:40:52,400
There's a bunch of iteration and it probably takes me, you know, somewhere around 10 hours

2894
02:40:52,400 --> 02:40:54,920
to create one hour of content to do one hour.

2895
02:40:54,920 --> 02:40:55,920
It's interesting.

2896
02:40:55,920 --> 02:40:59,040
I mean, is it difficult to go back to the like the basics?

2897
02:40:59,040 --> 02:41:02,400
Do you draw a lot of like wisdom from going back to the basics?

2898
02:41:02,400 --> 02:41:05,320
Yeah, going back to back propagation lost functions where they come from.

2899
02:41:05,320 --> 02:41:10,440
And one thing I like about teaching a lot honestly is it definitely strengthens your understanding.

2900
02:41:10,440 --> 02:41:12,720
So it's not a purely altruistic activity.

2901
02:41:12,720 --> 02:41:13,880
It's a way to learn.

2902
02:41:13,880 --> 02:41:19,640
If you have to explain something to someone, you realize you have gaps in knowledge.

2903
02:41:19,640 --> 02:41:24,080
And so I even surprised myself in those lectures like, well, the result will obviously look

2904
02:41:24,080 --> 02:41:27,640
at this and then the result doesn't look like it and I'm like, okay, I thought I understood

2905
02:41:27,640 --> 02:41:28,640
this.

2906
02:41:28,640 --> 02:41:29,640
Yeah.

2907
02:41:29,640 --> 02:41:34,520
So that's why it's really cool to literally code you run it in a notebook and it gives

2908
02:41:34,520 --> 02:41:35,520
you a result.

2909
02:41:35,520 --> 02:41:39,600
And you're like, oh, wow, and like actual numbers, actual input, actual, you know, actual

2910
02:41:39,600 --> 02:41:40,600
code.

2911
02:41:40,600 --> 02:41:41,600
Yeah, it's not mathematical symbols, etc.

2912
02:41:41,600 --> 02:41:43,160
The source of truth is the code.

2913
02:41:43,160 --> 02:41:44,440
It's not slides.

2914
02:41:44,440 --> 02:41:46,120
It's just like, let's build it.

2915
02:41:46,120 --> 02:41:47,120
It's beautiful.

2916
02:41:47,120 --> 02:41:48,920
You're a rare human in that sense.

2917
02:41:48,920 --> 02:41:54,880
What advice would you give to researchers trying to develop and publish idea that have a

2918
02:41:54,880 --> 02:42:01,720
big impact in the world of AI, so maybe undergrads, maybe early graduate students?

2919
02:42:01,720 --> 02:42:02,720
Yeah.

2920
02:42:02,720 --> 02:42:06,080
I mean, I would say like they definitely have to be a little bit more strategic than I

2921
02:42:06,080 --> 02:42:09,840
had to be as a PhD student because of the way AI is evolving.

2922
02:42:09,840 --> 02:42:13,880
It's going the way of physics where, you know, in physics, you used to be able to do experiments

2923
02:42:13,880 --> 02:42:17,160
on your bench top and everything was great and you could make progress.

2924
02:42:17,160 --> 02:42:21,280
And now you have to work in like LHC or like CERN.

2925
02:42:21,280 --> 02:42:23,960
And so AI is going in that direction as well.

2926
02:42:23,960 --> 02:42:28,480
So there's certain kinds of things that's just not possible to do on the bench top anymore.

2927
02:42:28,480 --> 02:42:32,800
And I think that didn't used to be the case at the time.

2928
02:42:32,800 --> 02:42:41,320
Do you still think that there's like GAN type papers to be written or like a very simple

2929
02:42:41,320 --> 02:42:44,320
idea that requires just one computer to illustrate a simple example?

2930
02:42:44,320 --> 02:42:48,080
I mean, one example that's been very influential recently is diffusion models.

2931
02:42:48,080 --> 02:42:49,440
The fusion models are amazing.

2932
02:42:49,440 --> 02:42:53,840
The fusion models are six years old for the longest time people who are kind of ignoring them

2933
02:42:53,840 --> 02:42:55,280
as far as I can tell.

2934
02:42:55,280 --> 02:42:59,080
And they're an amazing generative model, especially in images.

2935
02:42:59,080 --> 02:43:00,320
And so stable diffusion and so on.

2936
02:43:00,320 --> 02:43:01,800
It's all the fusion based.

2937
02:43:01,800 --> 02:43:02,960
The fusion is new.

2938
02:43:02,960 --> 02:43:06,880
It was not there and came from, well, it came from Google, but a researcher could have

2939
02:43:06,880 --> 02:43:07,880
come up with it.

2940
02:43:07,880 --> 02:43:11,920
In fact, some of the first, actually, no, those came from Google as well.

2941
02:43:11,920 --> 02:43:14,560
But a researcher could come up with that in an academic institution.

2942
02:43:14,560 --> 02:43:15,560
Yeah.

2943
02:43:15,560 --> 02:43:18,000
What do you find most fascinating about diffusion models?

2944
02:43:18,000 --> 02:43:22,720
So from the societal impact of the technical architecture.

2945
02:43:22,720 --> 02:43:24,760
What I like about diffusion is it works so well.

2946
02:43:24,760 --> 02:43:26,840
Is that surprising to you?

2947
02:43:26,840 --> 02:43:32,560
The amount of the variety, almost the novelty of the synthetic data is generating.

2948
02:43:32,560 --> 02:43:33,560
Yes.

2949
02:43:33,560 --> 02:43:36,440
So the stable diffusion images are incredible.

2950
02:43:36,440 --> 02:43:41,040
It's the speed of improvement in generating images has been insane.

2951
02:43:41,040 --> 02:43:45,080
We went very quickly from generating like tiny digits to tiny faces and it all looked

2952
02:43:45,080 --> 02:43:46,920
messed up and now we have a stable diffusion.

2953
02:43:46,920 --> 02:43:48,240
And that happened very quickly.

2954
02:43:48,240 --> 02:43:49,960
There's a lot that academia can still contribute.

2955
02:43:49,960 --> 02:43:55,920
You know, for example, flash attention is a very efficient kernel for running the attention

2956
02:43:55,920 --> 02:43:59,760
operation inside the transformer that came from academic environment.

2957
02:43:59,760 --> 02:44:04,160
It's a very clever way to structure the kernel that does the calculation, so it doesn't

2958
02:44:04,160 --> 02:44:07,240
materialize the attention matrix.

2959
02:44:07,240 --> 02:44:09,760
And so there's, I think there's still lots of things to contribute, but you have to be

2960
02:44:09,760 --> 02:44:10,760
just more strategic.

2961
02:44:10,760 --> 02:44:14,000
Do you think neural networks can be made to reason?

2962
02:44:14,000 --> 02:44:15,000
Yes.

2963
02:44:15,000 --> 02:44:17,440
Do you think they're already reason?

2964
02:44:17,440 --> 02:44:18,440
Yes.

2965
02:44:18,440 --> 02:44:21,480
What's your definition of reasoning?

2966
02:44:21,480 --> 02:44:22,720
Information processing.

2967
02:44:22,720 --> 02:44:32,040
So in the way the humans think through a problem and come up with novel ideas, it feels

2968
02:44:32,040 --> 02:44:33,080
like reasoning.

2969
02:44:33,080 --> 02:44:34,080
Yeah.

2970
02:44:34,080 --> 02:44:43,040
So the novelty, I don't want to say, but out of distribution ideas, you think it's possible?

2971
02:44:43,040 --> 02:44:44,040
Yes.

2972
02:44:44,040 --> 02:44:46,520
I think we're seeing that already in the current neural nets.

2973
02:44:46,520 --> 02:44:51,240
You're able to remix the training set information into true generalization in some sense.

2974
02:44:51,240 --> 02:44:52,560
That doesn't appear.

2975
02:44:52,560 --> 02:44:54,760
It doesn't appear in the training set.

2976
02:44:54,760 --> 02:44:56,560
Like you're doing something interesting algorithmically.

2977
02:44:56,560 --> 02:45:03,280
You're manipulating some symbols and you're coming up with some correct unique answer

2978
02:45:03,280 --> 02:45:05,000
in a new setting.

2979
02:45:05,000 --> 02:45:11,320
What would illustrate to you, holy shit, this thing is definitely thinking?

2980
02:45:11,320 --> 02:45:15,400
To me, thinking or reasoning is just information processing and generalization.

2981
02:45:15,400 --> 02:45:17,840
I think the neural nets already do that today.

2982
02:45:17,840 --> 02:45:26,320
So being able to perceive the world or perceive whatever the inputs are and to make predictions

2983
02:45:26,320 --> 02:45:29,000
based on that or actions based on that, that's the reason.

2984
02:45:29,000 --> 02:45:30,000
Yeah.

2985
02:45:30,000 --> 02:45:34,960
You're giving correct answers in novel settings by manipulating information.

2986
02:45:34,960 --> 02:45:36,720
You've learned the correct algorithm.

2987
02:45:36,720 --> 02:45:39,760
You're not doing just some kind of a lookup table on the Earth's neighbor search.

2988
02:45:39,760 --> 02:45:42,480
Let me ask you about AGI.

2989
02:45:42,480 --> 02:45:48,080
What are some moonshirt ideas you think might make significant progress towards AGI?

2990
02:45:48,080 --> 02:45:52,480
Maybe another way is what are the big blockers that we're missing now.

2991
02:45:52,480 --> 02:45:57,720
So basically, I am fairly bullish on our ability to build AGI's.

2992
02:45:57,720 --> 02:46:02,960
Basically automated systems that we can interact with and are very human-like and we can interact

2993
02:46:02,960 --> 02:46:05,800
with them in a digital realm or a physical realm.

2994
02:46:05,800 --> 02:46:12,880
Currently, it seems most of the models that do these magical tasks are in a text realm.

2995
02:46:12,880 --> 02:46:17,960
I think, as I mentioned, I'm suspicious that the text realm is not enough to actually

2996
02:46:17,960 --> 02:46:20,440
build full understanding of the world.

2997
02:46:20,440 --> 02:46:24,000
I do actually think you need to go into pixels and understand the physical world and how

2998
02:46:24,000 --> 02:46:25,000
it works.

2999
02:46:25,000 --> 02:46:28,800
So I do think that we need to extend these models to consume images and videos and train

3000
02:46:28,800 --> 02:46:31,880
on a lot more data that is multimodal in that way.

3001
02:46:31,880 --> 02:46:34,880
Do you think you need to touch the world to understand it also?

3002
02:46:34,880 --> 02:46:37,360
Well, that's the big open question, I would say, in my mind.

3003
02:46:37,360 --> 02:46:43,440
If you also require the embodiment and the ability to interact with the world, run experiments

3004
02:46:43,440 --> 02:46:48,840
and have a data of that form, then you need to go to Optimus or something like that.

3005
02:46:48,840 --> 02:46:56,120
So I would say Optimus in some way is like a hedge in AGI because it seems to me that

3006
02:46:56,120 --> 02:47:00,400
it's possible that just having data from the internet is not enough.

3007
02:47:00,400 --> 02:47:06,800
If that is the case, then Optimus may lead to AGI because Optimus, to me, there's nothing

3008
02:47:06,800 --> 02:47:07,800
beyond Optimus.

3009
02:47:07,800 --> 02:47:11,480
You have this humanoid form factor that can actually do stuff in the world.

3010
02:47:11,480 --> 02:47:14,720
You can have millions of them interacting with humans and so on.

3011
02:47:14,720 --> 02:47:20,240
And if that doesn't give rise to AGI, at some point, I'm not sure what will.

3012
02:47:20,240 --> 02:47:25,000
So from a completeness perspective, I think that's the really good platform.

3013
02:47:25,000 --> 02:47:29,320
But it's a much harder platform because you are dealing with atoms and you need to actually

3014
02:47:29,320 --> 02:47:32,800
like build these things and integrate them into society.

3015
02:47:32,800 --> 02:47:36,800
So I think that path takes longer, but it's much more certain.

3016
02:47:36,800 --> 02:47:40,200
And then there's a path of the internet and just like training these compression models

3017
02:47:40,200 --> 02:47:45,120
effectively on trying to compress all the internet.

3018
02:47:45,120 --> 02:47:48,560
And that might also give these agents as well.

3019
02:47:48,560 --> 02:47:51,600
Compress the internet but also interact with the internet.

3020
02:47:51,600 --> 02:47:52,600
Yeah.

3021
02:47:52,600 --> 02:47:57,920
So it's not obvious to me, in fact, I suspect you can reach AGI without ever entering

3022
02:47:57,920 --> 02:48:00,520
the physical world.

3023
02:48:00,520 --> 02:48:09,080
And which is a little bit more concerning because that results in it happening faster.

3024
02:48:09,080 --> 02:48:11,960
So it just feels like we're in boiling water.

3025
02:48:11,960 --> 02:48:14,440
We won't know as it's happening.

3026
02:48:14,440 --> 02:48:17,880
I would like to, I'm not afraid of AGI.

3027
02:48:17,880 --> 02:48:19,280
I'm excited about it.

3028
02:48:19,280 --> 02:48:26,000
There's always concerns, but I would like to know when it happens and have hints about

3029
02:48:26,000 --> 02:48:27,000
when it happens.

3030
02:48:27,000 --> 02:48:29,920
Like a year from now, it will happen, that kind of thing.

3031
02:48:29,920 --> 02:48:30,920
Yeah.

3032
02:48:30,920 --> 02:48:32,720
I just feel like in the digital realm, it just might happen.

3033
02:48:32,720 --> 02:48:33,720
Yeah.

3034
02:48:33,720 --> 02:48:37,000
I think all we have available to us because no one has built AGI again.

3035
02:48:37,000 --> 02:48:42,520
So all we have available to us is, is there enough fertile ground on the periphery?

3036
02:48:42,520 --> 02:48:43,520
I would say yes.

3037
02:48:43,520 --> 02:48:46,760
And we have the progress so far, which has been very rapid.

3038
02:48:46,760 --> 02:48:48,640
And there are next steps that are available.

3039
02:48:48,640 --> 02:48:54,440
And so I would say, yeah, it's quite likely that will be interacting with digital entities.

3040
02:48:54,440 --> 02:48:57,200
How do you know that we, somebody has built AGI?

3041
02:48:57,200 --> 02:49:00,080
It's going to be a slow, I think it's going to be a slow incremental transition.

3042
02:49:00,080 --> 02:49:01,760
It's going to be product based and focused.

3043
02:49:01,760 --> 02:49:03,960
It's going to be GitHub Copial getting better.

3044
02:49:03,960 --> 02:49:06,600
And then GPT's helping you write.

3045
02:49:06,600 --> 02:49:09,720
And then these oracles that you can go to with mathematical problems.

3046
02:49:09,720 --> 02:49:15,440
I think we're on a, on a verge of being able to ask very complex questions in chemistry,

3047
02:49:15,440 --> 02:49:19,960
physics, math of these oracles and have them complete solutions.

3048
02:49:19,960 --> 02:49:22,720
So AGI to use primarily focused on intelligence.

3049
02:49:22,720 --> 02:49:27,880
So consciousness doesn't enter into it.

3050
02:49:27,880 --> 02:49:30,320
So in my mind, consciousness is not a special thing.

3051
02:49:30,320 --> 02:49:32,160
You will, you will figure out and bolt on.

3052
02:49:32,160 --> 02:49:37,600
I think it's an emergent phenomenon of a large enough and complex enough generative model

3053
02:49:37,600 --> 02:49:38,600
sort of.

3054
02:49:38,600 --> 02:49:45,240
So if you have a complex enough world model that understands the world, then it also understands

3055
02:49:45,240 --> 02:49:50,120
its predicament in the world as being a language model, which to me is a form of consciousness

3056
02:49:50,120 --> 02:49:53,120
or self-awareness.

3057
02:49:53,120 --> 02:49:55,680
In order to understand the world deeply, you probably have to integrate yourself into

3058
02:49:55,680 --> 02:49:56,680
the world.

3059
02:49:56,680 --> 02:49:57,680
Yeah.

3060
02:49:57,680 --> 02:50:02,120
In order to interact with humans and other living beings, consciousness is a very useful

3061
02:50:02,120 --> 02:50:03,120
tool.

3062
02:50:03,120 --> 02:50:04,120
Yeah.

3063
02:50:04,120 --> 02:50:06,400
I think consciousness is like a modeling insight.

3064
02:50:06,400 --> 02:50:07,400
Modeling insight.

3065
02:50:07,400 --> 02:50:08,400
Yeah.

3066
02:50:08,400 --> 02:50:11,880
It's a, you have a powerful enough model of understanding the world that you actually understand

3067
02:50:11,880 --> 02:50:13,320
that you are an entity in it.

3068
02:50:13,320 --> 02:50:18,120
Yeah, but there's also this, perhaps just a narrative we tell ourselves, there's a,

3069
02:50:18,120 --> 02:50:22,440
it feels like something to experience the world, the hard problem of consciousness.

3070
02:50:22,440 --> 02:50:23,440
Yeah.

3071
02:50:23,440 --> 02:50:24,960
But that could be just an narrative that we tell ourselves.

3072
02:50:24,960 --> 02:50:27,200
Yeah, I don't think, well, yeah, I think it will emerge.

3073
02:50:27,200 --> 02:50:29,440
I think it's going to be something very boring.

3074
02:50:29,440 --> 02:50:33,440
Like, we'll be talking to these digital AI, they will claim their conscious.

3075
02:50:33,440 --> 02:50:35,120
They will appear conscious.

3076
02:50:35,120 --> 02:50:38,800
They will do all the things that you would expect of other humans and it's going to just

3077
02:50:38,800 --> 02:50:39,800
be a stalemate.

3078
02:50:39,800 --> 02:50:45,400
I, I think there would be a lot of actual fascinating ethical questions, like Supreme

3079
02:50:45,400 --> 02:50:51,920
Court level questions of whether you're allowed to turn off a conscious AI.

3080
02:50:51,920 --> 02:50:57,720
If you're allowed to build the conscious AI, maybe there would have to be the same kind

3081
02:50:57,720 --> 02:51:03,640
of the base that you have around, sorry, to bring up a political topic, but, you know,

3082
02:51:03,640 --> 02:51:11,840
a portion, which is the deeper question with abortion is what is life?

3083
02:51:11,840 --> 02:51:16,480
And the deep question with AI is also what is life and what is conscious.

3084
02:51:16,480 --> 02:51:20,920
And I think they'll be very fascinating to bring up.

3085
02:51:20,920 --> 02:51:28,520
It might become illegal to build systems that are capable of such level of intelligence

3086
02:51:28,520 --> 02:51:32,320
that consciousness would emerge and therefore the capacity to suffer would emerge.

3087
02:51:32,320 --> 02:51:35,840
And a system that says, no, please don't kill me.

3088
02:51:35,840 --> 02:51:41,320
Well, that's what the Lambda, the Lambda chatbot already told the school engineer, right?

3089
02:51:41,320 --> 02:51:45,000
Like it was talking about not wanting to die or so on.

3090
02:51:45,000 --> 02:51:48,800
So that might become illegal to do that, right?

3091
02:51:48,800 --> 02:51:49,800
Right.

3092
02:51:49,800 --> 02:51:54,240
Because otherwise you might have a lot of, a lot of creatures that don't want to die and

3093
02:51:54,240 --> 02:51:59,400
they will, you can just spawn and penetrate someone cluster.

3094
02:51:59,400 --> 02:52:03,120
And then that might lead to like horrible consequences, because then there might be a lot

3095
02:52:03,120 --> 02:52:07,360
of people that secretly love murder and they'll start practicing murder and those systems.

3096
02:52:07,360 --> 02:52:13,080
I mean, there's just, to me, all of this stuff just brings a beautiful mirror to the human

3097
02:52:13,080 --> 02:52:15,960
condition and human nature and we'll get to explore it.

3098
02:52:15,960 --> 02:52:21,000
And that's what like the best of the Supreme Court of all the different debates we have

3099
02:52:21,000 --> 02:52:23,480
about ideas of what it means to be human.

3100
02:52:23,480 --> 02:52:27,600
We get to ask those deep questions that would be asking throughout human history.

3101
02:52:27,600 --> 02:52:33,160
There has always been the other in human history where the good guys and that's the bad guys

3102
02:52:33,160 --> 02:52:37,320
and we're going to, you know, throughout human history, let's murder the bad guys.

3103
02:52:37,320 --> 02:52:42,000
And the same will probably happen with robots, so it'll be the other at first and then we'll

3104
02:52:42,000 --> 02:52:43,000
get to ask questions.

3105
02:52:43,000 --> 02:52:44,680
So what does it mean to be alive?

3106
02:52:44,680 --> 02:52:46,080
What does it mean to be conscious?

3107
02:52:46,080 --> 02:52:47,080
Yeah.

3108
02:52:47,080 --> 02:52:50,360
And I think there's some canary in a coal mine, even with what we have today.

3109
02:52:50,360 --> 02:52:53,680
And you know, like for example, these, there's these like wifews that you get like work

3110
02:52:53,680 --> 02:52:56,880
with and some people are trying to like, this company is going to shut down, but this

3111
02:52:56,880 --> 02:53:01,280
person really like, yeah, love their waifu and like, it's trying to like, port it somewhere

3112
02:53:01,280 --> 02:53:06,840
else and like, it's not possible and like, I think like definitely people will have feelings

3113
02:53:06,840 --> 02:53:13,480
towards, towards these systems because in some sense, they are like a mirror of humanity

3114
02:53:13,480 --> 02:53:18,640
because they are like sort of like a big average of humanity in a way that it's trained.

3115
02:53:18,640 --> 02:53:22,440
But we can, that average, we can actually watch.

3116
02:53:22,440 --> 02:53:26,200
It's nice to be able to interact with the big average of humanity and do like a search

3117
02:53:26,200 --> 02:53:27,200
query on it.

3118
02:53:27,200 --> 02:53:28,200
Yeah.

3119
02:53:28,200 --> 02:53:29,880
Yeah, it's very fascinating.

3120
02:53:29,880 --> 02:53:32,040
And we can also, of course, also like shape it.

3121
02:53:32,040 --> 02:53:35,160
It's not just a pure average, we can mess with the training data, we can mess with the

3122
02:53:35,160 --> 02:53:37,760
objective, we can fine-tune them in various ways.

3123
02:53:37,760 --> 02:53:42,680
So we have some, you know, impact on what those systems look like.

3124
02:53:42,680 --> 02:53:50,080
If you want to achieve AGI and you could have a conversation with her and ask her, talk

3125
02:53:50,080 --> 02:53:54,240
about anything, maybe ask her a question, what kind of stuff would you, would you ask?

3126
02:53:54,240 --> 02:53:59,160
I would have some practical questions on my mind like, do I or my loved ones really have

3127
02:53:59,160 --> 02:54:00,160
to die?

3128
02:54:00,160 --> 02:54:03,000
What can we do about that?

3129
02:54:03,000 --> 02:54:07,400
Do you think it will answer clearly or would it answer poetically?

3130
02:54:07,400 --> 02:54:09,040
I would expect it to give solutions.

3131
02:54:09,040 --> 02:54:12,280
I would expect it to be like, well, I've read all of these textbooks and I know all

3132
02:54:12,280 --> 02:54:13,600
these things that you've produced.

3133
02:54:13,600 --> 02:54:17,720
And it seems to me like, here are the experiments that I think it would be useful to run next.

3134
02:54:17,720 --> 02:54:20,120
And here's some gene therapies that I think would be helpful.

3135
02:54:20,120 --> 02:54:22,480
And here are the kinds of experiments that you should run.

3136
02:54:22,480 --> 02:54:23,480
Okay.

3137
02:54:23,480 --> 02:54:25,480
Let's go with this thought experiment.

3138
02:54:25,480 --> 02:54:26,480
Okay.

3139
02:54:26,480 --> 02:54:33,160
That mortality is actually like a prerequisite for happiness.

3140
02:54:33,160 --> 02:54:37,160
So if we become immortal, we'll actually become deeply unhappy.

3141
02:54:37,160 --> 02:54:39,760
And the model is able to know that.

3142
02:54:39,760 --> 02:54:42,600
So what is it supposed to tell you stupid human about it?

3143
02:54:42,600 --> 02:54:46,920
Yes, you can become immortal, but you will become deeply unhappy.

3144
02:54:46,920 --> 02:54:52,640
If the model is, if the AGI system is trying to empathize with you human, what is it supposed

3145
02:54:52,640 --> 02:54:53,640
to tell you?

3146
02:54:53,640 --> 02:54:58,600
Say yes, you don't have to die, but you're really not going to like it, because it's going

3147
02:54:58,600 --> 02:54:59,600
to be deeply honest.

3148
02:54:59,600 --> 02:55:01,800
Like, there's an interstellar.

3149
02:55:01,800 --> 02:55:02,800
What is it?

3150
02:55:02,800 --> 02:55:08,160
The AGI says, like, humans want 90% honesty.

3151
02:55:08,160 --> 02:55:11,880
So like, you have to pick how honest I want to answer these practical questions.

3152
02:55:11,880 --> 02:55:12,880
Yeah.

3153
02:55:12,880 --> 02:55:14,200
I love AI interstellar, by the way.

3154
02:55:14,200 --> 02:55:19,000
I think it's like such a sidekick to the entire story, but at the same time, it's like

3155
02:55:19,000 --> 02:55:20,000
really interesting.

3156
02:55:20,000 --> 02:55:22,120
It's kind of limited in certain ways, right?

3157
02:55:22,120 --> 02:55:23,120
Yeah.

3158
02:55:23,120 --> 02:55:24,560
And I think that's totally fine, by the way.

3159
02:55:24,560 --> 02:55:32,440
I don't think, I think it's fine and plausible to have a limited and imperfect AGI's.

3160
02:55:32,440 --> 02:55:37,520
Is that the feature almost as an example, like it has a fixed mind of compute on its physical

3161
02:55:37,520 --> 02:55:38,520
body.

3162
02:55:38,520 --> 02:55:43,680
And it might just be that even though you can have a super amazing mega brain, super intelligent

3163
02:55:43,680 --> 02:55:47,960
AI, you can also kind of like, you know, less intelligent AI, so you can deploy it in

3164
02:55:47,960 --> 02:55:49,640
a power efficient way.

3165
02:55:49,640 --> 02:55:50,640
And then they're not perfect.

3166
02:55:50,640 --> 02:55:51,640
They might make mistakes.

3167
02:55:51,640 --> 02:55:57,680
I meant more like, say you had infinite compute, and it's still good to make mistakes sometimes.

3168
02:55:57,680 --> 02:56:03,440
Like, in order to integrate yourself, like, what is it going back to Goodwill Hunting?

3169
02:56:03,440 --> 02:56:09,080
Robin Williams character says like the human imperfections, that's good stuff, right?

3170
02:56:09,080 --> 02:56:10,080
Isn't it?

3171
02:56:10,080 --> 02:56:12,600
Isn't that the, like, we don't want perfect?

3172
02:56:12,600 --> 02:56:18,520
We want flaws in part to form connected with each other because it feels like something

3173
02:56:18,520 --> 02:56:26,120
you can attach your feelings to the, the flaws in that same way you want AI that's flawed.

3174
02:56:26,120 --> 02:56:27,120
I don't know.

3175
02:56:27,120 --> 02:56:28,120
I feel like perfectionist.

3176
02:56:28,120 --> 02:56:33,600
But then you're saying, okay, yeah, but that's not AGI, but see AGI would need to be intelligent

3177
02:56:33,600 --> 02:56:38,920
enough to give answers to humans and understand, and I think perfect isn't something humans

3178
02:56:38,920 --> 02:56:42,600
can't understand because even science doesn't give perfect answers.

3179
02:56:42,600 --> 02:56:48,360
There's always gabs and mysteries and I don't know, I don't know if humans want

3180
02:56:48,360 --> 02:56:49,360
perfect.

3181
02:56:49,360 --> 02:56:54,840
Yeah, I can imagine just having a conversation with this kind of oracle entity as you'd

3182
02:56:54,840 --> 02:57:00,040
imagine them, and yeah, maybe it can tell you about, you know, based on my analysis

3183
02:57:00,040 --> 02:57:03,320
of human condition, you might not want this.

3184
02:57:03,320 --> 02:57:08,320
And here are some of the things that might, but every, every dumb human will say, yeah,

3185
02:57:08,320 --> 02:57:12,480
yeah, trust me, I can, give me the truth, I can handle it.

3186
02:57:12,480 --> 02:57:13,480
But that's the beauty.

3187
02:57:13,480 --> 02:57:14,480
Like, people can choose.

3188
02:57:14,480 --> 02:57:21,000
So, but then the old marshmallow test with the kids and so on, I feel like too many

3189
02:57:21,000 --> 02:57:28,160
people can't handle the truth, probably including myself, like the deep truth of the human

3190
02:57:28,160 --> 02:57:29,160
condition.

3191
02:57:29,160 --> 02:57:30,760
I don't know if I can handle it.

3192
02:57:30,760 --> 02:57:35,920
Like, what if there's some darks that, what if we are an alien science experiment, and

3193
02:57:35,920 --> 02:57:41,160
it realizes that, what if it hacked, I mean, I mean, this is the matrix, you know, all

3194
02:57:41,160 --> 02:57:42,160
over again.

3195
02:57:42,160 --> 02:57:46,000
I don't know, I would, what would I talk about?

3196
02:57:46,000 --> 02:57:52,600
I don't even, yeah, I probably I'll go with the safe for scientific questions at first

3197
02:57:52,600 --> 02:57:58,480
that have nothing to do with my own personal life, and mortality, just like about physics

3198
02:57:58,480 --> 02:58:03,800
and so on, to build up, like, let's see where is that, or maybe see if it has a sense

3199
02:58:03,800 --> 02:58:04,800
of humor.

3200
02:58:04,800 --> 02:58:09,320
That's another question, would it be able to, presumably in order to, if it understands

3201
02:58:09,320 --> 02:58:14,960
humans deeply, would be able to generate, yeah, to generate humor?

3202
02:58:14,960 --> 02:58:19,680
Yeah, I think that's actually a wonderful benchmark almost, like, is it able, I think that's

3203
02:58:19,680 --> 02:58:22,360
a really good point, basically, to make you laugh.

3204
02:58:22,360 --> 02:58:25,720
Yeah, if it's able to be like a very effective stand-up comedian that is doing something very

3205
02:58:25,720 --> 02:58:28,960
interesting computationally, I think being funny is extremely hard.

3206
02:58:28,960 --> 02:58:37,520
Yeah, because it's hard in a way, like a touring test, the original intent of the touring

3207
02:58:37,520 --> 02:58:43,480
test is hard, because you have to convince humans, and there's nothing, that's why, that's

3208
02:58:43,480 --> 02:58:48,280
why, I mean, comedians talk about this, like, there's, this is deeply honest, because if

3209
02:58:48,280 --> 02:58:51,960
people can't help but laugh, and if they don't laugh, that means you're not funny, if

3210
02:58:51,960 --> 02:58:52,960
they laugh, that's funny.

3211
02:58:52,960 --> 02:58:53,960
Yeah, that's funny.

3212
02:58:53,960 --> 02:58:57,240
And you're showing, you need a lot of knowledge to create, to create humor about, like,

3213
02:58:57,240 --> 02:59:01,280
you mentioned human condition and so on, and then you need to be clever with it.

3214
02:59:01,280 --> 02:59:06,480
You mentioned a few movies, you tweeted, movies that I've seen five plus times, but I'm

3215
02:59:06,480 --> 02:59:11,200
ready and willing to keep watching, interstellar, gladiator, contact, good, we'll hunting the

3216
02:59:11,200 --> 02:59:16,200
matrix, load of the rings, all three avatar, fifth element, so on, it goes on.

3217
02:59:16,200 --> 02:59:23,760
Terminator 2, mean girls, I'm not going to ask about that, I think, I think girls is great.

3218
02:59:23,760 --> 02:59:29,480
What are some of the jump onto your memory that you love, and why, like you mentioned

3219
02:59:29,480 --> 02:59:34,560
the matrix, as a computer person, why do you love the matrix?

3220
02:59:34,560 --> 02:59:37,680
There's so many properties that make it, like, beautiful, interesting, so there's all

3221
02:59:37,680 --> 02:59:42,320
these philosophical questions, but then there's also AGI's, and there's simulation, and

3222
02:59:42,320 --> 02:59:47,960
it's cool, and there's, you know, the black, you know, the look of it, the feel of it,

3223
02:59:47,960 --> 02:59:53,600
the feel of it, the action, the bullet time, it was just like innovating in so many ways.

3224
02:59:53,600 --> 02:59:57,600
And then good, well, good, well hunting way, like that one.

3225
02:59:57,600 --> 03:00:03,560
Yeah, I just, I really like this torture genius sort of character who's, like, grappling

3226
03:00:03,560 --> 03:00:08,120
with whether or not he has, like, any responsibility or, like, what to do with this gift that he

3227
03:00:08,120 --> 03:00:11,120
was given, or like, how to think about the whole thing.

3228
03:00:11,120 --> 03:00:16,520
And there's also a dance between the genius and the, the personal, like, what it means

3229
03:00:16,520 --> 03:00:19,920
to love another human being, and there's a lot of themes there, it's just a beautiful

3230
03:00:19,920 --> 03:00:20,920
movie.

3231
03:00:20,920 --> 03:00:24,560
And then the fatherly figure, the mentor in the, in the psychiatrist and the, it like,

3232
03:00:24,560 --> 03:00:28,280
really like, it messes with you, you know, there's some movies that's just like, really

3233
03:00:28,280 --> 03:00:31,120
mess with you on a deep level.

3234
03:00:31,120 --> 03:00:33,800
Do you relate to that movie at all?

3235
03:00:33,800 --> 03:00:34,800
No.

3236
03:00:34,800 --> 03:00:35,800
It's not your fault.

3237
03:00:35,800 --> 03:00:42,920
As I said, Lord of the Rings, that's self-explanatory, Terminator 2, which is interesting.

3238
03:00:42,920 --> 03:00:44,280
You rewatch that a lot.

3239
03:00:44,280 --> 03:00:46,240
Is that better than Terminator 1?

3240
03:00:46,240 --> 03:00:47,240
You like Arnold?

3241
03:00:47,240 --> 03:00:50,080
I do like Terminator 1 as well.

3242
03:00:50,080 --> 03:00:56,040
I like Terminator 2 a little bit more, but in terms of, like, its surface properties.

3243
03:00:56,040 --> 03:00:58,440
Do you think SkyNet is at all a possibility?

3244
03:00:58,440 --> 03:01:00,640
Oh, yes.

3245
03:01:00,640 --> 03:01:04,960
Like the actual sort of autonomous weapon system kind of thing.

3246
03:01:04,960 --> 03:01:06,760
Do you worry about that stuff?

3247
03:01:06,760 --> 03:01:07,760
I do worry about it.

3248
03:01:07,760 --> 03:01:08,760
A hundred percent.

3249
03:01:08,760 --> 03:01:09,760
A hundred percent.

3250
03:01:09,760 --> 03:01:10,760
I do worry about it.

3251
03:01:10,760 --> 03:01:15,240
And so the, I mean, the, you know, some of these fears of AGS and how this will plan

3252
03:01:15,240 --> 03:01:18,040
out, I mean, these will be like very powerful entities, probably at some point.

3253
03:01:18,040 --> 03:01:21,640
And so for a long time, there are going to be tools in the hands of humans.

3254
03:01:21,640 --> 03:01:26,120
You know, people talk about, like, alignment of AGS and how to make, the problem is, like,

3255
03:01:26,120 --> 03:01:27,880
even humans are not aligned.

3256
03:01:27,880 --> 03:01:33,560
So how this will be used and what this is going to look like is, um, yes, troubling.

3257
03:01:33,560 --> 03:01:39,440
So do you think it'll happen slow, slow enough that we'll be able to, as a, as a human

3258
03:01:39,440 --> 03:01:41,840
civilization, think through the problems?

3259
03:01:41,840 --> 03:01:42,840
Yes.

3260
03:01:42,840 --> 03:01:45,960
That's my hope is that it happens slowly enough and, and open enough way where a lot of

3261
03:01:45,960 --> 03:01:48,400
people can see and participate in it.

3262
03:01:48,400 --> 03:01:52,280
Just figure out how to deal with this transition, I think, which is going to be interesting.

3263
03:01:52,280 --> 03:01:57,000
I draw a lot of inspiration from nuclear weapons because I sure thought it would be, it would

3264
03:01:57,000 --> 03:02:04,400
be fucked once they develop nuclear weapons, but like, it's almost like, uh, when, uh, when

3265
03:02:04,400 --> 03:02:08,880
the systems are not so dangerous, they destroy human civilization, we deploy them and, and

3266
03:02:08,880 --> 03:02:10,200
learn the lessons.

3267
03:02:10,200 --> 03:02:15,720
And then we quickly, if it's too dangerous, we'll quickly, we might still deploy it, uh,

3268
03:02:15,720 --> 03:02:17,960
but you very quickly learn not to use them.

3269
03:02:17,960 --> 03:02:19,760
And so there'll be like this balance achieved.

3270
03:02:19,760 --> 03:02:21,920
Humans are very clever as a species.

3271
03:02:21,920 --> 03:02:23,160
It's interesting.

3272
03:02:23,160 --> 03:02:26,320
We exploit the resources as much as we can, but we don't.

3273
03:02:26,320 --> 03:02:27,320
We avoid destroying ourselves.

3274
03:02:27,320 --> 03:02:28,320
It seems like.

3275
03:02:28,320 --> 03:02:29,320
Yeah.

3276
03:02:29,320 --> 03:02:30,800
Well, I don't know about that, actually.

3277
03:02:30,800 --> 03:02:32,400
I hope it continues.

3278
03:02:32,400 --> 03:02:37,600
Um, I mean, I'm definitely like concerned about nuclear weapons and so on, not just as

3279
03:02:37,600 --> 03:02:42,480
a result of the recent conflict, even before that, uh, that's probably my number one concern

3280
03:02:42,480 --> 03:02:43,480
for society.

3281
03:02:43,480 --> 03:02:50,720
So if humanity, uh, destroys itself, or destroys, you know, 90% of people, that would

3282
03:02:50,720 --> 03:02:52,480
be because of nukes.

3283
03:02:52,480 --> 03:02:53,640
I think so.

3284
03:02:53,640 --> 03:02:56,320
Um, and it's not even about full destruction to me.

3285
03:02:56,320 --> 03:03:00,000
It's bad enough if we reset society, though, it would be like terrible, it would be really

3286
03:03:00,000 --> 03:03:01,000
bad.

3287
03:03:01,000 --> 03:03:03,240
And I can't believe we're like so close to it.

3288
03:03:03,240 --> 03:03:04,240
Yeah.

3289
03:03:04,240 --> 03:03:05,240
It's like so crazy to me.

3290
03:03:05,240 --> 03:03:08,480
It feels like we might be a few tweets away from something like that.

3291
03:03:08,480 --> 03:03:09,480
Yeah.

3292
03:03:09,480 --> 03:03:14,320
Basically, it's extremely unnerving, but it has been for me for a long time.

3293
03:03:14,320 --> 03:03:23,360
It seems unstable that world leaders, just having a bad mood can like, um, take one

3294
03:03:23,360 --> 03:03:26,400
step towards a bad direction and it escalates.

3295
03:03:26,400 --> 03:03:27,400
Yeah.

3296
03:03:27,400 --> 03:03:33,800
And because of a collection of bad moods, it can escalate without being able to, um, stop.

3297
03:03:33,800 --> 03:03:34,800
Yeah.

3298
03:03:34,800 --> 03:03:37,400
It's just a huge amount of power.

3299
03:03:37,400 --> 03:03:42,120
And then also with the proliferation, basically, I don't, I don't actually really see, I don't

3300
03:03:42,120 --> 03:03:44,120
actually know what the good outcomes are here.

3301
03:03:44,120 --> 03:03:46,840
So I'm definitely worried about that a lot.

3302
03:03:46,840 --> 03:03:50,880
And then AGI is not currently there, but I think at some point, it will more and more

3303
03:03:50,880 --> 03:03:56,400
become something like it, the danger with AGI even is that I think it's even less likely

3304
03:03:56,400 --> 03:04:01,560
worse in a sense that there are good outcomes of AGI.

3305
03:04:01,560 --> 03:04:05,400
And then the bad outcomes are like an epsilon away, like a tiny one away.

3306
03:04:05,400 --> 03:04:10,760
And so I think capitalism and humanity and so on will drive for the positive, uh, ways

3307
03:04:10,760 --> 03:04:14,300
of using that technology, but then if bad outcomes are just like a tiny, like flip a

3308
03:04:14,300 --> 03:04:20,280
minus sign away, uh, that's a really bad position to be in a tiny perturbation of the system

3309
03:04:20,280 --> 03:04:23,160
results in the destruction of the human species.

3310
03:04:23,160 --> 03:04:25,080
So we are lying to walk.

3311
03:04:25,080 --> 03:04:26,080
Yeah.

3312
03:04:26,080 --> 03:04:28,600
I think in general, it was really weird about like the dynamics of humanity and this explosion

3313
03:04:28,600 --> 03:04:34,040
was talked about is just like the insane coupling afforded by technology and, uh, just

3314
03:04:34,040 --> 03:04:37,960
the instability of the whole dynamical system, I think it's just, it doesn't look good

3315
03:04:37,960 --> 03:04:38,960
honestly.

3316
03:04:38,960 --> 03:04:39,960
Yes.

3317
03:04:39,960 --> 03:04:43,560
So that explosion could be destructive or constructive and the probabilities are non-zero

3318
03:04:43,560 --> 03:04:44,560
in both.

3319
03:04:44,560 --> 03:04:45,560
Yeah.

3320
03:04:45,560 --> 03:04:48,640
I'm going to have to, I do feel like I have to try to be optimistic and so on.

3321
03:04:48,640 --> 03:04:49,640
Yes.

3322
03:04:49,640 --> 03:04:54,360
In this case, I still am predominantly optimistic, but there's definitely me too.

3323
03:04:54,360 --> 03:04:59,000
Uh, do you think we'll become a multi planetary species?

3324
03:04:59,000 --> 03:05:04,240
Probably yes, but I don't know if it's dominant feature of, uh, future humanity.

3325
03:05:04,240 --> 03:05:08,880
There might be some people on some planets and so on, but I'm not sure if it's like, yeah,

3326
03:05:08,880 --> 03:05:14,400
if it's like a major player in our culture and so on, we still have to solve the drivers

3327
03:05:14,400 --> 03:05:16,920
of self-destruction here on earth.

3328
03:05:16,920 --> 03:05:20,080
So just having a backup on Mars is not going to solve the problem.

3329
03:05:20,080 --> 03:05:21,880
So by the way, I love the backup on Mars.

3330
03:05:21,880 --> 03:05:22,880
I think that's amazing.

3331
03:05:22,880 --> 03:05:23,960
We should absolutely do that.

3332
03:05:23,960 --> 03:05:24,960
Yes.

3333
03:05:24,960 --> 03:05:25,960
And I'm so thankful.

3334
03:05:25,960 --> 03:05:31,400
Would you, uh, would you go to Mars personally, no, I do like earth quite a lot.

3335
03:05:31,400 --> 03:05:32,400
Okay.

3336
03:05:32,400 --> 03:05:33,400
I'll go to Mars.

3337
03:05:33,400 --> 03:05:34,400
I'll go for you.

3338
03:05:34,400 --> 03:05:38,000
I'll tweet at you for maybe eventually I would once it's safe enough, but I don't

3339
03:05:38,000 --> 03:05:43,080
actually know if it's on my lifetime scale, unless I can extend it by a lot.

3340
03:05:43,080 --> 03:05:47,000
I do think that, for example, a lot of people might disappear into, um, virtual realities

3341
03:05:47,000 --> 03:05:48,000
and stuff like that.

3342
03:05:48,000 --> 03:05:52,520
I think that could be the major thrust of, um, sort of the cultural development of humanity

3343
03:05:52,520 --> 03:05:53,520
if it survives.

3344
03:05:53,520 --> 03:05:58,600
Uh, so it might not be, it's just really hard to work in physical realm and go out there.

3345
03:05:58,600 --> 03:06:01,720
And I think ultimately all your experiences are in your brain.

3346
03:06:01,720 --> 03:06:02,720
Yeah.

3347
03:06:02,720 --> 03:06:06,040
And so it's much easier to disappear into digital realm.

3348
03:06:06,040 --> 03:06:10,240
And I think people will find them more compelling, easier, safer, more interesting.

3349
03:06:10,240 --> 03:06:14,680
So you're a little bit captivated by virtual reality by the possible worlds, whether it's

3350
03:06:14,680 --> 03:06:17,000
the metaverse or some other manifestation of that.

3351
03:06:17,000 --> 03:06:18,000
Yeah.

3352
03:06:18,000 --> 03:06:19,000
Yeah.

3353
03:06:19,000 --> 03:06:20,000
It's really interesting.

3354
03:06:20,000 --> 03:06:25,480
And so, um, I'm interested in just, just talking a lot to Carmack.

3355
03:06:25,480 --> 03:06:29,320
Where's the, where's the thing that's currently preventing that?

3356
03:06:29,320 --> 03:06:30,320
Yeah.

3357
03:06:30,320 --> 03:06:35,600
To be clear, I think what's interesting about the future is, um, it's not that I kind

3358
03:06:35,600 --> 03:06:39,240
of feel like the variance in a human condition grows.

3359
03:06:39,240 --> 03:06:40,560
It's the primary thing that's changing.

3360
03:06:40,560 --> 03:06:44,040
It's not as much demean of the distribution, it's like the variance of it.

3361
03:06:44,040 --> 03:06:46,960
So there will probably be people in Mars and there will be people in VR and there will

3362
03:06:46,960 --> 03:06:48,200
people here on Earth.

3363
03:06:48,200 --> 03:06:51,160
It's just like there will be so many more ways of being.

3364
03:06:51,160 --> 03:06:54,800
And so I kind of feel like I see it as like a spreading out of a human experience.

3365
03:06:54,800 --> 03:06:57,840
There's something about the internet that allows you to discover those little groups

3366
03:06:57,840 --> 03:07:02,600
and you gravitate to something about your biology, likes that kind of world that you find

3367
03:07:02,600 --> 03:07:03,600
each other.

3368
03:07:03,600 --> 03:07:04,600
Yeah.

3369
03:07:04,600 --> 03:07:06,520
And we'll have transhumanists and then we'll have the omnis and they're going to, everything

3370
03:07:06,520 --> 03:07:07,520
is just going to coexist.

3371
03:07:07,520 --> 03:07:11,640
You know, the cool thing about it because I've interacted with a bunch of internet communities

3372
03:07:11,640 --> 03:07:15,600
is, um, they don't know about each other.

3373
03:07:15,600 --> 03:07:19,800
Like you can have a very happy existence, just like having a very close neck community

3374
03:07:19,800 --> 03:07:21,120
and not knowing about each other.

3375
03:07:21,120 --> 03:07:26,440
I mean, even you even sense this just having travel to Ukraine, there's, they, they don't

3376
03:07:26,440 --> 03:07:29,080
know so many things about America.

3377
03:07:29,080 --> 03:07:30,080
Yeah.

3378
03:07:30,080 --> 03:07:33,160
Like, when you travel across the world, I think you experience this too.

3379
03:07:33,160 --> 03:07:35,960
There are certain cultures that are like, they have their own thing going on.

3380
03:07:35,960 --> 03:07:36,960
They don't.

3381
03:07:36,960 --> 03:07:40,760
So you, you can see that happening more and more and more and more in the future.

3382
03:07:40,760 --> 03:07:42,080
We have little communities.

3383
03:07:42,080 --> 03:07:43,080
Yeah.

3384
03:07:43,080 --> 03:07:44,080
Yeah.

3385
03:07:44,080 --> 03:07:45,080
I think so.

3386
03:07:45,080 --> 03:07:46,680
That seems to be the, that seems to be how it's going right now.

3387
03:07:46,680 --> 03:07:48,840
And I don't see that trend, like really reversing.

3388
03:07:48,840 --> 03:07:53,040
I think people are diverse and they're able to choose their own, like, path and existence.

3389
03:07:53,040 --> 03:07:55,600
And I sort of like celebrate that.

3390
03:07:55,600 --> 03:08:00,240
And so we use been something much time in the metaverse in the virtual reality or which

3391
03:08:00,240 --> 03:08:01,240
community area.

3392
03:08:01,240 --> 03:08:08,760
Are you the physicalist, the, the, the, the physical reality, enjoyer or do you see drawing

3393
03:08:08,760 --> 03:08:12,560
a lot of pleasure and fulfillment in the digital world?

3394
03:08:12,560 --> 03:08:13,560
Yeah.

3395
03:08:13,560 --> 03:08:17,400
I think, well, currently the virtual reality is not that compelling.

3396
03:08:17,400 --> 03:08:21,720
I do think it can improve a lot, but I don't really know to what extent.

3397
03:08:21,720 --> 03:08:24,320
Maybe, you know, there's actually, like, even more exotic things you can think about

3398
03:08:24,320 --> 03:08:26,520
with, like, neural links or stuff like that.

3399
03:08:26,520 --> 03:08:31,760
So currently, I kind of see myself as mostly a team human person.

3400
03:08:31,760 --> 03:08:32,760
I love nature.

3401
03:08:32,760 --> 03:08:33,760
Yeah.

3402
03:08:33,760 --> 03:08:34,760
I love harmony.

3403
03:08:34,760 --> 03:08:35,760
I love people.

3404
03:08:35,760 --> 03:08:36,760
I love humanity.

3405
03:08:36,760 --> 03:08:39,160
I love emotions of humanity.

3406
03:08:39,160 --> 03:08:44,600
And I, I just want to be like in this, like, solar punk little utopia that's my happy place.

3407
03:08:44,600 --> 03:08:45,600
Yes.

3408
03:08:45,600 --> 03:08:50,080
My happy place is like people I love thinking about cool problems around by a lush, beautiful,

3409
03:08:50,080 --> 03:08:51,080
dynamic nature.

3410
03:08:51,080 --> 03:08:52,080
Yeah.

3411
03:08:52,080 --> 03:08:56,480
And secretly high tech in places that count places like the use technology to,

3412
03:08:56,480 --> 03:09:00,120
to empower that love for other humans and nature.

3413
03:09:00,120 --> 03:09:01,120
Yeah.

3414
03:09:01,120 --> 03:09:03,320
I think a technology used, like, very sparingly.

3415
03:09:03,320 --> 03:09:07,400
I don't love when it sort of gets in the way of humanity in many ways.

3416
03:09:07,400 --> 03:09:11,760
I like just people being humans in the way we sort of, like, slightly evolved and prefer

3417
03:09:11,760 --> 03:09:13,520
I think just by default.

3418
03:09:13,520 --> 03:09:16,200
People kept asking me because they know you love reading.

3419
03:09:16,200 --> 03:09:23,400
Are there particular books that you enjoyed that had an impact on you for silly or for

3420
03:09:23,400 --> 03:09:27,360
profound reasons that you recommend?

3421
03:09:27,360 --> 03:09:29,680
You mentioned the vital question.

3422
03:09:29,680 --> 03:09:30,680
Many, of course.

3423
03:09:30,680 --> 03:09:33,160
I think in biology as an example, the vital question is a good one.

3424
03:09:33,160 --> 03:09:38,880
Anything by Niclaine, really, life ascending, I would say, is like a bit more potentially

3425
03:09:38,880 --> 03:09:44,280
representative as like a summary of a lot of the things he's been talking about.

3426
03:09:44,280 --> 03:09:46,280
I was very impacted by the selfish gene.

3427
03:09:46,280 --> 03:09:49,960
I thought that was a really good book that helped me understand altruism as an example

3428
03:09:49,960 --> 03:09:53,760
and where it comes from and just realizing that, you know, the selection is on a level of

3429
03:09:53,760 --> 03:09:54,760
genes.

3430
03:09:54,760 --> 03:09:56,520
It was a huge insight for me at the time and it sort of, like, cleared up a lot of things

3431
03:09:56,520 --> 03:09:57,520
for me.

3432
03:09:57,520 --> 03:10:02,080
What do you think about the idea that ideas are the organisms, the meat?

3433
03:10:02,080 --> 03:10:03,080
Yes.

3434
03:10:03,080 --> 03:10:04,080
Love it.

3435
03:10:04,080 --> 03:10:05,080
100%.

3436
03:10:05,080 --> 03:10:11,720
Are you able to walk around with that notion for a while that there is an evolutionary kind

3437
03:10:11,720 --> 03:10:13,400
of process with ideas as well?

3438
03:10:13,400 --> 03:10:14,400
There absolutely is.

3439
03:10:14,400 --> 03:10:18,520
There's memes just like genes and they compete and they live in our brains.

3440
03:10:18,520 --> 03:10:19,520
It's beautiful.

3441
03:10:19,520 --> 03:10:22,120
Are we silly humans thinking that we're the organisms?

3442
03:10:22,120 --> 03:10:26,320
Is it possible that the primary organisms are the ideas?

3443
03:10:26,320 --> 03:10:31,640
Yeah, I would say like the idea is kind of living in a software of like our civilization

3444
03:10:31,640 --> 03:10:33,760
in the minds and so on.

3445
03:10:33,760 --> 03:10:37,880
We think as humans that the hardware is the fundamental thing.

3446
03:10:37,880 --> 03:10:43,200
I human is a hardware entity, but it could be the software, right?

3447
03:10:43,200 --> 03:10:44,200
Yeah.

3448
03:10:44,200 --> 03:10:48,080
Yeah, I would say like there needs to be some grounding at some point to like a physical

3449
03:10:48,080 --> 03:10:49,080
reality.

3450
03:10:49,080 --> 03:10:56,600
But if we clone an Andre, the software is a thing like is this thing that makes that

3451
03:10:56,600 --> 03:10:57,600
thing special, right?

3452
03:10:57,600 --> 03:10:58,600
Yeah.

3453
03:10:58,600 --> 03:10:59,600
I guess I, you're right.

3454
03:10:59,600 --> 03:11:01,800
But then cloning might be exceptionally difficult.

3455
03:11:01,800 --> 03:11:05,200
There might be a deep integration between the software and the hardware in ways we don't

3456
03:11:05,200 --> 03:11:06,200
quite understand.

3457
03:11:06,200 --> 03:11:10,080
Well, from the ultimate point of view, like what makes me special is more like the gang

3458
03:11:10,080 --> 03:11:13,280
of genes that are writing in my chromosomes, I suppose, right?

3459
03:11:13,280 --> 03:11:15,760
Like they're they're they're replicating unit, I suppose.

3460
03:11:15,760 --> 03:11:20,240
No, but that's just for the cute, the thing that makes you special, sure.

3461
03:11:20,240 --> 03:11:29,680
Well, the reality is what makes you special is your ability to survive based on the software

3462
03:11:29,680 --> 03:11:34,080
that runs on the hardware that was built by the genes.

3463
03:11:34,080 --> 03:11:37,480
So the software is the thing that makes you survive, not the hardware, all right?

3464
03:11:37,480 --> 03:11:38,480
It's a little bit of both.

3465
03:11:38,480 --> 03:11:42,080
You know, it's just like a second layer, it's a new second layer that hasn't been there

3466
03:11:42,080 --> 03:11:43,080
before the brain.

3467
03:11:43,080 --> 03:11:46,080
So they both coexist, but there's also layers of the software.

3468
03:11:46,080 --> 03:11:52,120
I mean, it's it's not, it's a it's a abstraction at the top of abstractions.

3469
03:11:52,120 --> 03:11:53,120
But okay.

3470
03:11:53,120 --> 03:11:58,520
So soft is gene and the plane, I would say sometimes books are like not sufficient.

3471
03:11:58,520 --> 03:12:01,520
I like to reach for textbooks sometimes.

3472
03:12:01,520 --> 03:12:05,440
I kind of feel like books are for too much of a general consumption sometime and they

3473
03:12:05,440 --> 03:12:10,000
just kind of like they're too high up in a lot of abstraction and it's not good enough.

3474
03:12:10,000 --> 03:12:11,000
So I like textbooks.

3475
03:12:11,000 --> 03:12:14,840
I like the cell, I think the cell was pretty cool.

3476
03:12:14,840 --> 03:12:19,880
That's why also I like the writing of Nick Lane is because he's pretty willing to step

3477
03:12:19,880 --> 03:12:26,040
one level down and he doesn't, yeah, he's willing to go there.

3478
03:12:26,040 --> 03:12:27,920
But he's also willing to sort of be throughout the stack.

3479
03:12:27,920 --> 03:12:31,000
So he'll go down to a lot of detail, but then he will come back up.

3480
03:12:31,000 --> 03:12:34,880
And I think he has a, yeah, basically, I really appreciate that.

3481
03:12:34,880 --> 03:12:39,800
That's why I love college, early college, even high school, just textbooks on the basics.

3482
03:12:39,800 --> 03:12:40,800
Yeah.

3483
03:12:40,800 --> 03:12:45,680
You were science of mathematics, of biology of chemistry, yes, those are, they condense

3484
03:12:45,680 --> 03:12:51,400
down like a, it's sufficiently general that you can understand that both the philosophy

3485
03:12:51,400 --> 03:12:56,560
and the details, but also like you get homework problems and you get to play with it as much

3486
03:12:56,560 --> 03:12:59,640
as you would if you were in programming stuff.

3487
03:12:59,640 --> 03:13:00,640
Yeah.

3488
03:13:00,640 --> 03:13:04,400
And then I'm also suspicious of textbooks, honestly, because as an example in deep learning,

3489
03:13:04,400 --> 03:13:07,240
there's no like amazing textbooks and I feel this changing very quickly.

3490
03:13:07,240 --> 03:13:11,520
I imagine the same is true and say synthetic biology and so on.

3491
03:13:11,520 --> 03:13:13,560
These books like the cell are kind of outdated.

3492
03:13:13,560 --> 03:13:14,680
They're still high level.

3493
03:13:14,680 --> 03:13:16,560
Like what is the actual real source of truth?

3494
03:13:16,560 --> 03:13:23,280
It's people in wet labs working with cells, you know, sequencing genomes and, yeah, actually

3495
03:13:23,280 --> 03:13:25,000
working with, working with it.

3496
03:13:25,000 --> 03:13:28,040
And I don't have that much exposure to that or what that looks like.

3497
03:13:28,040 --> 03:13:31,000
So I still don't fully, I'm reading through the cell and it's kind of interesting and

3498
03:13:31,000 --> 03:13:35,040
I'm learning, but it's still not sufficient, I would say, in terms of understanding what

3499
03:13:35,040 --> 03:13:38,360
it's a clean summarization of the mainstream narrative.

3500
03:13:38,360 --> 03:13:39,360
Yeah.

3501
03:13:39,360 --> 03:13:43,760
But you have to learn that before you break out at the towards the cutting edge.

3502
03:13:43,760 --> 03:13:44,760
Yeah.

3503
03:13:44,760 --> 03:13:47,520
What is the actual process of working with these cells and growing them and incubating

3504
03:13:47,520 --> 03:13:50,800
them and, you know, it's kind of like a massive cooking recipes of making sure your

3505
03:13:50,800 --> 03:13:55,360
self-slows and proliferate and then you're sequencing them running experiments and just

3506
03:13:55,360 --> 03:13:56,360
how that works.

3507
03:13:56,360 --> 03:13:59,080
I think it's kind of like the source of truth of, at the end of the day, what's really

3508
03:13:59,080 --> 03:14:01,800
useful in terms of creating therapies and so on.

3509
03:14:01,800 --> 03:14:02,800
Yeah.

3510
03:14:02,800 --> 03:14:06,880
And the future AI textbooks will be because, you know, there's artificial intelligence

3511
03:14:06,880 --> 03:14:07,880
to modern approach.

3512
03:14:07,880 --> 03:14:12,200
I actually haven't read if it's come out the recent version, the recent, there's been

3513
03:14:12,200 --> 03:14:13,200
a recent addition.

3514
03:14:13,200 --> 03:14:15,880
I also saw there's a science of deep learning book.

3515
03:14:15,880 --> 03:14:18,600
I'm waiting for textbooks that worth recommending or worth reading.

3516
03:14:18,600 --> 03:14:19,600
Yeah.

3517
03:14:19,600 --> 03:14:23,600
It's it's tricky because it's like papers and code, code, code.

3518
03:14:23,600 --> 03:14:25,720
Honestly, I think papers are quite good.

3519
03:14:25,720 --> 03:14:28,800
I especially like the appendix, appendix of any paper as well.

3520
03:14:28,800 --> 03:14:33,240
It's like the most detail you can have.

3521
03:14:33,240 --> 03:14:35,920
It doesn't have to be cohesive connected to anything else.

3522
03:14:35,920 --> 03:14:39,200
You just described me very specific ways of the particular thing.

3523
03:14:39,200 --> 03:14:40,200
Yeah.

3524
03:14:40,200 --> 03:14:43,120
Many times papers can be actually quite readable, not always, but sometimes the introduction

3525
03:14:43,120 --> 03:14:47,080
and the abstract is readable, even for someone outside of the field, not this is not always

3526
03:14:47,080 --> 03:14:48,080
true.

3527
03:14:48,080 --> 03:14:52,600
And sometimes I think unfortunately scientists use complex terms even when it's not necessary.

3528
03:14:52,600 --> 03:14:54,040
I think that's harmful.

3529
03:14:54,040 --> 03:14:55,960
I think there's no reason for that.

3530
03:14:55,960 --> 03:15:01,680
And papers sometimes are longer than they need to be in the parts that don't matter.

3531
03:15:01,680 --> 03:15:02,680
Yeah.

3532
03:15:02,680 --> 03:15:05,920
The appendix would be long, but then the paper itself, you know, look at Einstein, make

3533
03:15:05,920 --> 03:15:06,920
it simple.

3534
03:15:06,920 --> 03:15:10,160
Yeah, but certainly I've come across papers I would say in say like synthetic biology or

3535
03:15:10,160 --> 03:15:13,520
something that I thought were quite readable for the abstract and the introduction.

3536
03:15:13,520 --> 03:15:16,280
And then you're reading the rest of it and you don't fully understand, but you kind

3537
03:15:16,280 --> 03:15:20,360
of are getting a gist and I think it's cool.

3538
03:15:20,360 --> 03:15:25,760
What advice, you give advice to folks interested in machine learning and research, but in

3539
03:15:25,760 --> 03:15:32,200
general life advice to a young person high school early college about how to have a career

3540
03:15:32,200 --> 03:15:34,920
that can be proud of or life they can be proud of.

3541
03:15:34,920 --> 03:15:37,960
Yeah, I think I'm very hesitant to give general advice.

3542
03:15:37,960 --> 03:15:38,960
I think it's really hard.

3543
03:15:38,960 --> 03:15:41,320
I've mentioned like some of the stuff I've mentioned is fairly general.

3544
03:15:41,320 --> 03:15:46,000
I think like focus on just the amount of work you're spending on like a thing compared

3545
03:15:46,000 --> 03:15:48,000
yourself only to yourself, not to others.

3546
03:15:48,000 --> 03:15:49,000
That's good.

3547
03:15:49,000 --> 03:15:50,000
I think those are fairly general.

3548
03:15:50,000 --> 03:15:52,320
How do you pick the thing?

3549
03:15:52,320 --> 03:15:57,960
You just have like a deep interest in something or like try to like find the argmax over like

3550
03:15:57,960 --> 03:15:59,280
the things that you're interested in.

3551
03:15:59,280 --> 03:16:01,040
Argmax at that moment and stick with it.

3552
03:16:01,040 --> 03:16:05,280
How do you not get distracted and switch to another thing?

3553
03:16:05,280 --> 03:16:07,160
You can if you like.

3554
03:16:07,160 --> 03:16:12,280
Well, if you do an argmax repeatedly every week, it doesn't converge.

3555
03:16:12,280 --> 03:16:13,280
It doesn't.

3556
03:16:13,280 --> 03:16:14,280
It's a problem.

3557
03:16:14,280 --> 03:16:17,320
Yeah, you can like low pass filter yourself in terms of like what has consistently been

3558
03:16:17,320 --> 03:16:18,480
true for you.

3559
03:16:18,480 --> 03:16:23,520
But yeah, I definitely see how it can be hard, but I would say like you're going to work

3560
03:16:23,520 --> 03:16:26,160
the hardest on the thing that you care about the most.

3561
03:16:26,160 --> 03:16:30,680
So low pass filter yourself and really introspect in your past, where are the things that gave

3562
03:16:30,680 --> 03:16:34,800
you energy and what are the things that took energy away from you, concrete examples.

3563
03:16:34,800 --> 03:16:38,560
And usually from those concrete examples, sometimes parents can emerge.

3564
03:16:38,560 --> 03:16:41,160
I like it when things look like this when I'm in these positions.

3565
03:16:41,160 --> 03:16:44,480
So that's not necessarily the field, but the kind of stuff you're doing in a particular

3566
03:16:44,480 --> 03:16:45,480
field.

3567
03:16:45,480 --> 03:16:50,560
It seems like you were energized by implementing stuff, building actual things.

3568
03:16:50,560 --> 03:16:55,920
Yeah, being low level, learning, and then also communicating so that others can go through

3569
03:16:55,920 --> 03:16:59,480
the same realizations and shortening that gap.

3570
03:16:59,480 --> 03:17:01,880
Because I usually have to do way too much work to understand a thing.

3571
03:17:01,880 --> 03:17:04,360
And then I'm like, okay, this is actually like, okay, I think I get it.

3572
03:17:04,360 --> 03:17:06,000
And like, why was it so much work?

3573
03:17:06,000 --> 03:17:09,040
It should have been much less work.

3574
03:17:09,040 --> 03:17:12,680
And that gives me a lot of frustration and that's why I sometimes go teach.

3575
03:17:12,680 --> 03:17:19,640
So aside from the teaching you're doing now, putting out videos, aside from a potential

3576
03:17:19,640 --> 03:17:26,080
Godfather Part II, with the AGI at Tesla and beyond, what does the future of Project

3577
03:17:26,080 --> 03:17:27,080
Kapati hold?

3578
03:17:27,080 --> 03:17:28,080
Have you figured that out yet?

3579
03:17:28,080 --> 03:17:29,080
I don't know.

3580
03:17:29,080 --> 03:17:35,880
I mean, as you see through the fog of war, that is all of our future.

3581
03:17:35,880 --> 03:17:41,080
Do you start seeing silhouettes of what that possible future could look like?

3582
03:17:41,080 --> 03:17:47,480
The consistent thing I've been always interested in for me at least is AI and that's probably

3583
03:17:47,480 --> 03:17:51,080
why I'm spending my rest of my life on because I just care about it a lot.

3584
03:17:51,080 --> 03:17:55,160
And I actually care about like many other problems as well, like say aging, which I basically

3585
03:17:55,160 --> 03:17:56,600
view as disease.

3586
03:17:56,600 --> 03:18:02,320
And I care about that as well, but I don't think it's a good idea to go after it specifically.

3587
03:18:02,320 --> 03:18:06,160
I don't actually think that humans will be able to come up with the answer.

3588
03:18:06,160 --> 03:18:10,360
I think the correct thing to do is to ignore those problems and you solve AI and then use

3589
03:18:10,360 --> 03:18:11,960
that to solve everything else.

3590
03:18:11,960 --> 03:18:13,240
And I think there's a chance that this will work.

3591
03:18:13,240 --> 03:18:15,120
I think it's a very high chance.

3592
03:18:15,120 --> 03:18:18,560
And that's kind of like the way I'm betting at least.

3593
03:18:18,560 --> 03:18:25,360
So when you think about AI, are you interested in all kinds of applications, all kinds of domains,

3594
03:18:25,360 --> 03:18:29,880
and any domain you focus on will allow you to get insights to the big problem of AGI?

3595
03:18:29,880 --> 03:18:30,880
Yeah.

3596
03:18:30,880 --> 03:18:31,880
For me, it's the ultimate meta problem.

3597
03:18:31,880 --> 03:18:33,600
I don't want to work on any one specific problem.

3598
03:18:33,600 --> 03:18:34,600
There's too many problems.

3599
03:18:34,600 --> 03:18:36,680
So how can you work on all problems simultaneously?

3600
03:18:36,680 --> 03:18:39,560
You solve the meta problem, which to me is just intelligence.

3601
03:18:39,560 --> 03:18:42,480
And how do you automate it?

3602
03:18:42,480 --> 03:18:49,800
Is there cool small projects like archive sanity and so on that you're thinking about the

3603
03:18:49,800 --> 03:18:53,280
world, the ML world can anticipate?

3604
03:18:53,280 --> 03:18:55,280
There's always like some fun side projects.

3605
03:18:55,280 --> 03:18:56,280
Yeah.

3606
03:18:56,280 --> 03:18:57,280
Archive sanity is one.

3607
03:18:57,280 --> 03:18:58,920
Basically, like there's way too many archive papers.

3608
03:18:58,920 --> 03:19:02,440
How can I organize it and recommend papers and so on?

3609
03:19:02,440 --> 03:19:04,520
I transcribed all of your podcasts.

3610
03:19:04,520 --> 03:19:05,520
Yeah.

3611
03:19:05,520 --> 03:19:11,240
From that experience, from transcribing the process of like you like consuming audiobooks

3612
03:19:11,240 --> 03:19:18,240
and podcasts and so on, here's a process that achieves closer to human level performance

3613
03:19:18,240 --> 03:19:19,240
and annotation.

3614
03:19:19,240 --> 03:19:20,240
Yeah.

3615
03:19:20,240 --> 03:19:24,080
Well, I definitely was like surprised that transcription with opening as Whisperer was

3616
03:19:24,080 --> 03:19:28,720
working so well, compared to what I'm familiar with from Siri and like a few other systems,

3617
03:19:28,720 --> 03:19:29,720
I guess.

3618
03:19:29,720 --> 03:19:34,280
It worked so well and that's what gave me some energy to like try it out.

3619
03:19:34,280 --> 03:19:36,840
And I thought it could be fun to run on podcasts.

3620
03:19:36,840 --> 03:19:41,280
It's kind of not obvious to me why Whisperer is so much better compared to anything else

3621
03:19:41,280 --> 03:19:44,040
because I feel like there should be a lot of incentive for a lot of companies to produce

3622
03:19:44,040 --> 03:19:46,800
transcription systems and that they've done so over a long time.

3623
03:19:46,800 --> 03:19:48,600
Whisperer is not a super exotic model.

3624
03:19:48,600 --> 03:19:50,440
It's a transformer.

3625
03:19:50,440 --> 03:19:54,320
It takes male spectrograms and you know, just outputs tokens of text.

3626
03:19:54,320 --> 03:19:55,840
It's not crazy.

3627
03:19:55,840 --> 03:19:58,480
The model and everything has been around for a long time.

3628
03:19:58,480 --> 03:20:02,000
I'm not actually 100% sure why it's not obvious to me either.

3629
03:20:02,000 --> 03:20:08,480
It makes me feel like I'm missing something because there's huge, even Google and so on

3630
03:20:08,480 --> 03:20:10,280
YouTube transcription.

3631
03:20:10,280 --> 03:20:11,280
Yeah.

3632
03:20:11,280 --> 03:20:12,280
Yeah.

3633
03:20:12,280 --> 03:20:18,240
It's unclear but some of it is also integrating into a bigger system that is so the user interface

3634
03:20:18,240 --> 03:20:20,080
how's deployed and all that kind of stuff.

3635
03:20:20,080 --> 03:20:25,280
Maybe running it as an independent thing is much easier like an order magnitude easier

3636
03:20:25,280 --> 03:20:31,280
than deploying to a large integrated system like YouTube transcription or anything like

3637
03:20:31,280 --> 03:20:32,280
meetings.

3638
03:20:32,280 --> 03:20:38,560
Zoom has transcription that's kind of crappy but creating interface where the text is

3639
03:20:38,560 --> 03:20:40,680
different individual speakers.

3640
03:20:40,680 --> 03:20:47,760
It's able to display it in compelling ways, run it real time, all that kind of stuff.

3641
03:20:47,760 --> 03:20:49,000
Maybe that's difficult.

3642
03:20:49,000 --> 03:20:56,320
I still have the explanation I have because I'm currently paying quite a bit for human

3643
03:20:56,320 --> 03:20:59,120
transcription and human caption annotation.

3644
03:20:59,120 --> 03:21:03,840
I'm like, it seems like there's a huge incentive to automate that.

3645
03:21:03,840 --> 03:21:04,840
Yeah.

3646
03:21:04,840 --> 03:21:05,840
It's very confusing.

3647
03:21:05,840 --> 03:21:08,640
I mean, I don't know if you looked at some of the whisper transcripts but they're quite

3648
03:21:08,640 --> 03:21:09,640
good.

3649
03:21:09,640 --> 03:21:10,640
They're good.

3650
03:21:10,640 --> 03:21:17,080
And especially in tricky cases, I've seen whisper performance on like super tricky cases

3651
03:21:17,080 --> 03:21:18,520
and it doesn't credibly well.

3652
03:21:18,520 --> 03:21:19,520
So I don't know.

3653
03:21:19,520 --> 03:21:21,000
A podcast is pretty simple.

3654
03:21:21,000 --> 03:21:26,800
It's like high quality audio and you're speaking usually pretty clearly.

3655
03:21:26,800 --> 03:21:31,960
So I don't know what open AI's plans are either.

3656
03:21:31,960 --> 03:21:34,800
But yeah, there's always fun projects basically.

3657
03:21:34,800 --> 03:21:38,000
And stable diffusion also is opening up a huge amount of experimentation.

3658
03:21:38,000 --> 03:21:42,840
I would say in the visual realm and generating images and videos and movies ultimately.

3659
03:21:42,840 --> 03:21:43,840
Yeah, videos now.

3660
03:21:43,840 --> 03:21:45,640
And so that's going to be pretty crazy.

3661
03:21:45,640 --> 03:21:50,480
That's going to almost certainly work and it's going to be really interesting when the

3662
03:21:50,480 --> 03:21:52,760
cost of content creation is going to fall to zero.

3663
03:21:52,760 --> 03:21:56,320
You used to need a painter for a few months to paint a thing and now it's going to be

3664
03:21:56,320 --> 03:21:58,560
speak to your phone to get your video.

3665
03:21:58,560 --> 03:22:05,640
So Hollywood will start using it to generate scenes which completely opens up.

3666
03:22:05,640 --> 03:22:12,520
Yeah, so you can make a like a movie like Avatar eventually for under a million dollars.

3667
03:22:12,520 --> 03:22:13,520
Much less.

3668
03:22:13,520 --> 03:22:14,520
Maybe just by talking to your phone.

3669
03:22:14,520 --> 03:22:17,800
I mean, I know it sounds kind of crazy.

3670
03:22:17,800 --> 03:22:21,160
And then there'd be some voting mechanism like how do you have a like would there be

3671
03:22:21,160 --> 03:22:27,560
a show on Netflix that's generated completely automatically?

3672
03:22:27,560 --> 03:22:32,760
And what does it look like also when you can generate it on demand and it's and there's

3673
03:22:32,760 --> 03:22:34,320
infinity of it.

3674
03:22:34,320 --> 03:22:35,320
Yeah.

3675
03:22:35,320 --> 03:22:38,320
Oh, man.

3676
03:22:38,320 --> 03:22:39,320
All the synthetic content.

3677
03:22:39,320 --> 03:22:43,840
I mean, it's humbling because we treat ourselves as special for being able to generate art

3678
03:22:43,840 --> 03:22:46,520
and ideas and all that kind of stuff.

3679
03:22:46,520 --> 03:22:50,040
If that can be done in an automated way by AI.

3680
03:22:50,040 --> 03:22:51,040
Yeah.

3681
03:22:51,040 --> 03:22:54,440
I think it's fascinating to me how these the predictions of AI and what it's going to

3682
03:22:54,440 --> 03:22:57,840
look like and what's going to be capable of are completely inverted and wrong.

3683
03:22:57,840 --> 03:23:01,400
And sci-fi of 50s and 60s is just like totally not right.

3684
03:23:01,400 --> 03:23:05,640
They imagine AI is like super calculating their approvers and we're getting things that

3685
03:23:05,640 --> 03:23:07,240
can talk to you about emotions.

3686
03:23:07,240 --> 03:23:10,040
They can do art is just like weird.

3687
03:23:10,040 --> 03:23:12,120
Are you excited about that feature?

3688
03:23:12,120 --> 03:23:18,480
Just AI's like hybrid systems heterogeneous systems of humans and AI's talking about

3689
03:23:18,480 --> 03:23:23,520
emotions, Netflix and chill with an AI system that's where the Netflix thing you watch is

3690
03:23:23,520 --> 03:23:26,080
also generated by AI.

3691
03:23:26,080 --> 03:23:29,520
I think it's going to be interesting for sure.

3692
03:23:29,520 --> 03:23:32,800
And I think I'm cautiously optimistic, but it's not obvious.

3693
03:23:32,800 --> 03:23:42,240
Well, the sad thing is your brain and mine developed in a time where before Twitter, before

3694
03:23:42,240 --> 03:23:43,240
the internet.

3695
03:23:43,240 --> 03:23:47,880
So I wonder people that are born inside of it might have a different experience.

3696
03:23:47,880 --> 03:23:53,960
Like I and maybe you can still resist it and the people born now will not.

3697
03:23:53,960 --> 03:23:58,640
Well, I do feel like humans are extremely malleable.

3698
03:23:58,640 --> 03:24:01,160
And you're probably right.

3699
03:24:01,160 --> 03:24:05,240
What is the meaning of life, Andre?

3700
03:24:05,240 --> 03:24:11,160
We talked about sort of the universe having a conversation with us humans or with the

3701
03:24:11,160 --> 03:24:16,800
systems we create to try to answer for the universe, for the creator of the universe

3702
03:24:16,800 --> 03:24:17,800
to notice us.

3703
03:24:17,800 --> 03:24:22,760
We're trying to create systems that are loud enough to answer back.

3704
03:24:22,760 --> 03:24:25,120
I don't know if that's the meaning of life.

3705
03:24:25,120 --> 03:24:27,040
That's like meaning of life for some people.

3706
03:24:27,040 --> 03:24:30,480
The first level answer I would say is anyone can choose their own meaning of life because

3707
03:24:30,480 --> 03:24:33,200
we are conscious entity and it's beautiful.

3708
03:24:33,200 --> 03:24:38,360
Number one, but I do think that like a deeper meaning of life is someone is interested

3709
03:24:38,360 --> 03:24:43,680
is along the lines of like what the hell is all this and like why?

3710
03:24:43,680 --> 03:24:47,520
And if you look at the fundamental physics and the quantum field theory and the standard

3711
03:24:47,520 --> 03:24:55,640
model, they're like very complicated and there's this like 19 free parameters of our universe.

3712
03:24:55,640 --> 03:24:59,760
And like what's going on with all this stuff and why is it here and can I hack it?

3713
03:24:59,760 --> 03:25:00,760
Can I work with it?

3714
03:25:00,760 --> 03:25:01,760
Is there a message for me?

3715
03:25:01,760 --> 03:25:03,520
Am I supposed to create a message?

3716
03:25:03,520 --> 03:25:05,840
And so I think there's some fundamental answers there.

3717
03:25:05,840 --> 03:25:10,000
But I think there's actually even like you can't actually like really make dent in those

3718
03:25:10,000 --> 03:25:11,360
without more time.

3719
03:25:11,360 --> 03:25:15,280
And so to me, also there's a big question around just getting more time, honestly.

3720
03:25:15,280 --> 03:25:18,240
Yeah, that's kind of like what I think about quite a bit as well.

3721
03:25:18,240 --> 03:25:25,520
So kind of the ultimate or at least first way to sneak up to the why question is to try

3722
03:25:25,520 --> 03:25:30,560
to escape the system, the universe.

3723
03:25:30,560 --> 03:25:35,840
And then for that, you sort of backtrack and say, okay, for that, it's going to take

3724
03:25:35,840 --> 03:25:36,840
a very long time.

3725
03:25:36,840 --> 03:25:41,320
So the why question boils down from an engineering perspective to how do we extend?

3726
03:25:41,320 --> 03:25:44,880
Yeah, I think that's the question number one, practically speaking because you can't,

3727
03:25:44,880 --> 03:25:48,840
you're not going to calculate the answer to the deeper questions in time you have.

3728
03:25:48,840 --> 03:25:53,920
And that could be extending your own lifetime or extending just the lifetime of human civilization.

3729
03:25:53,920 --> 03:25:57,480
Of whoever wants to, not many people might not want that.

3730
03:25:57,480 --> 03:26:02,560
But I think people who do want that, I think, I think it's probably possible.

3731
03:26:02,560 --> 03:26:05,440
And I don't think, I don't know that people fully realize this.

3732
03:26:05,440 --> 03:26:09,040
I kind of feel like people think of death as an inevitability.

3733
03:26:09,040 --> 03:26:11,320
But at the end of the day, this is a physical system.

3734
03:26:11,320 --> 03:26:13,080
Somethings go wrong.

3735
03:26:13,080 --> 03:26:17,120
It makes sense why things like this happen evolutionarily, speaking.

3736
03:26:17,120 --> 03:26:21,040
And there's most certainly interventions that mitigate it.

3737
03:26:21,040 --> 03:26:26,640
I mean, that would be interesting if death is eventually looked at as a fascinating thing

3738
03:26:26,640 --> 03:26:28,920
that used to happen to humans.

3739
03:26:28,920 --> 03:26:30,000
I don't think it's unlikely.

3740
03:26:30,000 --> 03:26:33,760
I think it's likely.

3741
03:26:33,760 --> 03:26:38,640
And it's up to our imagination to try to predict what the world without death looks

3742
03:26:38,640 --> 03:26:39,640
like.

3743
03:26:39,640 --> 03:26:43,480
Yes, it's hard to, I think the values will completely change.

3744
03:26:43,480 --> 03:26:45,240
It could be.

3745
03:26:45,240 --> 03:26:50,880
I don't really buy all these ideas that, oh, without death, there's no meaning, there's

3746
03:26:50,880 --> 03:26:52,360
nothingness.

3747
03:26:52,360 --> 03:26:54,880
I don't intuitively buy all those arguments.

3748
03:26:54,880 --> 03:26:57,440
I think there's plenty of meaning, plenty of things to learn.

3749
03:26:57,440 --> 03:26:58,440
They're interesting, exciting.

3750
03:26:58,440 --> 03:27:03,960
I want to know, I want to calculate, I want to improve the condition of all the humans

3751
03:27:03,960 --> 03:27:05,840
and organisms that are alive.

3752
03:27:05,840 --> 03:27:08,560
At the way we find meaning, we change.

3753
03:27:08,560 --> 03:27:13,120
There is a lot of humans, probably including myself, that finds meaning in the finiteness

3754
03:27:13,120 --> 03:27:14,880
of things.

3755
03:27:14,880 --> 03:27:16,640
But that doesn't mean that's the only source of meaning.

3756
03:27:16,640 --> 03:27:17,640
Yeah.

3757
03:27:17,640 --> 03:27:21,120
I do think many people will go with that, which I think is great.

3758
03:27:21,120 --> 03:27:24,240
I love the idea that people can just choose their own adventure.

3759
03:27:24,240 --> 03:27:28,640
Like you are born as a conscious, free entity, by default, I'd like to think.

3760
03:27:28,640 --> 03:27:37,880
And you have your alienable rights for life in the pursuit of happiness and the nature

3761
03:27:37,880 --> 03:27:39,880
of the landscape of happiness.

3762
03:27:39,880 --> 03:27:41,760
You can choose your own adventure mostly.

3763
03:27:41,760 --> 03:27:48,560
And that's not a fully true, but I'm still pretty sure I'm an NPC, but an NPC can't

3764
03:27:48,560 --> 03:27:51,600
know it's an NPC.

3765
03:27:51,600 --> 03:27:54,240
There could be different degrees and levels of consciousness.

3766
03:27:54,240 --> 03:27:58,880
I don't think there's a more beautiful way to end it.

3767
03:27:58,880 --> 03:28:00,240
Andre, you're an incredible person.

3768
03:28:00,240 --> 03:28:02,360
I'm really honored you would talk with me.

3769
03:28:02,360 --> 03:28:07,000
Everything you've done for the machine learning world, for the AI world, to just inspire

3770
03:28:07,000 --> 03:28:09,000
people to educate millions of people.

3771
03:28:09,000 --> 03:28:10,000
It's been great.

3772
03:28:10,000 --> 03:28:11,840
And I can't wait to see what you do next.

3773
03:28:11,840 --> 03:28:12,840
It's been an honor, man.

3774
03:28:12,840 --> 03:28:14,320
Thank you so much for talking to me.

3775
03:28:14,320 --> 03:28:15,320
Awesome.

3776
03:28:15,320 --> 03:28:16,320
Thank you.

3777
03:28:16,320 --> 03:28:19,320
Thanks for listening to this conversation with Andre Karpathi.

3778
03:28:19,320 --> 03:28:23,760
To support this podcast, please check out our sponsors in the description.

3779
03:28:23,760 --> 03:28:28,720
And now let me leave you with some words from Samuel Carlin.

3780
03:28:28,720 --> 03:28:35,760
The purpose of models is not to fit the data, but to sharpen the questions.

3781
03:28:35,760 --> 03:28:38,200
Thanks for listening and hope to see you next time.

